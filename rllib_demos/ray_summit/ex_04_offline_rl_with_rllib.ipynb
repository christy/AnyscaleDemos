{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Notebook 04. Introduction to Offline RL with RLlib\n",
    "\n",
    "¬© 2019-2022, Anyscale. All Rights Reserved <br>\n",
    "üìñ [Back to Table of Contents](./ex_00_rllib_notebooks_table_of_contents.ipynb) <br>\n",
    "‚û°Ô∏è [Next notebook](./ex_05_rllib_and_ray_serve.ipynb) <br>\n",
    "‚¨ÖÔ∏è  [Previous notebook](./ex_03_train_tune_rllib_model.ipynb) <br>\n",
    "\n",
    "### Learning objectives\n",
    "In this this notebook, you will learn:\n",
    " * [What's offline RL (aka \"batch RL\")?](#offline_rl)\n",
    " * [How to configure RLlib for offline RL](#offline_rl_with_rllib)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gym: 0.21.0\n",
      "ray: 3.0.0.dev0\n"
     ]
    }
   ],
   "source": [
    "# Import required packages.\n",
    "\n",
    "import gym\n",
    "import gym\n",
    "from gym.wrappers import RecordVideo\n",
    "from IPython.display import Video\n",
    "import os\n",
    "import pandas\n",
    "\n",
    "import ray\n",
    "# Import the config class of the algorithm, we would like to train with: CRR.\n",
    "from ray.rllib.algorithms.crr import CRRConfig\n",
    "from ray import tune\n",
    "\n",
    "print(f\"gym: {gym.__version__}\")\n",
    "print(f\"ray: {ray.__version__}\")\n",
    "\n",
    "# !ale-import-roms --import-from-pkg atari_py.atari_roms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What's offline RL (aka \"batch RL\")? <a class=\"anchor\" id=\"offline_rl\"></a>\n",
    "\n",
    "So far, we have dealt with a so-called \"online\" setting for RL, in which we had direct control over a live environment (or a simulator). We were able to send arbitrary actions to this simulator and collect its responses (rewards and observations), thereby learning \"as we go\". This setup is called \"online\" RL:\n",
    "\n",
    "<img src=\"images/online_rl.png\" width=800></img>\n",
    "\n",
    "However, often and especially in real-life industry settings, we are faced with the problem of not having a simulator at hand.\n",
    "In this case, we need to fall back to offline RL:\n",
    "\n",
    "<img src=\"images/offline_rl.png\" width=800></img>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** Due to the dynamic nature of adversarial multi-agent scenarios, we will cover the topic of\n",
    "of offline RL here only for the single-agent case.\n",
    "Research on multi-agent offline RL is bleeding edge and not well explored by RLlib thus far (see references for more information)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Offline RL comes in two flavours:\n",
    "\n",
    "#### 1) Pure imitation learning\n",
    "\n",
    "The agent will try to imitate 100% the actions/behavior that it finds in the offline data).\n",
    "This setup is nothing else but supervised learning with a `-log(p)` loss function.\n",
    "\n",
    "#### 2) Imitation learning plus improvement over the recorded behavior\n",
    "\n",
    "The agent will partly imitate the offline, recorded behavior, but also try to improve over it, learning a policy that will\n",
    "perform better in the actual environment. This is achieved by focusing on those actions within the distribution that seem more \n",
    "promising, e.g. via weighting based on the received rewards.\n",
    "\n",
    "### If we don't have a live environment, how do we know, how well our trained policy will perform?\n",
    "\n",
    "One of the challenges in offline RL is the evaluation of the trained policy. In online RL (when a simulator\n",
    "is available), one can either use the data collected for training to compute episode total rewards. Remember\n",
    "that observations, actions, rewards, and done flags are all part of this training data. Alternatively,\n",
    "one could run a separate worker (with the same trained policy) and run it on a fresh evaluation-only environment.\n",
    "In this latter case, we would also have the chance to switch off any exploratory behavior (e.g. stochasticity used\n",
    "for better action entropy).\n",
    "\n",
    "In offline RL, no such data from a live environment is available to us. There are two common ways of addressing this dilemma:\n",
    "\n",
    "1) We deploy the learnt policy/ies into production, or maybe just a portion of our production system (similar to A/B testing), and see what happens.\n",
    "\n",
    "2) We use a method called \"off policy evaluation\" (OPE) to compute an estimate on how the new policy would perform if we were to deploy it into a real environment. There are different OPE methods available in RLlib off-the-shelf.\n",
    "\n",
    "3) The third option - which we will use here - is kind of cheating and only possible if you actually do have a simulator available (but you only want to use it for evaluation, not for training, because you want to see how cool offline RL is :) )\n",
    "\n",
    "### An example offline RL experiment\n",
    "Modern offline RL algorithms are capable of learning to perfectly play e.g. the Pendulum environment, when only behavioral data from a beginner agent is available (not completely random, but nowhere perfectly solving the problem either)! We'll explore this right now using RLlib's new CRR algorithm.\n",
    "\n",
    "The Pendulum-v1 environment looks as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"images/pendulum.gif\" width=\"300\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Image\n",
    "Image(url=\"images/pendulum.gif\", width=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Continuous actions between -2.0 and 2.0 encode torques that will be applied to the hinge of a freely rotating pole.\n",
    "- The observations are x- and y- positions as well as the angular velocity.\n",
    "- The goal is to apply torques to the hinge such that the pendulum balances in an upright position.\n",
    "\n",
    "\n",
    "We will now pretend that we don't have a simulator for our problem available (the Pendulum-v1 problem), however, let's assume we possess a lot of pre-recorded, historic data from two legacy (non-RL) systems:\n",
    "- A **beginner system** that only knew how to get to a low episode reward.\n",
    "- An **expert system** that was able to solve the Pendulum-v1 environment perfectly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>actions</th>\n",
       "      <th>dones</th>\n",
       "      <th>obs</th>\n",
       "      <th>rewards</th>\n",
       "      <th>new_obs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SampleBatch</td>\n",
       "      <td>[[1.5321280956]]</td>\n",
       "      <td>[0.0]</td>\n",
       "      <td>BCJNGGhAmAAAAAAAAABckgAAAFKABZWNAAEA8hmMEm51bX...</td>\n",
       "      <td>[-1.7447183132]</td>\n",
       "      <td>BCJNGGhAmAAAAAAAAABckgAAAFKABZWNAAEA8hmMEm51bX...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SampleBatch</td>\n",
       "      <td>[[1.368789196]]</td>\n",
       "      <td>[0.0]</td>\n",
       "      <td>BCJNGGhAmAAAAAAAAABckgAAAFKABZWNAAEA8hmMEm51bX...</td>\n",
       "      <td>[-1.9830874205]</td>\n",
       "      <td>BCJNGGhAmAAAAAAAAABckQAAAFKABZWNAAEA8hmMEm51bX...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SampleBatch</td>\n",
       "      <td>[[-1.8610941172]]</td>\n",
       "      <td>[0.0]</td>\n",
       "      <td>BCJNGGhAmAAAAAAAAABckQAAAFKABZWNAAEA8hmMEm51bX...</td>\n",
       "      <td>[-2.3656635284]</td>\n",
       "      <td>BCJNGGhAmAAAAAAAAABckAAAAFKABZWNAAEA8hmMEm51bX...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>SampleBatch</td>\n",
       "      <td>[[1.9505341053]]</td>\n",
       "      <td>[0.0]</td>\n",
       "      <td>BCJNGGhAmAAAAAAAAABckAAAAFKABZWNAAEA8hmMEm51bX...</td>\n",
       "      <td>[-3.2168772221]</td>\n",
       "      <td>BCJNGGhAmAAAAAAAAABcjwAAAFKABZWNAAEA8hmMEm51bX...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>SampleBatch</td>\n",
       "      <td>[[-0.6989564896]]</td>\n",
       "      <td>[0.0]</td>\n",
       "      <td>BCJNGGhAmAAAAAAAAABcjwAAAFKABZWNAAEA8hmMEm51bX...</td>\n",
       "      <td>[-4.0010590553]</td>\n",
       "      <td>BCJNGGhAmAAAAAAAAABckAAAAFKABZWNAAEA8hmMEm51bX...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          type            actions  dones  \\\n",
       "0  SampleBatch   [[1.5321280956]]  [0.0]   \n",
       "1  SampleBatch    [[1.368789196]]  [0.0]   \n",
       "2  SampleBatch  [[-1.8610941172]]  [0.0]   \n",
       "3  SampleBatch   [[1.9505341053]]  [0.0]   \n",
       "4  SampleBatch  [[-0.6989564896]]  [0.0]   \n",
       "\n",
       "                                                 obs          rewards  \\\n",
       "0  BCJNGGhAmAAAAAAAAABckgAAAFKABZWNAAEA8hmMEm51bX...  [-1.7447183132]   \n",
       "1  BCJNGGhAmAAAAAAAAABckgAAAFKABZWNAAEA8hmMEm51bX...  [-1.9830874205]   \n",
       "2  BCJNGGhAmAAAAAAAAABckQAAAFKABZWNAAEA8hmMEm51bX...  [-2.3656635284]   \n",
       "3  BCJNGGhAmAAAAAAAAABckAAAAFKABZWNAAEA8hmMEm51bX...  [-3.2168772221]   \n",
       "4  BCJNGGhAmAAAAAAAAABcjwAAAFKABZWNAAEA8hmMEm51bX...  [-4.0010590553]   \n",
       "\n",
       "                                             new_obs  \n",
       "0  BCJNGGhAmAAAAAAAAABckgAAAFKABZWNAAEA8hmMEm51bX...  \n",
       "1  BCJNGGhAmAAAAAAAAABckQAAAFKABZWNAAEA8hmMEm51bX...  \n",
       "2  BCJNGGhAmAAAAAAAAABckAAAAFKABZWNAAEA8hmMEm51bX...  \n",
       "3  BCJNGGhAmAAAAAAAAABcjwAAAFKABZWNAAEA8hmMEm51bX...  \n",
       "4  BCJNGGhAmAAAAAAAAABckAAAAFKABZWNAAEA8hmMEm51bX...  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's first take a look at some of this (JSON) data using pandas:\n",
    "json_file = \"offline_rl_data/pendulum_expert.json\"\n",
    "dataframe = pandas.read_json(json_file, lines=True)  # don't forget lines=True -> Each line in the json is one \"rollout\" of 4 timesteps.\n",
    "dataframe.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<ray.rllib.algorithms.crr.crr.CRRConfig at 0x2826da60a00>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Learning a decent policy using offline RL requires specialized RL algorithms.\n",
    "# Examples of offline RL algos are RLlib's \"CRR\", \"MARWIL\", or \"CQL\".\n",
    "# For this example, we'll use the \"Pendulum-v0\" environment and have the \"CRR\"\n",
    "# (critic regularized regression) algorithm learn how to solve this environment, purely from\n",
    "# data recorded from a random/beginner agent.\n",
    "\n",
    "# Create a defaut CRR config:\n",
    "config = CRRConfig()\n",
    "\n",
    "# Set it up for the correct environment:\n",
    "# NOTE: We said above that we wouldn't really have an environment available (so how can\n",
    "# we set one up here??).\n",
    "# The following is only to tell the algorithm, which environment our offline data was actually taken from.\n",
    "config.environment(env=\"Pendulum-v1\")\n",
    "# If you really really don't have an environment, set `env=None` here and additionally define your action- and\n",
    "# observation spaces.\n",
    "# config.environment(env=None, action_space=..., observation_space=...)\n",
    "\n",
    "#################################################\n",
    "# This is the most important piece of code \n",
    "# in this notebook:\n",
    "# It explains how to point your \n",
    "# algorithm to the correct offline data file\n",
    "# (instead of a live-environment).\n",
    "#################################################\n",
    "config.offline_data(\n",
    "    input_=\"dataset\",\n",
    "    input_config={\n",
    "        # If you feel daring here, use the `pendulum_beginner.json` file instead of the expert one here.\n",
    "        # You may need to train a little longer, then, in order to get a decent policy.\n",
    "        # But since you have the actual Pendulum environment available for evaluation, you should be able\n",
    "        # to perfectly stop learning once a good episode reward (> -300.0) has been reached.\n",
    "        \"paths\": os.path.join(os.getcwd(), \"offline_rl_data/pendulum_expert.json\"),\n",
    "        \"format\": \"json\",\n",
    "    },\n",
    "    # The (continuous) actions in our input files are already normalized\n",
    "    # (meaning between -1.0 and 1.0) -> We don't have to do anything with them prior to\n",
    "    # computing losses.\n",
    "    actions_in_input_normalized=True,\n",
    ")\n",
    "\n",
    "# RLlib's CRR is a very new algorithm (since 1.13) and only supports\n",
    "# the PyTorch framework thus far. We'll provide a tf version in the near future.\n",
    "config.framework(\"torch\")\n",
    "\n",
    "# Set up evaluation as follows:\n",
    "config.evaluation(\n",
    "    # Run evaluation once per `train()` call.\n",
    "    evaluation_interval=1,\n",
    "\n",
    "    # Use separate resources (RLlib rollout workers).\n",
    "    evaluation_num_workers=2,\n",
    "\n",
    "    # Run 20 episodes per evaluation (per iteration) -> 10 per eval worker (we have 2 eval workers).\n",
    "    evaluation_duration=20,\n",
    "    evaluation_duration_unit=\"episodes\",\n",
    "\n",
    "    # Use a slightly different config for the evaluation:\n",
    "    evaluation_config={\n",
    "        # - Use a real environment (so we can fully trust the evaluation results, rewards, etc..)\n",
    "        \"input\": \"sampler\",\n",
    "        # - Switch off exploration for better (less stochastic) action computations.\n",
    "        \"explore\": False,\n",
    "    },\n",
    "\n",
    "    # Run evaluation alternatingly with training (not in parallel).\n",
    "    evaluation_parallel_to_training=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary\n",
    "\n",
    "In this notebook, we have learnt:\n",
    "\n",
    "* What offline RL is, how it differs from \"normal\" RL, and when you should use offline RL\n",
    "* How you can evaluate a offline-RL-learnt policy, even if you don't have a environment available (e.g. via OPE)\n",
    "* How to configure an RLlib offline-capable algorithm (e.g. CQL, CRR, or MARWIL) to read data from a datafile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercises\n",
    "\n",
    "#### 1) Finish CRR configuration\n",
    "\n",
    "Keep configuring our CRR algorithm by calling the config object's `training()` method and passing the following settings into that call:\n",
    "\n",
    "```\n",
    "gamma: 0.99\n",
    "train_batch_size: 1024\n",
    "target_network_update_freq: 1\n",
    "tau: 0.0001\n",
    "weight_type: \"exp\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<ray.rllib.algorithms.crr.crr.CRRConfig at 0x28272c1fbb0>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make the `training()` call on your config here in this cell:\n",
    "config.training(\n",
    "    gamma=0.99,\n",
    "    # <- complete the other arguments to configure our CRR algo\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Use `tune.run()` to kick off the experiment\n",
    "\n",
    "Similar to how we did it in the previous notebook, use `tune.run()` to kick off our offline RL learning experiment.\n",
    "Let's see how fast CRR can learn to play pendulum from beginner's data!\n",
    "\n",
    "- As stopping criteria, use `timesteps_total=2000000` and `evaluation/episode_reward_mean=-300`.\n",
    "- Also, make sure checkpoints are created every iteration (`checkpoint_freq=1`).\n",
    "- Set the output directory (`local_dir` arg) to \"results\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-08-01 17:37:49 (running for 00:06:27.58)<br>Memory usage on this node: 17.1/31.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/20 CPUs, 0/1 GPUs, 0.0/10.87 GiB heap, 0.0/5.43 GiB objects<br>Result logdir: C:\\Dropbox\\Projects\\ray-summit-2022-training\\ray-rllib\\results\\CRR<br>Number of trials: 1/1 (1 TERMINATED)<br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(CRR pid=19520)\u001b[0m 2022-08-01 17:31:27,121\tWARNING deprecation.py:47 -- DeprecationWarning: `min_iter_time_s` has been deprecated. Use `min_time_s_per_iteration` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(CRR pid=19520)\u001b[0m 2022-08-01 17:31:27,121\tINFO algorithm.py:332 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(CRR pid=19520)\u001b[0m 2022-08-01 17:31:27,165\tWARNING read_api.py:286 -- The number of blocks in this dataset (1) limits its parallelism to 1 concurrent tasks. This is much less than the number of available CPU slots in the cluster. Use `.repartition(n)` to increase the number of dataset blocks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(CRR pid=19520)\u001b[0m [dataset]: Run `pip install tqdm` to enable progress reporting.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(CRR pid=19520)\u001b[0m 2022-08-01 17:31:32,376\tWARNING deprecation.py:47 -- DeprecationWarning: `simple_optimizer` has been deprecated. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(CRR pid=19520)\u001b[0m 2022-08-01 17:31:32,376\tWARNING deprecation.py:47 -- DeprecationWarning: `min_iter_time_s` has been deprecated. Use `min_time_s_per_iteration` instead. This will raise an error in the future!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(RolloutWorker pid=11168)\u001b[0m DatasetReader 1 has 1136, samples.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=7724)\u001b[0m DatasetReader 4 has 1137, samples.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=22416)\u001b[0m DatasetReader 3 has 1136, samples.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=16112)\u001b[0m DatasetReader 2 has 1137, samples.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(CRR pid=19520)\u001b[0m 2022-08-01 17:31:37,214\tINFO trainable.py:160 -- Trainable.setup took 10.094 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n",
      "\u001b[2m\u001b[36m(CRR pid=19520)\u001b[0m 2022-08-01 17:31:37,215\tWARNING util.py:65 -- Install gputil for GPU system monitoring.\n",
      "\u001b[2m\u001b[36m(CRR pid=19520)\u001b[0m 2022-08-01 17:31:37,296\tWARNING deprecation.py:47 -- DeprecationWarning: `concat_samples` has been deprecated. Use `concat_samples() from rllib.policy.sample_batch` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(CRR pid=19520)\u001b[0m C:\\Programs\\Anaconda3\\envs\\rllib_tutorial\\lib\\site-packages\\ray\\rllib\\algorithms\\crr\\torch\\crr_torch_policy.py:363: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\torch\\csrc\\utils\\tensor_numpy.cpp:178.)\n",
      "\u001b[2m\u001b[36m(CRR pid=19520)\u001b[0m   torch.from_numpy(self.action_space.low).to(target_a_next),\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=11168)\u001b[0m 2022-08-01 17:31:37,288\tWARNING deprecation.py:47 -- DeprecationWarning: `concat_samples` has been deprecated. Use `concat_samples() from rllib.policy.sample_batch` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=19748)\u001b[0m 2022-08-01 17:31:47,460\tWARNING deprecation.py:47 -- DeprecationWarning: `concat_samples` has been deprecated. Use `concat_samples() from rllib.policy.sample_batch` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=11168)\u001b[0m E0801 17:37:49.710000000 18384 src/core/ext/transport/chttp2/transport/chttp2_transport.cc:1103] Received a GOAWAY with error code ENHANCE_YOUR_CALM and debug data equal to \"too_many_pings\"\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=7724)\u001b[0m E0801 17:37:49.710000000  4348 src/core/ext/transport/chttp2/transport/chttp2_transport.cc:1103] Received a GOAWAY with error code ENHANCE_YOUR_CALM and debug data equal to \"too_many_pings\"\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=22416)\u001b[0m E0801 17:37:49.710000000 12540 src/core/ext/transport/chttp2/transport/chttp2_transport.cc:1103] Received a GOAWAY with error code ENHANCE_YOUR_CALM and debug data equal to \"too_many_pings\"\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=19748)\u001b[0m E0801 17:37:49.711000000 19008 src/core/ext/transport/chttp2/transport/chttp2_transport.cc:1103] Received a GOAWAY with error code ENHANCE_YOUR_CALM and debug data equal to \"too_many_pings\"\n",
      "2022-08-01 17:37:49,831\tINFO tune.py:737 -- Total run time: 387.90 seconds (387.50 seconds for the tuning loop).\n"
     ]
    }
   ],
   "source": [
    "# Perform the `tune.run()` call here:\n",
    "results = tune.run(\n",
    "    \"CRR\",\n",
    "    # config=...  <- check out the previous notebook on how to use tune.run() with an RLlib config object\n",
    "    # ...\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'results' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [19]\u001b[0m, in \u001b[0;36m<cell line: 4>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Extract the best checkpoint from our experiment.\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# We only had a single trial (one Algorithm instance), so this should be returned here.\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m best_trial \u001b[38;5;241m=\u001b[39m \u001b[43mresults\u001b[49m\u001b[38;5;241m.\u001b[39mget_best_trial()\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# From that trial, extract the best checkpoint (max `evaluation/episode_reward_mean` value).\u001b[39;00m\n\u001b[0;32m      7\u001b[0m best_checkpoint \u001b[38;5;241m=\u001b[39m results\u001b[38;5;241m.\u001b[39mget_best_checkpoint(trial\u001b[38;5;241m=\u001b[39mbest_trial, metric\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mevaluation/episode_reward_mean\u001b[39m\u001b[38;5;124m\"\u001b[39m, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'results' is not defined"
     ]
    }
   ],
   "source": [
    "# Extract the best checkpoint from our experiment.\n",
    "\n",
    "# We only had a single trial (one Algorithm instance), so this should be returned here.\n",
    "best_trial = results.get_best_trial()\n",
    "\n",
    "# From that trial, extract the best checkpoint (max `evaluation/episode_reward_mean` value).\n",
    "best_checkpoint = results.get_best_checkpoint(trial=best_trial, metric=\"evaluation/episode_reward_mean\", mode=\"max\")\n",
    "\n",
    "# We would expect this to be either the very last checkpoint or one close to it:\n",
    "print(f\"Best checkpoint from training: {best_checkpoint}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3) Let's record our trained algorithm on a live Pendulum environment\n",
    "\n",
    "Analogous to how episode recording for CartPole was done in a previous notebook here, we will now\n",
    "restore a CRR Algorithm from one of the checkpoints created during the above `tune.run()` experiment (we will chose\n",
    "a checkpoint that was showing good mean rewards on the evaluation live-environment)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a brand new CRR Algorithm using our existing config.\n",
    "crr = config.build()\n",
    "# Override the new CRR's state by restoring from one of our checkpoints.\n",
    "# Here, we use the best checkpoint (according to the `evaluation/episode_reward_mean` criterium).\n",
    "crr.restore(best_checkpoint)\n",
    "\n",
    "print(\"CRR Algorithm restored from checkpoint\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using this\n",
    "restored algorithm, we will record a single episode as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrap a new Pendulum-v1 env with the gym VideoRecorder.\n",
    "env = RecordVideo(gym.make(\"Pendulum-v1\"), \"videos\")\n",
    "# Reset the env.\n",
    "obs = env.reset()\n",
    "\n",
    "# Run a single episode using actions computed by our trained CRR.\n",
    "while True:\n",
    "    action = crr.compute_single_action(observation=obs)\n",
    "    obs, reward, done, _ = env.step(action)\n",
    "    if done:\n",
    "        break\n",
    "        \n",
    "env.close()\n",
    "\n",
    "# Play the recorded video.\n",
    "Video(\"videos/rl-video-episode-0.mp4\", width=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References\n",
    "\n",
    "* [Offline Reinforcement Learning: Tutorial, Review, and Perspectives on Open Problems (by Sergey Levine, Aviral Kumar, George Tucker, Justin Fu, 2020)](https://arxiv.org/abs/2005.01643)\n",
    "* [Batch Reinforcement Learning (by Sascha Lange, Thomas Gabel, Martin Riedmiller, 2012)](http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.229.787)\n",
    "\n",
    "##### Early Work\n",
    "* [Least-squares policy iteration (by Michail G. Lagoudakis, Ronald Parr, 2003)](http://www.jmlr.org/papers/v4/lagoudakis03a.html)\n",
    "* [Tree-based batch mode reinforcement learning (by Damien Ernst, Pierre Geurts, Louis Wehenkel, 2005)](https://www.jmlr.org/papers/volume6/ernst05a/ernst05a.pdf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚¨ÖÔ∏è [Previous notebook](./ex_03_train_tune_rllib_model.ipynb) <br>\n",
    "‚û°Ô∏è [Next notebook](./ex_05_rllib_and_ray_serve.ipynb) <br>\n",
    "\n",
    "üìñ [Back to Table of Contents](./ex_00_rllib_notebooks_table_of_contents.ipynb)<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
