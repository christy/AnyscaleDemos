{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 02. Create a custom multi-agent RLlib environment\n",
    "\n",
    "¬© 2019-2022, Anyscale. All Rights Reserved <br>\n",
    "üìñ [Back to Table of Contents](./ex_00_rllib_notebooks_table_of_contents.ipynb) <br>\n",
    "‚û°Ô∏è [Next notebook](./ex_03_train_tune_rllib_model.ipynb) <br>\n",
    "‚¨ÖÔ∏è [Previous notebook](./ex_01_intro_gym_and_rllib.ipynb) <br>\n",
    "\n",
    "### Learning objectives\n",
    "\n",
    "In this notebook, you will learn how to:\n",
    " * [Code a custom RLlib multi-agent environment](#multi_agent_env)\n",
    " * [Select an algorithm and instantiate a config object using that algorithm's config class](#rllib_algo)\n",
    " * [Train a RL model using a multi-agent capable algorithm from RLlib](#rllib_run)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gym: 0.21.0\n",
      "ray: 3.0.0.dev0\n"
     ]
    }
   ],
   "source": [
    "# import required packages\n",
    "\n",
    "import time\n",
    "import gym\n",
    "from gym.spaces import Discrete, MultiDiscrete\n",
    "import numpy as np\n",
    "from ipywidgets import Output\n",
    "from IPython import display\n",
    "\n",
    "import ray\n",
    "from ray.rllib.algorithms.ppo import PPOConfig\n",
    "from ray.rllib.env.multi_agent_env import MultiAgentEnv\n",
    "from ray import tune\n",
    "from ray.tune.logger import pretty_print\n",
    "\n",
    "print(f\"gym: {gym.__version__}\")\n",
    "print(f\"ray: {ray.__version__}\")\n",
    "\n",
    "# !ale-import-roms --import-from-pkg atari_py.atari_roms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Code a custom RLlib multi-agent environment <a class=\"anchor\" id=\"multi_agent_env\"></a>\n",
    "\n",
    "### Review OpenAI Gym Environments\n",
    "\n",
    "We learned in the last lesson about OpenAI Gym Environments.  Specifically we covered:\n",
    "<ul>\n",
    "    <li>Action Spaces</li>\n",
    "    <li>Observation Spaces</li>\n",
    "    <li>Rewards</li>\n",
    "    <li><i>`done`</i> signal</li>\n",
    "    <li>Important gym.Env API methods:</li>\n",
    "    <ul>\n",
    "        <li>reset(self)</li>\n",
    "        <li>step(self, action: dict)</li>\n",
    "    </ul>\n",
    "    </ul>\n",
    "\n",
    "### Our example multi-agent Env\n",
    "\n",
    "We will create the following (adversarial) multi-agent environment and will run RLlib experiments on this environment in the notebooks to come.\n",
    "\n",
    "<img src=\"images/multi_agent_arena_0.png\" width=800 />\n",
    "<hr />\n",
    "<img src=\"images/multi_agent_arena_1.png\" width=800 />\n",
    "<hr />\n",
    "<img src=\"images/multi_agent_arena_2.png\" width=800 />\n",
    "<hr />\n",
    "<img src=\"images/multi_agent_arena_3.png\" width=800 />\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RLlib MultiAgentEnv API\n",
    "\n",
    "For the single-agent case, RLlib supports the gym.Env API. Thus, if you would like to provide a cutom environment, you only have to hand RLlib your own gym.Env sub-class. Override [these important methods here required by Gym](https://www.gymlibrary.ml/content/api/#standard-methods)  and all is well.\n",
    "\n",
    "However, gym.Env does not support multi-agent scenarios. So if your problem requires several agents interacting with each other in the environment, you will have to use RLlib gym.Env sub-class: `MultiAgentEnv`, which allows you to\n",
    "code custom multi-agent problems.\n",
    "\n",
    "We will now implmenent a child class of `MultiAgentEnv` and code our game logic into this new class.\n",
    "\n",
    "In the following code, we will override the methods:\n",
    "<ul>\n",
    "<li>__init__(self)</li>\n",
    "<li>reset(self)</li>\n",
    "<li>step(self, action)</li>\n",
    "</ul>\n",
    "\n",
    "\n",
    "Our `MultiAgentArena` class will also come with some more utility methods, which we will not discuss here (out of scope)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's code our multi-agent environment:\n",
    "\n",
    "# We will write the code together for all segments tagged with [!LIVE CODING!].\n",
    "# These will be - in particular - the __init__(), reset(), and step() methods.\n",
    "\n",
    "\n",
    "class MultiAgentArena(MultiAgentEnv):  # MultiAgentEnv is a gym.Env sub-class\n",
    "    \n",
    "    def __init__(self, config=None):\n",
    "\n",
    "        # [!LIVE CODING!]\n",
    "        \n",
    "        # Create a default env config.        \n",
    "        \n",
    "        # Store the dimensions of the grid.\n",
    "        \n",
    "        # End an episode after this many timesteps (return done=True).\n",
    "        \n",
    "        # Define our observation space (per-agent!).\n",
    "        \n",
    "        # Define our action space (per-agent!).\n",
    "        # 0=up, 1=right, 2=down, 3=left.\n",
    "\n",
    "        # Reset env.\n",
    "        \n",
    "        # END: [!LIVE CODING!]\n",
    "\n",
    "\n",
    "        # For rendering.\n",
    "        self.out = None\n",
    "        if config.get(\"render\"):\n",
    "            self.out = Output()\n",
    "            display.display(self.out)\n",
    "        self._spaces_in_preferred_format = False\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"Reset all state and returns initial observation of new episode.\"\"\"\n",
    "\n",
    "\n",
    "        # [!LIVE CODING!]\n",
    "\n",
    "        # Store each agents' current position as row-major coords.\n",
    "        \n",
    "        # Each agents' accumulated rewards in the this episode.\n",
    "        \n",
    "        # Reset agent1's visited fields.\n",
    "        \n",
    "        # How many timesteps have we done in this episode.\n",
    "                \n",
    "        # END: [!LIVE CODING!]\n",
    "\n",
    "\n",
    "        # Did we have a collision in recent step?\n",
    "        self.collision = False\n",
    "\n",
    "        # How many collisions in total have we had in this episode?\n",
    "        self.num_collisions = 0\n",
    "\n",
    "        # Return the initial observation in the new episode.\n",
    "        return self._get_obs()\n",
    "\n",
    "\n",
    "    def step(self, action: dict):\n",
    "        \"\"\"\n",
    "        Returns (next observation, rewards, dones, infos) after having taken the given actions.\n",
    "        \n",
    "        e.g.\n",
    "        `action={\"agent1\": action_for_agent1, \"agent2\": action_for_agent2}`\n",
    "        \"\"\"\n",
    "\n",
    "        # [!LIVE CODING!]\n",
    "\n",
    "        # Increase our time steps counter by 1.\n",
    "        \n",
    "        # An episode is \"done\" when we reach the time step limit.\n",
    "        \n",
    "        # Agent2 always moves first.\n",
    "        # events = [collision|agent1_new_field]\n",
    "        events = self._move(self.agent2_pos, action[\"agent2\"], is_agent1=False)\n",
    "        events |= self._move(self.agent1_pos, action[\"agent1\"], is_agent1=True)\n",
    "\n",
    "        # Determine rewards based on the collected events:\n",
    "\n",
    "        # Generate a `done` dict (per-agent and total).\n",
    "\n",
    "        # END: [!LIVE CODING!]\n",
    "\n",
    "\n",
    "        # Useful for rendering.\n",
    "        self.collision = \"collision\" in events\n",
    "        if self.collision is True:\n",
    "            self.num_collisions += 1    \n",
    "\n",
    "        return self._get_obs(), rewards, dones, {}  # <- info dict (not needed here).\n",
    "\n",
    "    def _get_obs(self):\n",
    "        \"\"\"\n",
    "        Returns obs dict (agent name to discrete-pos tuple) using each\n",
    "        agent's current x/y-positions.\n",
    "        \"\"\"\n",
    "        ag1_discrete_pos = self.agent1_pos[0] * self.width + \\\n",
    "            (self.agent1_pos[1] % self.width)\n",
    "        ag2_discrete_pos = self.agent2_pos[0] * self.width + \\\n",
    "            (self.agent2_pos[1] % self.width)\n",
    "        return {\n",
    "            \"agent1\": np.array([ag1_discrete_pos, ag2_discrete_pos]),\n",
    "            \"agent2\": np.array([ag2_discrete_pos, ag1_discrete_pos]),\n",
    "        }\n",
    "\n",
    "    def _move(self, coords, action, is_agent1):\n",
    "        \"\"\"\n",
    "        Moves an agent (agent1 iff is_agent1=True, else agent2) from `coords` (x/y) using the\n",
    "        given action (0=up, 1=right, etc..) and returns a resulting events dict:\n",
    "        Agent1: \"new\" when entering a new field. \"bumped\" when having been bumped into by agent2.\n",
    "        Agent2: \"bumped\" when bumping into agent1 (agent1 then gets -1.0).\n",
    "        \"\"\"\n",
    "        orig_coords = coords[:]\n",
    "        # Change the row: 0=up (-1), 2=down (+1)\n",
    "        coords[0] += -1 if action == 0 else 1 if action == 2 else 0\n",
    "        # Change the column: 1=right (+1), 3=left (-1)\n",
    "        coords[1] += 1 if action == 1 else -1 if action == 3 else 0\n",
    "\n",
    "        # Solve collisions.\n",
    "        # Make sure, we don't end up on the other agent's position.\n",
    "        # If yes, don't move (we are blocked).\n",
    "        if (is_agent1 and coords == self.agent2_pos) or (not is_agent1 and coords == self.agent1_pos):\n",
    "            coords[0], coords[1] = orig_coords\n",
    "            # Agent2 blocked agent1 (agent1 tried to run into agent2)\n",
    "            # OR Agent2 bumped into agent1 (agent2 tried to run into agent1)\n",
    "            return {\"collision\"}\n",
    "\n",
    "        # No agent blocking -> check walls.\n",
    "        if coords[0] < 0:\n",
    "            coords[0] = 0\n",
    "        elif coords[0] >= self.height:\n",
    "            coords[0] = self.height - 1\n",
    "        if coords[1] < 0:\n",
    "            coords[1] = 0\n",
    "        elif coords[1] >= self.width:\n",
    "            coords[1] = self.width - 1\n",
    "\n",
    "        # If agent1 -> \"agent1_new_field\" if new tile covered.\n",
    "        if is_agent1 and not tuple(coords) in self.agent1_visited_fields:\n",
    "            self.agent1_visited_fields.add(tuple(coords))\n",
    "            return {\"agent1_new_field\"}\n",
    "\n",
    "        # No new tile for agent1.\n",
    "        return set()\n",
    "\n",
    "    def render(self, mode=None):\n",
    "\n",
    "        if self.out is not None:\n",
    "            self.out.clear_output(wait=True)\n",
    "\n",
    "        print(\"_\" * (self.width + 2))\n",
    "        for r in range(self.height):\n",
    "            print(\"|\", end=\"\")\n",
    "            for c in range(self.width):\n",
    "                field = r * self.width + c % self.width\n",
    "                if self.agent1_pos == [r, c]:\n",
    "                    print(\"1\", end=\"\")\n",
    "                elif self.agent2_pos == [r, c]:\n",
    "                    print(\"2\", end=\"\")\n",
    "                elif (r, c) in self.agent1_visited_fields:\n",
    "                    print(\".\", end=\"\")\n",
    "                else:\n",
    "                    print(\" \", end=\"\")\n",
    "            print(\"|\")\n",
    "        print(\"‚Äæ\" * (self.width + 2))\n",
    "        print(f\"{'!!Collision!!' if self.collision else ''}\")\n",
    "        print(\"R1={: .1f}\".format(self.agent1_R))\n",
    "        print(\"R2={: .1f} ({} collisions)\".format(self.agent2_R, self.num_collisions))\n",
    "        print()\n",
    "        time.sleep(0.25)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "In the cell below:\n",
    "<ul>\n",
    "    <li>Initialize the environment</li>\n",
    "    <li>Make both agents take a few steps</li>\n",
    "    <li>Render the environment after each agent takes a step.</li>\n",
    "    </ul>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7488c2d02dcf4f44839b9f42559c4214",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent1's x/y position=[3, 1]\n",
      "Agent2's x/y position=[5, 7]\n",
      "Env timesteps=6\n"
     ]
    }
   ],
   "source": [
    "env = MultiAgentArena(config={\"render\": True})\n",
    "obs = env.reset()\n",
    "\n",
    "with env.out:\n",
    "    # Agent1 moves down, Agent2 moves up.\n",
    "    obs, rewards, dones, infos = env.step(action={\"agent1\": 2, \"agent2\": 0})\n",
    "    env.render()\n",
    "\n",
    "    # Agent1 moves right, Agent2 moves left.\n",
    "    obs, rewards, dones, infos = env.step(action={\"agent1\": 1, \"agent2\": 3})\n",
    "    env.render()\n",
    "\n",
    "    # Agent1 moves right, Agent2 moves left.\n",
    "    obs, rewards, dones, infos = env.step(action={\"agent1\": 1, \"agent2\": 3})\n",
    "    env.render()\n",
    "\n",
    "    # Agent1 moves down, Agent2 moves up.\n",
    "    obs, rewards, dones, infos = env.step(action={\"agent1\": 2, \"agent2\": 0})\n",
    "    env.render()\n",
    "\n",
    "    # Agent1 moves left, Agent2 moves up.\n",
    "    obs, rewards, dones, infos = env.step(action={\"agent1\": 3, \"agent2\": 0})\n",
    "    env.render()\n",
    "\n",
    "    # Agent1 moves down, Agent2 moves up.\n",
    "    obs, rewards, dones, infos = env.step(action={\"agent1\": 2, \"agent2\": 0})\n",
    "    env.render()\n",
    "\n",
    "\n",
    "print(\"Agent1's x/y position={}\".format(env.agent1_pos))\n",
    "print(\"Agent2's x/y position={}\".format(env.agent2_pos))\n",
    "print(\"Env timesteps={}\".format(env.timesteps))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Select an algorithm and instantiate a config object using that algorithm's config class <a class=\"anchor\" id=\"rllib_algo\"></a>\n",
    "\n",
    "<ol>\n",
    "    <li><a href=\"https://docs.ray.io/en/master/rllib/rllib-algorithms.html\">Open RLlib docs</a></li>\n",
    "    <li>Scroll down and click url of algo you're searching for, e.g. <i><b>PPO</b></i></li>\n",
    "    <li>On the <a href=\"\"https://docs.ray.io/en/master/rllib/rllib-algorithms.html#ppo>algo docs page </a>, click on the link <i><b>Implementation</b></i>.  This will open the <a href=\"https://github.com/ray-project/ray/blob/master/rllib/algorithms/ppo/ppo.py\">algo code file on github</a>.</li>\n",
    "    <li>Search the github code file for the word <i><b>config</b></i></li>\n",
    "    <li>Typically the docstring example will show: </li>\n",
    "    <ol>\n",
    "        <li>Example code implementing RLlib API, then </li>\n",
    "        <li>Example code implementing Ray Tune API.</li>\n",
    "    </ol>\n",
    "    <li>Scroll down to the config <b>__init()__</b> method</li>\n",
    "    <ol>\n",
    "            <li>Algorithm default hyperparameter values are here.</li>\n",
    "    </ol>\n",
    "    </ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<ray.rllib.algorithms.ppo.ppo.PPOConfig at 0x7fc0d6fed910>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Config is an object instead of a dictionary since Ray version >= 1.13.\n",
    "config = PPOConfig()\n",
    "\n",
    "# uncomment below to see the long list of specifically PPO default config values\n",
    "# print(pretty_print(PPOConfig().to_dict()))\n",
    "\n",
    "# Point the PPO to our new environment class.\n",
    "config.environment(env=MultiAgentArena)\n",
    "# Specify sampling behavior (use 4 workers to collect data in parallel, each one running through 1 environment copy).\n",
    "config.rollouts(num_rollout_workers=4, num_envs_per_worker=1)\n",
    "# Specify some training-related parameters.\n",
    "config.training(lr=0.00005, train_batch_size=4000)  # Default values for this algorithm.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How do we tell RLlib that we would like to train different behaviors for our 2 agents?\n",
    "<img src=\"images/multi_agent_setup.png\" width=\"70%\"></img>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<ray.rllib.algorithms.ppo.ppo.PPOConfig at 0x7fc0d6fed910>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Setup multi-agent mapping:\n",
    "\n",
    "# Environment provides M agent IDs.\n",
    "# RLlib has N policies (neural networks).\n",
    "# The `policy_mapping_fn` maps M agent IDs to N policies (M <= N).\n",
    "\n",
    "# If you don't provide a policy_mapping_fn, all agent IDs will map to \"default_policy\".\n",
    "config.multi_agent(\n",
    "    # Tell RLlib to create 2 policies with these IDs here:\n",
    "    policies=[\"policy1\", \"policy2\"],\n",
    "    # Tell RLlib to map agent1 to policy1 and agent2 to policy2.\n",
    "    policy_mapping_fn=lambda agent_id, episode, worker, **kwargs: \"policy1\" if agent_id == \"agent1\" else \"policy2\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a RL model using a multi-agent algorithm from RLlib <a class=\"anchor\" id=\"rllib_run\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-28 16:18:29,917\tWARNING deprecation.py:47 -- DeprecationWarning: `on_trainer_init(trainer, **kwargs)` has been deprecated. Use `on_algorithm_init(algorithm, **kwargs)` instead. This will raise an error in the future!\n",
      "2022-07-28 16:18:32,349\tINFO services.py:1477 -- View the Ray dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8269\u001b[39m\u001b[22m\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=59588)\u001b[0m 2022-07-28 16:18:43,459\tWARNING deprecation.py:47 -- DeprecationWarning: `on_trainer_init(trainer, **kwargs)` has been deprecated. Use `on_algorithm_init(algorithm, **kwargs)` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=59588)\u001b[0m 2022-07-28 16:18:43,460\tWARNING env.py:223 -- Your MultiAgentEnv <MultiAgentArena instance> does not have some or all of the needed base-class attributes! Make sure you call `super().__init__` from within your MutiAgentEnv's constructor. This will raise an error in the future.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=59589)\u001b[0m 2022-07-28 16:18:43,460\tWARNING deprecation.py:47 -- DeprecationWarning: `on_trainer_init(trainer, **kwargs)` has been deprecated. Use `on_algorithm_init(algorithm, **kwargs)` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=59590)\u001b[0m 2022-07-28 16:18:43,460\tWARNING deprecation.py:47 -- DeprecationWarning: `on_trainer_init(trainer, **kwargs)` has been deprecated. Use `on_algorithm_init(algorithm, **kwargs)` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=59591)\u001b[0m 2022-07-28 16:18:43,460\tWARNING deprecation.py:47 -- DeprecationWarning: `on_trainer_init(trainer, **kwargs)` has been deprecated. Use `on_algorithm_init(algorithm, **kwargs)` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=59588)\u001b[0m 2022-07-28 16:18:43,579\tWARNING deprecation.py:47 -- DeprecationWarning: `on_trainer_init(trainer, **kwargs)` has been deprecated. Use `on_algorithm_init(algorithm, **kwargs)` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=59589)\u001b[0m 2022-07-28 16:18:43,579\tWARNING deprecation.py:47 -- DeprecationWarning: `on_trainer_init(trainer, **kwargs)` has been deprecated. Use `on_algorithm_init(algorithm, **kwargs)` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=59590)\u001b[0m 2022-07-28 16:18:43,580\tWARNING deprecation.py:47 -- DeprecationWarning: `on_trainer_init(trainer, **kwargs)` has been deprecated. Use `on_algorithm_init(algorithm, **kwargs)` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=59591)\u001b[0m 2022-07-28 16:18:43,580\tWARNING deprecation.py:47 -- DeprecationWarning: `on_trainer_init(trainer, **kwargs)` has been deprecated. Use `on_algorithm_init(algorithm, **kwargs)` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=59588)\u001b[0m 2022-07-28 16:18:44,310\tWARNING deprecation.py:47 -- DeprecationWarning: `on_trainer_init(trainer, **kwargs)` has been deprecated. Use `on_algorithm_init(algorithm, **kwargs)` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=59589)\u001b[0m 2022-07-28 16:18:44,310\tWARNING deprecation.py:47 -- DeprecationWarning: `on_trainer_init(trainer, **kwargs)` has been deprecated. Use `on_algorithm_init(algorithm, **kwargs)` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=59590)\u001b[0m 2022-07-28 16:18:44,325\tWARNING deprecation.py:47 -- DeprecationWarning: `on_trainer_init(trainer, **kwargs)` has been deprecated. Use `on_algorithm_init(algorithm, **kwargs)` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=59591)\u001b[0m 2022-07-28 16:18:44,319\tWARNING deprecation.py:47 -- DeprecationWarning: `on_trainer_init(trainer, **kwargs)` has been deprecated. Use `on_algorithm_init(algorithm, **kwargs)` instead. This will raise an error in the future!\n",
      "2022-07-28 16:18:44,959\tWARNING deprecation.py:47 -- DeprecationWarning: `on_trainer_init(trainer, **kwargs)` has been deprecated. Use `on_algorithm_init(algorithm, **kwargs)` instead. This will raise an error in the future!\n",
      "2022-07-28 16:18:45,080\tWARNING deprecation.py:47 -- DeprecationWarning: `on_trainer_init(trainer, **kwargs)` has been deprecated. Use `on_algorithm_init(algorithm, **kwargs)` instead. This will raise an error in the future!\n",
      "2022-07-28 16:18:45,781\tWARNING deprecation.py:47 -- DeprecationWarning: `on_trainer_init(trainer, **kwargs)` has been deprecated. Use `on_algorithm_init(algorithm, **kwargs)` instead. This will raise an error in the future!\n",
      "2022-07-28 16:18:46,411\tINFO trainable.py:160 -- Trainable.setup took 16.499 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n",
      "2022-07-28 16:18:46,412\tWARNING util.py:65 -- Install gputil for GPU system monitoring.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=59588)\u001b[0m 2022-07-28 16:18:46,840\tWARNING deprecation.py:47 -- DeprecationWarning: `concat_samples` has been deprecated. Use `concat_samples() from rllib.policy.sample_batch` instead. This will raise an error in the future!\n",
      "2022-07-28 16:18:48,037\tWARNING deprecation.py:47 -- DeprecationWarning: `concat_samples` has been deprecated. Use `concat_samples() from rllib.policy.sample_batch` instead. This will raise an error in the future!\n",
      "2022-07-28 16:18:48,048\tWARNING deprecation.py:47 -- DeprecationWarning: `_get_slice_indices` has been deprecated. This will raise an error in the future!\n",
      "2022-07-28 16:18:48,049\tWARNING deprecation.py:47 -- DeprecationWarning: `slice` has been deprecated. Use `SampleBatch[start:stop]` instead. This will raise an error in the future!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sum of rewards for both agents R=-13.65750000000001\n",
      "Agent1 R=-4.125\n",
      "Agent2 R=-9.532499999999981\n",
      "\n",
      "Sum of rewards for both agents R=-7.7475\n",
      "Agent1 R=1.125\n",
      "Agent2 R=-8.872499999999983\n",
      "\n",
      "Sum of rewards for both agents R=-4.001999999999992\n",
      "Agent1 R=4.535\n",
      "Agent2 R=-8.536999999999985\n",
      "\n",
      "Sum of rewards for both agents R=-0.2579999999999895\n",
      "Agent1 R=7.74\n",
      "Agent2 R=-7.997999999999985\n",
      "\n",
      "Sum of rewards for both agents R=0.5130000000000075\n",
      "Agent1 R=8.445\n",
      "Agent2 R=-7.931999999999984\n",
      "\n",
      "Sum of rewards for both agents R=1.1460000000000072\n",
      "Agent1 R=8.825\n",
      "Agent2 R=-7.678999999999985\n",
      "\n",
      "Sum of rewards for both agents R=0.4140000000000116\n",
      "Agent1 R=7.895\n",
      "Agent2 R=-7.480999999999986\n",
      "\n",
      "Sum of rewards for both agents R=1.5750000000000077\n",
      "Agent1 R=8.88\n",
      "Agent2 R=-7.304999999999987\n",
      "\n",
      "Sum of rewards for both agents R=1.8690000000000087\n",
      "Agent1 R=9.185\n",
      "Agent2 R=-7.3159999999999865\n",
      "\n",
      "Sum of rewards for both agents R=4.185000000000005\n",
      "Agent1 R=11.49\n",
      "Agent2 R=-7.304999999999986\n",
      "\n",
      "Training over 9 iterations completed.\n"
     ]
    }
   ],
   "source": [
    "# Use the config object's `build()` method for generating\n",
    "# an RLlib Algorithm instance that we can then train.\n",
    "ppo = config.build()\n",
    "\n",
    "# Train the PPO Algorithm instance.\n",
    "for i in range(10):\n",
    "    # Call its `train()` method.\n",
    "    result = ppo.train()\n",
    "    \n",
    "    print(f\"Sum of rewards for both agents R={result['episode_reward_mean']}\")\n",
    "    print(f\"Agent1 R={result['policy_reward_mean']['policy1']}\")\n",
    "    print(f\"Agent2 R={result['policy_reward_mean']['policy2']}\")\n",
    "    print()\n",
    "\n",
    "print(f\"Training over {i} iterations completed.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Note</b> that in an adversarial multi-agent setup, an agent benefits from the other agent's failures and vice-versa: agents get harmed more (receive negative rewards) the better the other agent is doing.\n",
    "    <br/>\n",
    "This highlights some important aspects of multi-agent training:\n",
    "    <br/>\n",
    "<ul>\n",
    "<li>From each agent's perspective, the environment is not as static as in respective single-agent scenarios (the other agent's behavior is probably harder to predict than the environment's own inherent dynamics/physics).</li>\n",
    "<li>As one agent learns how to behave more intelligently, the other agent has to counter this new behavior of its opponent and become smarter as well, asoasf.</li>\n",
    "    </ul>\n",
    "    </div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To stop the Algorithm and release its blocked resources, use:\n",
    "ppo.stop()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary\n",
    "\n",
    "In this notebook, we have learnt, how to:\n",
    "\n",
    "* Write our own custom multi-agent environment using RLlib's MultiAgentEnv superclass\n",
    "* Quickly test the environment using observation, reward, and action-dictionaries\n",
    "* Plug in the new environment into an RLlib Algorithm config and setup proper agentID to policyID mapping\n",
    "* Run a quick (manual) training loop w/o using Ray Tune"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Homework\n",
    "\n",
    "#### Run one episode of our multi-agent arena and render the episode while it plays out\n",
    "\n",
    " <img src=\"images/exercise_env_loop.png\" width=500>\n",
    " \n",
    "We already learned how to use an environment's `reset()` and `step()` calls to walk through a single agent environment: Call `reset()` once, continue using the returned observations to compute actions, pass these actions into consecutive calls to `step()` and stop when `step()` returns the `done=True` flag.\n",
    "\n",
    "Let's do the same thing now in the multi-agent setting, using our `MultiAgentArena` class.\n",
    "Remember that everything, from observations, over rewards, dones, and actions now become dictionaries mapping agent IDs\n",
    "(in our case \"agent1\" and \"agent2\") to the individual agent's observation, reward, action, and done information.\n",
    "\n",
    "Follow these instructions here to get this done:\n",
    "\n",
    "1. `reset` the already created (variable `env`) environment to get the first (initial) observations for \"agent1\" and \"agent2\".\n",
    "1. Enter an infinite while loop, in which you ..\n",
    "1. .. compute the actions for \"agent1\" and \"agent2\" (using random sampling).\n",
    "1. .. put the results of the action computations into an action dict (`{\"agent1\": [action1], \"agent2\": [action2]}`).\n",
    "1. .. pass this action dict into the env's `step()` method.\n",
    "1. .. check the returned `dones` dict for True (yes, episode is terminated) and if True, break out of the loop. Note here that you may also check the `dones` dict for the special \"__all__\" key and if `dones['__all__'] is True` -> the episode has ended for all agents.\n",
    "\n",
    "**Good luck! :)**\n",
    "\n",
    "\n",
    "Write your solution code into this following python cell here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from ipywidgets import Output\n",
    "from IPython import display\n",
    "import time\n",
    "\n",
    "\n",
    "# Leave the following as-is. It'll help us with rendering the env in this very cell's output.,\n",
    "out = Output()\n",
    "display.display(out)\n",
    "\n",
    "with out:\n",
    "\n",
    "    # Start coding here inside this `with`-block:\n",
    "    # 1) Reset the env ...\n",
    "\n",
    "    # 2) Enter an infinite while loop (in order to step through one episode) ...\n",
    "\n",
    "        # 3) Calculate both agents' actions individually, using random sampling with the \n",
    "        # action space: e.g. `a1 = env.action_space.sample()`.\n",
    "\n",
    "        # 4) Compile the actions dict from both individual agents' actions ...\n",
    "\n",
    "        # 5) Send the actions dict to the env's `step()` method to receive: obs, rewards, dones, info dicts ...\n",
    "\n",
    "        # 6) We'll do this together: Render the env.\n",
    "        # Don't write any code here (skip directly to 7).\n",
    "        out.clear_output(wait=True),\n",
    "        time.sleep(0.08),\n",
    "        env.render(),\n",
    "\n",
    "        # 7) Check, whether the episde is done (take a look at the\n",
    "        # `dones` dict returned from `step()`)\n",
    "        # If dones[\"__all__\"], break out of the while loop we entered in step 2).\n",
    "\n",
    "\n",
    "# 8) Run it! :)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## References\n",
    "\n",
    " * [Hands-on RL with Ray‚Äôs RLlib](https://github.com/sven1977/rllib_tutorials/tree/main/ray_summit_2021)\n",
    " * [Multi-agent environments in RLlib](https://docs.ray.io/en/latest/rllib/rllib-env.html#multi-agent-and-hierarchical)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚¨ÖÔ∏è [Previous notebook](./ex_01_intro_gym_and_rllib.ipynb) <br>\n",
    "‚û°Ô∏è [Next notebook](./ex_03_train_tune_rllib_model.ipynb) <br>\n",
    "\n",
    "üìñ [Back to Table of Contents](./ex_00_rllib_notebooks_table_of_contents.ipynb)<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
