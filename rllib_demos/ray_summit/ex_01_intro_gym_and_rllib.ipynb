{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "710f04f4",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Notebook 01. Introduction to OpenAI Gym and RLlib\n",
    "\n",
    "¬© 2019-2022, Anyscale. All Rights Reserved<br>\n",
    "üìñ [Back to Table of Contents](./ex_00_rllib_notebooks_table_of_contents.ipynb) <br>\n",
    "‚û°Ô∏è [Next notebook](./ex_02_create_multiagent_rllib_env.ipynb) <br>\n",
    "\n",
    "### Learning objectives\n",
    "In this this tutorial, you will learn:\n",
    " * [What is an Environment in Reinforcement Learning (RL)?](#intro_env)\n",
    " * [Overview of RL terminology](#intro_rl)\n",
    " * [Introduction to OpenAI Gym environments](#intro_gym)\n",
    " * [High-level OpenAI Gym API calls](#intro_gym_api)\n",
    " * [Overview of RLlib](#intro_rllib)\n",
    " * [Train a policy using an algorithm from RLlib](#intro_rllib_api)\n",
    " * [Evaluate a RLlib policy](#eval_rllib)\n",
    " * [Reload RLlib model from checkpoint and run inference](#reload_rllib)\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41aca4e8",
   "metadata": {},
   "source": [
    "## What is an environment in RL? <a class=\"anchor\" id=\"intro_env\"></a>\n",
    "\n",
    "Solving a problem in RL begins with an **environment**. In the simplest definition of RL:\n",
    "\n",
    "> An **agent** interacts with an **environment** and receives a reward.\n",
    "\n",
    "An environment in RL is the agent's world, it is a simulation of the problem to be solved. \n",
    "\n",
    "<img src=\"./images/env_key_concept1.png\" width=\"50%\" />\n",
    "\n",
    "The **environment** simulator might be of a:\n",
    "<ul>\n",
    "    <li>real, physical machine such as a gas turbine or autonomous vehicle</li>\n",
    "    <li>real, abstract system such as user behavior on a website or the stock market</li>\n",
    "    <li>virtual sytem on a computer such as a board game or a video game</li>\n",
    "    </ul>\n",
    "    \n",
    "The **agent** represents what is triggering the actions.  For example it could be:\n",
    "<ul>\n",
    "    <li>a software system that is triggering actions for machines</li>\n",
    "    <li>a type of user or investor</li>\n",
    "    <li>a game player or game system that is competing against real players </li>\n",
    "    </ul> \n",
    "    \n",
    "\n",
    "<i>Comparison of RL to supervised learning</i> <br>\n",
    "<ul>\n",
    "    <li><i>Data</i>.  In supervised learning, you start with a labeled dataset.  In contrast, the <b>data in RL is not given up front; the environment acts as a data generator</b>.  One can also do RL on a pre-collected dataset (called offline RL), we will touch on offline RL later. </li>\n",
    "    <li><i>Training</i>.  Traditional supervised learning views the world as more of a one-shot training, not as action -> fedback -> improved action -> repeat. </li>\n",
    "    </ul>\n",
    "\n",
    "<b>Why bother with an Agent, Environment, and RL?</b>  <br>\n",
    "<div class=\"alert alert-block alert-success\">    \n",
    "<b>üí° Reinforcement learning (RL) is useful when you have sequential decisions that need to be optimized over time.</b> \n",
    "</div>\n",
    "\n",
    "<br> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecf844e4",
   "metadata": {},
   "source": [
    "## Overview of RL terminology <a class=\"anchor\" id=\"intro_rl\"></a>\n",
    "\n",
    "An RL environment consists of: \n",
    "\n",
    "1. all possible actions (**action space**)\n",
    "2. a complete description of the environment, nothing hidden (**state space**)\n",
    "3. an observation by the agent of certain parts of the state (**observation space**)\n",
    "4. **reward**, which is the only feedback the agent receives after each action.\n",
    "\n",
    "The model that tries to maximize the expected sum over all future rewards is called a **policy**. The policy is a function mapping the environment's observations to an action to take, usually written **œÄ** (s(t)) -> a(t).  <i>In deep reinforcement learning, this function is a neural network</i>.\n",
    "\n",
    "<b>Policy vs Model? </b>\n",
    "In traditional supervised learning, model means a¬†trained algorithm, or a learned function.\n",
    "\n",
    "> <i>In RL, a model is roughly equivalent to a policy, but policy is more specific</i> because it is trained to act in a specific environment.  For deployment, we use the word \"model\" because more people understand the ML meaning of a trained model.\n",
    "\n",
    "Below is a high-level image of how the Agent and Environment work together to train a Policy model in a RL simulation feedback loop in RLlib.\n",
    "\n",
    "<img src=\"./images/env_key_concept2.png\" width=\"98%\" />\n",
    "\n",
    "The **RL simulation feedback loop** repeatedly collects data, for one (single-agent case) or multiple (multi-agent case) policies, trains the policies on these collected data, and makes sure the policies' weights are kept in synch. \n",
    "\n",
    "During simulation loops, the environment collects observations, taken actions, receives rewards and so-called **done** flags, indicating the boundaries of different episodes the agents play through in the simulation.\n",
    "\n",
    "Each simulation iteration is called a <b>time step</b>.  The simulation iterations of action -> reward -> next state -> train -> repeat, until the end state, is called an **episode**, or in RLlib, a **rollout**.  \n",
    "> üëâ Each episode consists of one or many time steps.\n",
    "\n",
    "<b>Per episode</b> (or between **done** flag == True), the RL simulation feedback loop repeats up to some specified end state (termination state or timesteps). Examples of termination are:\n",
    "<ul>\n",
    "    <li>the end of a maze (termination state)</li>  \n",
    "    <li>the player died in a game (termination state)</li>\n",
    "    <li>after 60 videos watched in a recommender system (timesteps).</li>\n",
    "    </ul>\n",
    "    \n",
    "<b>Why train for many episodes?</b>  When you are doing machine learning, you do not just do something once and report the result.  You do it many times, to make sure you did not just get \"lucky\" one time, and you report typically the average result over all the trials.  RL is similar.  \n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>üí° In RL, the policy is trained by repeating trials, or episodes (or rollouts), then reporting the calculated reward typically as an average of all achieved rewards.</b> \n",
    "</div>\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07193fc6",
   "metadata": {},
   "source": [
    "## Introduction to OpenAI Gym example: frozen lake <a class=\"anchor\" id=\"intro_gym\"></a>\n",
    "\n",
    "[OpenAI Gym](https://gym.openai.com/) is a well-known reference library of RL environments. \n",
    "\n",
    "#### 1. import gym\n",
    "\n",
    "Below is how you would import gym and view all available environments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "742e5847",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gym: 0.21.0\n",
      "Num Gym Environments: 103\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[EnvSpec(FrozenLake-v1), EnvSpec(FrozenLake8x8-v1)]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import libraries\n",
    "import gym\n",
    "print(f\"gym: {gym.__version__}\")\n",
    "\n",
    "# List all available gym environments\n",
    "all_env  =  list(gym.envs.registry.all())\n",
    "print(f'Num Gym Environments: {len(all_env)}')\n",
    "\n",
    "# You could loop through and list all environments if you wanted\n",
    "# [print(e) for e in all_env]\n",
    "envs_starting_with_f = [e for e in all_env if str(e).startswith(\"EnvSpec(Frozen\")]\n",
    "envs_starting_with_f"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94ddd364",
   "metadata": {},
   "source": [
    "#### 2. Instatiate your Gym object\n",
    "\n",
    "The way you instantiate a Gym environment is with the **make()** function.\n",
    "\n",
    "The .make() function takes arguments:\n",
    "- **name of the Gym environment**, type: str, Required.\n",
    "- **runtime parameter values**, Optional.\n",
    "\n",
    "For the required string argument, you need to know the Gym name.  You can find the Gym name in the Gym documentation for environments, either:\n",
    "<ol>\n",
    "    <li>The doc page in <a href=\"https://www.gymlibrary.ml/environments/toy_text/frozen_lake/\">Gym's website</a></li>\n",
    "    <li>The environment's <a href=\"https://github.com/openai/gym/blob/master/gym/envs/toy_text/frozen_lake.py\">source code </a></li>\n",
    "    <li>\n",
    "        <a href=\"https://www.gymlibrary.ml/environments/classic_control/cart_pole/#description\">Research paper (if one exists)</a> referenced in the environment page </li>\n",
    "    </ol>\n",
    "    \n",
    "Below is an example of how to create a basic Gym environment, [frozen lake](https://www.gymlibrary.ml/environments/toy_text/frozen_lake/).  We can see below that the termination condition of an episode will be time steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b00d01a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: <TimeLimit<FrozenLakeEnv<FrozenLake-v1>>>\n",
      "env_spec: EnvSpec(FrozenLake-v1)\n"
     ]
    }
   ],
   "source": [
    "env_name = \"FrozenLake-v1\"\n",
    "\n",
    "# Instantiate gym env object with a runtime parameter value (is_slippery).\n",
    "# is_slippery specifies if the environment is deterministic or stochastic\n",
    "env = gym.make(\n",
    "    env_name,\n",
    "    is_slippery=False,  # whether the grid-world behaves deterministically or not\n",
    ")\n",
    "\n",
    "# inspect the gym spec for the environment\n",
    "print(f\"env: {env}\")\n",
    "env_spec = env.spec\n",
    "print(f\"env_spec: {env_spec}\")\n",
    "\n",
    "# Note: \"TimeLimit\" means termination condition for an episode will be time steps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "783c1e21",
   "metadata": {},
   "source": [
    "#### 3. Inspect the environment action and observations spaces\n",
    "\n",
    "Gym Environments can be deterministic or stochastic.\n",
    "\n",
    "<ul>\n",
    "    <li>\n",
    "        <b>Deterministic</b> if the current state + selected action determines the next state of the environment.  <i>Chess is an example of a deterministic environment</i>, since all possible states/action combinations can be described as a discrete set of rules with states bounded by the pieces and size of the board.</li>\n",
    "    <li>\n",
    "        <b>Stochastic</b> if the policy output action is a probability distribution over a set of possible actions at time step t. In this case, the agent needs to compute its action from the policy in two steps. i) sample actions from the policy according to the probability distribution, ii) compute log likelihoods of the actions. <i>Random visitors to a website is an example of a stochastic environment</i>. </li>\n",
    "    </ul>\n",
    "\n",
    "<b>Gym actions.</b> The action_space describes the numerical structure of the legitimate actions that can be applied to the environment. \n",
    "\n",
    "For example, if we have 4 possible discrete actions, we could encode them as:\n",
    "<ul>\n",
    "    <li>0: LEFT</li>\n",
    "    <li>1: DOWN</li>\n",
    "    <li>2: RIGHT</li>\n",
    "    <li>3: UP</li>\n",
    "</ul>\n",
    "\n",
    "<b>Gym observations.</b>  The observation_space defines the structure as well as the legitimate values for the observation of a state of the environment.  \n",
    "\n",
    "For example, if we have a 4x4 grid, we could encode them as {0,1,2,3, 4, ‚Ä¶ ,15} for grid positions ((0,0), (0,1), (0,2), (0,3), ‚Ä¶. (3,3)).\n",
    "\n",
    "\n",
    "From the Gym [documentation](https://www.gymlibrary.ml/environments/toy_text/frozen_lake/) about the frozen lake environment, we see: <br>\n",
    "\n",
    "|Frozen Lake      | Gym space   |\n",
    "|---------------- | ----------- |\n",
    "|Action Space     | Discrete(4) |\n",
    "|Observation Space| Discrete(16)|\n",
    "\n",
    "\n",
    " \n",
    "<b><a href=\"https://github.com/openai/gym/tree/master/gym/spaces\">Gym spaces</a></b> are gym data types.  The main types are `Discrete` for discrete numbers and `Box` for continuous numbers.  \n",
    "\n",
    "Gym Space `Discrete` elements are Python type `int`, and Gym Space `Box` are Python type `float32`.\n",
    "\n",
    "Below is an example how to inspect the environment action and observations spaces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "78620a2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a gym environment.\n",
      "\n",
      "gym action space: Discrete(4)\n",
      "gym observation space: Discrete(16)\n"
     ]
    }
   ],
   "source": [
    "# check if it is a gym instance\n",
    "if isinstance(env, gym.Env):\n",
    "    print(\"This is a gym environment.\")\n",
    "    print()\n",
    "\n",
    "    # print gym Spaces\n",
    "    if isinstance(env.action_space, gym.spaces.Space):\n",
    "        print(f\"gym action space: {env.action_space}\")\n",
    "    if isinstance(env.observation_space, gym.spaces.Space):\n",
    "        print(f\"gym observation space: {env.observation_space}\") \n",
    "        \n",
    "# Note: the action space is discrete with 4 possible actions.\n",
    "# Note: the observation space is 4x4 and thus runs from 0 to 15."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27ad91e6",
   "metadata": {},
   "source": [
    "#### 4. Inspect gym environment default & runtime parameters\n",
    "\n",
    "Gym environments contain 2 sets of parameters that are set after the environment object is instantiated.\n",
    "<ul>\n",
    "    <li><b>Default parameters</b> are fixed in the Gym environment code itself.</li>\n",
    "    <li><b>Runtime parameters</b> are passed into the make() function as **kwargs.</li>\n",
    "    </ul>\n",
    "\n",
    "Below is an example of how to inspect the environment parameters.  Notice we can tell from the parameters that our frozen lake environment is: <br>\n",
    "1) <i>Deterministic</i>, and <br>\n",
    "2) Episode terminates with time step condition <i>max_episode_steps</i> = 100."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f4dc0275",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Default spec params...\n",
      "id: FrozenLake-v1\n",
      "reward_threshold: 0.7\n",
      "nondeterministic: False\n",
      "max_episode_steps: 100\n",
      "order_enforce: True\n",
      "\n",
      "Runtime spec params...\n",
      "map_name: 4x4\n",
      "is_slippery: False\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# inspect env.spec parameters\n",
    " \n",
    "# View default env spec params that are hard-coded in Gym code itself\n",
    "# Default parameters are fixed\n",
    "print(\"Default spec params...\")\n",
    "print(f\"id: {env_spec.id}\")\n",
    "# rewards above this value considered \"success\"\n",
    "print(f\"reward_threshold: {env_spec.reward_threshold}\")\n",
    "# env is deterministic or stochastic\n",
    "print(f\"nondeterministic: {env_spec.nondeterministic}\")\n",
    "# number of time steps per episode\n",
    "print(f\"max_episode_steps: {env_spec.max_episode_steps}\")\n",
    "# must reset before step or render\n",
    "print(f\"order_enforce: {env_spec.order_enforce}\") \n",
    "\n",
    "# View runtime **kwargs .spec params.  These params set after env instantiated.\n",
    "# print(f\"type(env_spec._kwargs): {type(env_spec._kwargs)}\") #dict\n",
    "print()\n",
    "print(\"Runtime spec params...\")\n",
    "[print(f\"{k}: {v}\") for k,v in env_spec._kwargs.items()]\n",
    "print()\n",
    "\n",
    "# Note:  We can tell that our frozen lake environment is: \n",
    "# 1) Success criteria is rewards >= 0.7\n",
    "# 2) Deterministic\n",
    "# 3) Episode terminates when number time_steps = 100\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2dd5020",
   "metadata": {
    "tags": []
   },
   "source": [
    "## High-level OpenAI Gym API calls <a class=\"anchor\" id=\"intro_gym_api\"></a>\n",
    "\n",
    "The most basic Gym API methods are:\n",
    "<ul>\n",
    "    <li><b>env.reset()</b> <br>Reset the environment to an initial state.  You should call this method every time at the start of a new episode.</li>\n",
    "    <li><b>env.render()</b>  <br>Visually inspect the environment. This is for human/debugging purposes; it is not seen by the agent/algorithm.  Note you cannot inspect an environment before it has been initialized with env.reset().</li>\n",
    "    <li><b>env.step(action)</b> <br>Take an action from the possible action space values.  It takes an action as input, computes the state of the environment after applying that action and returns the 4-tuple (observation, reward, done, info).</li>\n",
    "    <li><b>env.close()</b> <br>Close an environment.</li>\n",
    "    </ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3bd2bdc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n"
     ]
    }
   ],
   "source": [
    "# Print the starting observation.  \n",
    "# Recall possible observations are between 4x4 grid.\n",
    "print(env.reset())\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aee81cf9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "obs: 1, reward: 0.0, done: False\n",
      "  (Right)\n",
      "S\u001b[41mF\u001b[0mFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "obs: 5, reward: 0.0, done: True\n",
      "  (Down)\n",
      "SFFF\n",
      "F\u001b[41mH\u001b[0mFH\n",
      "FFFH\n",
      "HFFG\n"
     ]
    }
   ],
   "source": [
    "# Take an action\n",
    "# Recall the possible actions are: 0: LEFT, 1: DOWN, 2: RIGHT, 3: UP\n",
    "\n",
    "new_obs, reward, done, _ = env.step(2) #Right\n",
    "print(f\"obs: {new_obs}, reward: {reward}, done: {done}\")\n",
    "env.render()\n",
    "new_obs, reward, done, _ = env.step(1) #Down\n",
    "print(f\"obs: {new_obs}, reward: {reward}, done: {done}\")\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3afec105",
   "metadata": {},
   "source": [
    "We can also try to run an action in the frozen lake environment which is outside the defined number range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0620a168",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comment this cell if you want whole notebook to run without errors\n",
    "\n",
    "# Try to take an invalid action\n",
    "\n",
    "#env.step(4) # invalid\n",
    "\n",
    "# should see KeyError below"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7e9c090-0c94-47b7-8e7d-a54af12c446f",
   "metadata": {},
   "source": [
    "To test out your environment, typically you will loop through a few episodes to make sure it works.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4b8a6854",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e7555d4a2a34befb43d0e5f75fbf560",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from ipywidgets import Output\n",
    "from IPython import display\n",
    "import time\n",
    "\n",
    "# The following three lines are for rendering purposes only.\n",
    "# They allow us to render the env frame-by-frame in-place\n",
    "# (w/o creating a huge output which we would then have to scroll through).\n",
    "out = Output()\n",
    "display.display(out)\n",
    "with out:\n",
    "\n",
    "    # Putting the simple API methods together.\n",
    "    # Here is a pattern for running a bunch of episodes.\n",
    "    num_episodes = 5 # Number of episodes you want to run the agent\n",
    "    total_reward = 0.0  # Initialize reward to 0\n",
    "\n",
    "    # Loop through episodes\n",
    "    for ep in range(num_episodes):\n",
    "\n",
    "        # Reset the environment at the start of each episode\n",
    "        obs = env.reset()\n",
    "        done = False\n",
    "\n",
    "        # Loop through time steps per episode\n",
    "        while True:\n",
    "            # take random action, but you can also do something more intelligent \n",
    "            action = env.action_space.sample()\n",
    "\n",
    "            # apply the action\n",
    "            new_obs, reward, done, info = env.step(action)\n",
    "            total_reward += reward\n",
    "\n",
    "            # If the epsiode is up, then start another one\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "            # Render the env (in place).\n",
    "            time.sleep(0.3)\n",
    "            out.clear_output(wait=True)\n",
    "            print(f\"episode: {ep}\")\n",
    "            print(f\"obs: {new_obs}, reward: {total_reward}, done: {done}\")\n",
    "            env.render()\n",
    "\n",
    "# Close the env\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "147f1156",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Overview of RLlib <a class=\"anchor\" id=\"intro_rllib\"></a>\n",
    "\n",
    "<img width=\"7%\" src=\"./images/rllib-logo.png\"> is the most comprehensive open-source Reinforcement Learning framework. **[RLlib](https://github.com/ray-project/ray/tree/master/rllib)** is <b>distributed by default</b> since it is built on top of **[Ray](https://docs.ray.io/en/latest/)**, an easy-to-use, open-source, distributed computing framework for Python that can handle complex, heterogeneous applications. Ray and RLlib run on compute clusters on any cloud without vendor lock.\n",
    "\n",
    "RLlib includes <b>25+ available [algorithms](https://docs.ray.io/en/master/rllib/rllib-algorithms.html)</b>, converted to both <img width=\"3%\" src=\"./images/tensorflow-logo.png\">_TensorFlow_ and <img width=\"3%\" src=\"./images/pytorch-logo.png\">_PyTorch_, covering different sub-categories of RL: _model-free_, _offline RL_, _model-based_, and _gradient-free_.¬†Almost any RLlib algorithm can learn in a <b>multi-agent</b> setting.¬†Many algorithms support <b>RNNs</b> and <b>LSTMs</b>.\n",
    "\n",
    "On a very high level, RLlib is organized by **environments**, **algorithms**, **examples**, **tuned_examples**, and **models**.  \n",
    "\n",
    "    ray\n",
    "    |- rllib\n",
    "    |  |- env \n",
    "    |  |- algorithms\n",
    "    |  |  |- alpha_zero \n",
    "    |  |  |- appo \n",
    "    |  |  |- ppo \n",
    "    |  |  |- ... \n",
    "    |  |- examples \n",
    "    |  |- tuned_examples\n",
    "    |  |- models\n",
    "\n",
    "Within **_env_** you will find different [base classes](https://docs.ray.io/en/latest/rllib/package_ref/env.html) that you can inherit from to make it easy to implement your environment. RLlib supports environments created using the **OpenAI Gym API** (which supports most user cases). The base classes in the env directory allow for users to implement environments that are not covered by OpenAI Gym, such as multi agent environments or environments that have strict performance or hosting requirements. In the next notebook, we will use the **RLlib MultiAgentEnv** base class to train a **multi agent** RL model.\n",
    "\n",
    "Within **_examples_** you will find some examples of common custom rllib use cases.  \n",
    "\n",
    "Within **_tuned\\_examples_**, you will find, sorted by algorithm, suggested hyperparameter value choices within .yaml files. Ray¬†RLlib team ran simulations/benchmarks to find suggested hyperparameter value choices.¬†¬†These¬†files are used¬†for daily testing, and weekly hard-task testing to make sure they all run at speed,¬†for both TF and Torch.¬†Helps give you a leg-up with initial parameter choices!\n",
    "\n",
    "Within **_models_**, you will find advanced building blocks for customizing the specifics that are used by an algorithm to produce a policy that outputs actions.  In case the model architecture is a neural network, building blocks are given in either <img width=\"3%\" src=\"./images/tensorflow-logo.png\">_TensorFlow_, <img width=\"3%\" src=\"./images/pytorch-logo.png\">_PyTorch_, or both.  For example, the building blocks for DNN, CNN, RNN, LSTM are here. \n",
    "\n",
    "In this tutorial, we will mainly focus on the **_algorithms_** package, where we will find RLlib algos to train policy models on environments.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6a23493-80b1-4032-a98f-f14827026501",
   "metadata": {},
   "source": [
    "## Train a policy using an algorithm from RLlib <a class=\"anchor\" id=\"intro_rllib_api\"></a>\n",
    "\n",
    "Once you have an environment, next you need to decide which RL algorithm to use.\n",
    "\n",
    "#### Step 1.  Import ray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5655b3c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ray: 3.0.0.dev0\n"
     ]
    }
   ],
   "source": [
    "# import libraries\n",
    "import ray\n",
    "from ray import tune\n",
    "from ray.tune.logger import pretty_print\n",
    "print(f\"ray: {ray.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "380d2983-90fd-48bd-b61a-1ee79b62770f",
   "metadata": {},
   "source": [
    "#### Step 2. Check environment for errors   \n",
    "\n",
    "Before you start training, it is a good idea to check the environment for errors.  RLlib provides a convenient [Environment pre-check function](https://github.com/ray-project/ray/blob/master/rllib/utils/pre_checks/env.py#L22) for this.  It checks that the environment is compatible with OpenAI Gym and RLlib (and outputs a warning if necessary).\n",
    "\n",
    "We will start with a new environment, Cart-Pole.  Take a look at the Gym documentation:\n",
    "<ol>\n",
    "    <li>The doc page in <a href=\"https://www.gymlibrary.ml/environments/classic_control/cart_pole/\">Gym's website</a></li>\n",
    "    <li>The environment's <a href=\"https://github.com/openai/gym/blob/master/gym/envs/classic_control/cartpole.py\">source code </a></li>\n",
    "    <li>\n",
    "        <a href=\"https://www.gymlibrary.ml/environments/classic_control/cart_pole/#description\">Research paper (if one exists)</a> referenced in the environment page </li>\n",
    "    </ol>\n",
    "\n",
    "Below, we start a new Cart Pole environment, then in the next cell, check it for errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9ddfdb23-2554-48cf-aeed-cdc1b19f02fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: <TimeLimit<CartPoleEnv<CartPole-v1>>>\n",
      "env_spec: EnvSpec(CartPole-v1)\n",
      "\n",
      "Environment parameters...\n",
      "_env_name: CartPole\n",
      "_kwargs: {}\n",
      "entry_point: gym.envs.classic_control:CartPoleEnv\n",
      "id: CartPole-v1\n",
      "max_episode_steps: 500\n",
      "nondeterministic: false\n",
      "order_enforce: true\n",
      "reward_threshold: 475.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Instantiate gym env object with a runtime parameter value\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "\n",
    "# inspect the gym spec for the environment\n",
    "print(f\"env: {env}\")\n",
    "env_spec = env.spec\n",
    "print(f\"env_spec: {env_spec}\")\n",
    "\n",
    "# inspect gym env.spec parameters\n",
    "print()\n",
    "print(\"Environment parameters...\")\n",
    "print(pretty_print(vars(env_spec)))\n",
    "\n",
    "# Note: \"TimeLimit\" means termination condition for an episode will be time steps\n",
    "# Note:  We can tell that our CartPole environment is: \n",
    "# 1) Success criteria is rewards >= 475\n",
    "# 2) Deterministic\n",
    "# 3) Episode terminates when number time_steps = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "99b4730c-ad46-42c9-ae70-ef1f551883b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checking environment ...\n",
      "All checks passed. No errors found.\n"
     ]
    }
   ],
   "source": [
    "from ray.rllib.utils.pre_checks.env import check_env\n",
    "\n",
    "# How to check you do not have any environment errors\n",
    "print(\"checking environment ...\")\n",
    "try:\n",
    "    check_env(env)\n",
    "    print(\"All checks passed. No errors found.\")\n",
    "except:\n",
    "    print(\"failed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53918a3d-3c4f-4d89-8d22-4ff449cef6c7",
   "metadata": {},
   "source": [
    "<b>Get an environment baseline</b>\n",
    "\n",
    "Let's run through the environment, without rendering, and record the mean reward.  The purpose of this is to obtain a baseline before training a RLlib algorithm.\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "üí° If you are doing benchmarks, this random policy is often called a <b>\"baseline\".</b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "de621b2d-387d-4366-90e5-d47736d1587b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "**************\n",
      "Baseline mean_reward: 22.32 out of success: 475.0 after 60000 episodes\n",
      "**************\n"
     ]
    }
   ],
   "source": [
    "# Putting the simple API methods together.\n",
    "# Here is a pattern for running a bunch of episodes.\n",
    "num_episodes = 60000 # Number of episodes you want to run the agent\n",
    "total_reward = 0.0  # Initialize reward to 0\n",
    "\n",
    "# Loop through episodes\n",
    "for ep in range(num_episodes):\n",
    "\n",
    "    # Reset the environment at the start of each episode\n",
    "    obs = env.reset()\n",
    "    done = False\n",
    "    \n",
    "    # Loop through time steps per episode\n",
    "    while True:\n",
    "        # take random action, but you can also do something more intelligent \n",
    "        action = env.action_space.sample()\n",
    "\n",
    "        # apply the action\n",
    "        new_obs, reward, done, info = env.step(action)\n",
    "        total_reward += reward\n",
    "\n",
    "        # If the epsiode is up, then start another one\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "# calculate mean_reward\n",
    "print()\n",
    "print(\"**************\")\n",
    "mean_reward = total_reward / num_episodes\n",
    "print(f\"Baseline mean_reward: {mean_reward:.2f} out of success: {env_spec.reward_threshold} after {num_episodes} episodes\")\n",
    "print(\"**************\")\n",
    "        \n",
    "# Close the env\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4a757e7",
   "metadata": {},
   "source": [
    "#### Step 3.  Select an algorithm and find that algorithm's config class  \n",
    "\n",
    "There are many factors to consider when selecting which algorithm to use on your environment.  Following are some high-level best practices.\n",
    "<ol>\n",
    "    <li>\n",
    "        <b>The first distinction comes from your action space</b>, i.e., do you have discrete (e.g. LEFT, RIGHT, ‚Ä¶) or continuous actions (ex: go to a certain speed)? To check high-level if an algorithm will work, look at the <a href=\"https://docs.ray.io/en/master/rllib/rllib-algorithms.html\">RLlib algorithms doc page</a>.  <i>Algorithms are listed according to whether or not they support Discrete action spaces vs Continuous action spaces or both.</i> \n",
    "    </li>\n",
    "    <li>\n",
    "        <b>Choose a stable algorithm.</b>  When you look at the cumulative rewards per time step, they should rise steadily.  You do not want an algorithm where reward jumps up and down a lot.\n",
    "    </li>\n",
    "    <li><b>Choose the most sample-efficient algorithm that works for your environment</b>. <i>PPO is extremely sample-efficient.  SAC is much less sample-efficient.</i>\n",
    "    </li>\n",
    "</ol>\n",
    "\n",
    "\n",
    "Once you have selected the algorithm, <b>look up that algorithm's config class</b>.\n",
    "<ol>\n",
    "    <li>Open RLlib docs <a href=\"https://docs.ray.io/en/master/rllib/rllib-algorithms.html\">and navigate to the Algorithms page.</a></li>\n",
    "    <li>Scroll down and click url of algo you want to use, e.g. <i><b>PPO</b></i></li>\n",
    "    <li>On the <a href=\"https://docs.ray.io/en/master/rllib/rllib-algorithms.html#ppo\">algo docs page </a>, click on the link <i><b>Implementation</b></i>.  This will open the <a href=\"https://github.com/ray-project/ray/blob/master/rllib/algorithms/ppo/ppo.py\">algo code file on github</a>.</li>\n",
    "    <li>Search the github code file for the word <i><b>config</b></i></li>\n",
    "    <li>Typically the docstring example will show: </li>\n",
    "    <ol>\n",
    "        <li>Example code implementing RLlib API, then </li>\n",
    "        <li>Example code implementing Ray Tune API.</li>\n",
    "    </ol>\n",
    "    <li>Scroll down to the config <b>__init()__</b> method</li>\n",
    "    <ol>\n",
    "            <li>Algorithm default hyperparameter values are here.</li>\n",
    "    </ol>\n",
    "    </ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e61b7804-1332-45fe-a5bc-4b20364c02f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# config is an object instead of a dictionary since Ray version >= 1.13\n",
    "from ray.rllib.algorithms.ppo import PPOConfig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edc1cb28-1f27-4257-9521-6c7157d0e3ab",
   "metadata": {},
   "source": [
    "#### Step 4. Choose your config settings and instantiate a config object with those settings\n",
    "\n",
    "As of Ray 1.13, RLlib configs been converted from primitive dictionaries into Objects. This makes them harder to print, but easier to set/pass.\n",
    "\n",
    "**Note about RLlib config precedence**\n",
    "<ol>\n",
    "    <li>Highest precedence are <b>trainer instantiation settings</b>, these override any other config settings</li>\n",
    "    <li>RLlib <b>specific algorithm config</b> (see config class description above)</li>\n",
    "    <li>RLlib <b><a href\"https://github.com/ray-project/ray/blob/master/rllib/algorithms/algorithm_config.py#L58\">general config</a></b> settings have the lowest precedence</li>\n",
    "    </ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "422a7baf-cbd8-4141-a9d7-974239231c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common RLlib General config (for all algorithms)\n",
    "\n",
    "from ray.rllib.algorithms.algorithm_config import AlgorithmConfig\n",
    "config = AlgorithmConfig()\n",
    "\n",
    "# # uncomment below to see the long list of RLlib general config values\n",
    "# print(f\"RLlib's general default training config values:\")\n",
    "# print(pretty_print(config.to_dict()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca3bb6a2",
   "metadata": {},
   "source": [
    "**Note about num_workers**\n",
    "\n",
    "Number of Ray workers is the number of parallel workers or actors for rollouts.  Actual num_workers will be what you specifiy+1 for head node.\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "üí° <b>For num_workers, use ONE LESS than the number of cores you want to use</b> (or omit this argument and let Ray automatically use all cores)!\n",
    "</div>\n",
    "\n",
    "\n",
    "Below, num_workers = 7  # means actual number workers=8 including head node; where 8 is #cpu on my laptop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "99524e70",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'ray.rllib.algorithms.ppo.ppo.PPOConfig'>\n"
     ]
    }
   ],
   "source": [
    "# uncomment below to see the long list of specifically PPO default config values\n",
    "# print(pretty_print(PPOConfig().to_dict()))\n",
    "\n",
    "# Define algorithm config values\n",
    "env_name = \"CartPole-v1\"\n",
    "evaluation_interval = 2   #100, num training episodes to run between eval steps\n",
    "evaluation_duration = 20  #100, num eval episodes to run for the eval step\n",
    "num_workers = 4          # +1 for head node, num parallel workers or actors for rollouts\n",
    "num_gpus = 0             # num gpus to use in the cluster\n",
    "num_envs_per_worker = 1  #1, no vectorization of environments to run at same time\n",
    "\n",
    "# Define trainer runtime config values\n",
    "checkpoint_freq = evaluation_interval # freq save checkpoints >= evaulation_interval\n",
    "checkpoint_at_end = True                # always save last checkpoint\n",
    "relative_checkpoint_dir = \"my_PPO_logs\" # redirect logs instead of ~/ray_results/\n",
    "random_seed = 415\n",
    "# Set the log level to DEBUG, INFO, WARN, or ERROR \n",
    "log_level = \"ERROR\"\n",
    "\n",
    "# Create a new training config\n",
    "# override certain default algorithm config values\n",
    "config_PPO = (\n",
    "    PPOConfig()\n",
    "    .framework(framework='torch')\n",
    "    .environment(env=env_name, disable_env_checking=False)\n",
    "    .rollouts(num_rollout_workers=num_workers, num_envs_per_worker=num_envs_per_worker)\n",
    "    .resources(num_gpus=num_gpus, )\n",
    "#     .training(gamma=0.9, lr=0.01, kl_coeff=0.3)  # do not override defaults\n",
    "    .evaluation(evaluation_interval=evaluation_interval, \n",
    "                evaluation_duration=evaluation_duration)\n",
    "    .debugging(seed=random_seed, log_level=log_level)\n",
    ")\n",
    "\n",
    "print(type(config_PPO))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e43ae776-ffe7-486a-b091-a4a98dae6246",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<ray.rllib.algorithms.ppo.ppo.PPOConfig at 0x284727b80>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a PPOConfig object (same as we did in the previous notebook):\n",
    "config_PPO = PPOConfig()\n",
    "\n",
    "# Setup our config object the exact same way as before:\n",
    "# Point to our MultiAgentArena env:\n",
    "config_PPO.environment(env=\"CartPole-v1\",\n",
    "                       disable_env_checking=False)\n",
    "\n",
    "# Reduce the number of workers from 2 (default) to 1 \n",
    "# to save some resources on the expensive hyperparameter sweep.\n",
    "# IMPORTANT: More information on resource requirements for tune hyperparameter \n",
    "#            and different RLlib algorithm setups below\n",
    "config_PPO.rollouts(num_rollout_workers=1)\n",
    "\n",
    "# Set up evaluation:\n",
    "config_PPO.evaluation(\n",
    "    # Run evaluation once per `train()` call.\n",
    "    evaluation_interval=2,\n",
    "    # Use separate resources (RLlib rollout workers).\n",
    "    evaluation_num_workers=2,\n",
    "    # Run 20 episodes per evaluation (per iteration) \n",
    "    # -> 10 per eval worker (we have 2 eval workers).\n",
    "    evaluation_duration=20,\n",
    "    evaluation_duration_unit=\"timesteps\",\n",
    "    # # Run evaluation alternatingly with training (not in parallel).\n",
    "    # evaluation_parallel_to_training=False,\n",
    ")\n",
    "\n",
    "config_PPO.evaluation(evaluation_interval=evaluation_interval, \n",
    "                evaluation_duration=evaluation_duration)\n",
    "\n",
    "# Set the log level to DEBUG, INFO, WARN, or ERROR \n",
    "config_PPO.debugging(seed=415, log_level=\"ERROR\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abb86ff8",
   "metadata": {},
   "source": [
    "#### Step 5. Instantiate a Trainer from the environment and algorithm config objects\n",
    "\n",
    "**Two ways to train RLlib policies***\n",
    "<ol>\n",
    "    <li><a href=\"https://docs.ray.io/en/master/rllib/package_ref/index.html\">RLlib API.</a> The main methods are:</li>\n",
    "    <ul>\n",
    "        <li>train()</li>\n",
    "        <li>evaluate()</li>\n",
    "        <li>save()</li>\n",
    "        <li><b>restore()</b></li>\n",
    "        <li><b>compute_single_action()</b></li>\n",
    "    </ul>\n",
    "    <li><a href=\"https://docs.ray.io/en/master/tune/api_docs/overview.html\">Ray Tune API.</a>  The main methods are:</li>\n",
    "        <ul>\n",
    "            <li><b>run()</b></li>\n",
    "    </ul>\n",
    "    </ol>\n",
    "    \n",
    "*Actually 3 ways.  RLlib CLI from command line using .yml file is a 3rd way, but the .yml file is undocumented: <i>rllib train -f [myfile_name].yml</i><br>\n",
    "\n",
    "üëâ RLlib API <b>.train()</b> will train for 1 episode only.  Good for debugging since every single output will be shown for the 1 episode of training.  \n",
    "\n",
    "üëâ However for usual purposes, Ray Tune API <b>.run()</b> is more convenient since with 1 function call you get experiment management: save, checkpoint, evaluate, and train up to a stopping criteria.\n",
    "\n",
    "‚úîÔ∏è Both methods will run the RLlib [environment pre-check function](https://github.com/ray-project/ray/blob/master/rllib/utils/pre_checks/env.py#L22) you saw earlier in this notebook (Step 4. Check environment).\n",
    "\n",
    "üëç You have to use RLlib API method <b>.restore()</b> to reload a checkpointed RLlib model for Serving and Offline learning.  Tune API methods will not work.\n",
    "\n",
    "üëç After a model is trained, it can be used for inference.  The RLlib API method <b>compute_single_action()</b> will use the trained <i>`policy`</i> (RL word for trained model) to calculate actions for the entire number of time steps in 1 <i>`rollout`</i> (RLlib word for episode during inference).  You will see this method used at the end of this notebook. \n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>In summary: <br>\n",
    "    üí° If you are training a RLlib algorithm, train it using Ray Tune API method .run()!!  <br>\n",
    "    üëâ  If you are developing or debugging a RLlib algorithm, train it using RLlib API method .train()!! <br>\n",
    "    üëâ  If you need to restore a RLlib model, use RLlib API method .restore()!!</b>\n",
    "</div>\n",
    "\n",
    "üí° <b>Right-click on the cell below and choose \"Enable Scrolling for Outputs\"!</b>  This will make it easier to view, since model training output can be very long!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "26a6107f",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ###############\n",
    "# # EXAMPLE USING RLLIB API .train() FOR 1 EPISODE\n",
    "# # For completeness, here is how to use RLlib API's .train() method\n",
    "# # To train for N number of episodes, you put _.train()_ into a loop, \n",
    "# # similar to the way we ran the Gym _env.step()_ in a loop.\n",
    "# ###############\n",
    "\n",
    "# # To start fresh, restart Ray in case it is already running\n",
    "# if ray.is_initialized():\n",
    "#     ray.shutdown()\n",
    "\n",
    "# # Use the config object's `build()` method for generating\n",
    "# # an RLlib Algorithm instance that we can then train.\n",
    "# ppo_algo = config_PPO.build()\n",
    "# print(f\"Algorithm type: {type(ppo_algo)}\")\n",
    "\n",
    "# # train the PPO Algorithm instance for 8 episodes\n",
    "# for i in range(8):\n",
    "#     # Call its `train()` method\n",
    "#     result = ppo_algo.train()\n",
    "#     print(f\"Iteration={i}, Mean Reward={result['episode_reward_mean']}\")\n",
    "\n",
    "# # To stop the Algorithm and release its blocked resources, use:\n",
    "# ppo_algo.stop()\n",
    "# print()\n",
    "\n",
    "# # Below, you will see the output evaluation_interval times.\n",
    "# # Iteration=7, Mean Reward=237.3\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62252115",
   "metadata": {},
   "source": [
    "From the above cell, you can see how to train a RLlib algorithm 1 episode at a time.  But it is more practical to train RLlib algorithms using Ray Tune, since many more options are available.\n",
    "\n",
    "**Instantiate a trainer using Ray Tune API**\n",
    "\n",
    "Ray Tune offers experiment management in a single call <b>.run()</b>.  In the code below, we <b>specify a stopping criteria</b> to train until a certain Reward is achieved.  In case the desired training reward level is never reached, backup stop criteria can be given.  Tune will stop training whenever the earliest stop criteria is met.  However, best practice starting out is to only have 1 criteria, so you can be sure what is going to happen.\n",
    "\n",
    "üí° <b>Right-click on the cell below and choose \"Enable Scrolling for Outputs\"!</b>  This will make it easier to view, since model training output can be very long!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "24e34aa0",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "TuneError",
     "evalue": "Traceback (most recent call last):\n  File \"/Users/christy/Documents/ray/python/ray/tune/execution/trial_runner.py\", line 815, in _wait_and_handle_event\n    event = self.trial_executor.get_next_executor_event(\n  File \"/Users/christy/Documents/ray/python/ray/tune/execution/ray_trial_executor.py\", line 910, in get_next_executor_event\n    self._stage_and_update_status(live_trials)\n  File \"/Users/christy/Documents/ray/python/ray/tune/execution/ray_trial_executor.py\", line 299, in _stage_and_update_status\n    if not self._pg_manager.stage_trial_pg(trial):\n  File \"/Users/christy/Documents/ray/python/ray/tune/execution/placement_groups.py\", line 447, in stage_trial_pg\n    return self._stage_pgf_pg(pgf)\n  File \"/Users/christy/Documents/ray/python/ray/tune/execution/placement_groups.py\", line 461, in _stage_pgf_pg\n    self._staging_futures[pg.ready()] = (pgf, pg)\n  File \"/Users/christy/Documents/ray/python/ray/util/placement_group.py\", line 78, in ready\n    return bundle_reservation_check.options(\n  File \"/Users/christy/Documents/ray/python/ray/remote_function.py\", line 216, in remote\n    return func_cls._remote(args=args, kwargs=kwargs, **updated_options)\n  File \"/Users/christy/Documents/ray/python/ray/util/tracing/tracing_helper.py\", line 307, in _invocation_remote_span\n    return method(self, args, kwargs, *_args, **_kwargs)\n  File \"/Users/christy/Documents/ray/python/ray/remote_function.py\", line 400, in _remote\n    return invocation(args, kwargs)\n  File \"/Users/christy/Documents/ray/python/ray/remote_function.py\", line 375, in invocation\n    object_refs = worker.core_worker.submit_task(\n  File \"python/ray/_raylet.pyx\", line 1511, in ray._raylet.CoreWorker.submit_task\nTypeError: submit_task() takes exactly 11 positional arguments (12 given)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m~/Documents/ray/python/ray/tune/execution/trial_runner.py:815\u001b[0m, in \u001b[0;36mTrialRunner._wait_and_handle_event\u001b[0;34m(self, next_trial)\u001b[0m\n\u001b[1;32m    813\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    814\u001b[0m     \u001b[38;5;66;03m# Single wait of entire tune loop.\u001b[39;00m\n\u001b[0;32m--> 815\u001b[0m     event \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrial_executor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_next_executor_event\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    816\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_live_trials\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnext_trial\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\n\u001b[1;32m    817\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    818\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m event\u001b[38;5;241m.\u001b[39mtype \u001b[38;5;241m==\u001b[39m _ExecutorEventType\u001b[38;5;241m.\u001b[39mPG_READY:\n",
      "File \u001b[0;32m~/Documents/ray/python/ray/tune/execution/ray_trial_executor.py:910\u001b[0m, in \u001b[0;36mRayTrialExecutor.get_next_executor_event\u001b[0;34m(self, live_trials, next_trial_exists)\u001b[0m\n\u001b[1;32m    909\u001b[0m \u001b[38;5;66;03m# First update status of staged placement groups\u001b[39;00m\n\u001b[0;32m--> 910\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_stage_and_update_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlive_trials\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    911\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    912\u001b[0m     \u001b[38;5;66;03m###################################################################\u001b[39;00m\n\u001b[1;32m    913\u001b[0m     \u001b[38;5;66;03m# when next_trial_exists and there are cached resources\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    917\u001b[0m     \u001b[38;5;66;03m# a next trial to run, we return `PG_READY` future for trial\u001b[39;00m\n\u001b[1;32m    918\u001b[0m     \u001b[38;5;66;03m# runner. The next trial can then be scheduled on this PG.\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/ray/python/ray/tune/execution/ray_trial_executor.py:299\u001b[0m, in \u001b[0;36mRayTrialExecutor._stage_and_update_status\u001b[0;34m(self, trials)\u001b[0m\n\u001b[1;32m    297\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m--> 299\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_pg_manager\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstage_trial_pg\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    300\u001b[0m     \u001b[38;5;66;03m# Break if we reached the limit of pending placement groups.\u001b[39;00m\n\u001b[1;32m    301\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/ray/python/ray/tune/execution/placement_groups.py:447\u001b[0m, in \u001b[0;36m_PlacementGroupManager.stage_trial_pg\u001b[0;34m(self, trial)\u001b[0m\n\u001b[1;32m    446\u001b[0m pgf \u001b[38;5;241m=\u001b[39m trial\u001b[38;5;241m.\u001b[39mplacement_group_factory\n\u001b[0;32m--> 447\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_stage_pgf_pg\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpgf\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/ray/python/ray/tune/execution/placement_groups.py:461\u001b[0m, in \u001b[0;36m_PlacementGroupManager._stage_pgf_pg\u001b[0;34m(self, pgf)\u001b[0m\n\u001b[1;32m    460\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_staging[pgf]\u001b[38;5;241m.\u001b[39madd(pg)\n\u001b[0;32m--> 461\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_staging_futures[\u001b[43mpg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mready\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m] \u001b[38;5;241m=\u001b[39m (pgf, pg)\n\u001b[1;32m    463\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/ray/python/ray/util/placement_group.py:78\u001b[0m, in \u001b[0;36mPlacementGroup.ready\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbundle_cache) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m, (\n\u001b[1;32m     73\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mready() cannot be called on placement group object with a \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     74\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbundle length == 0, current bundle length: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     75\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbundle_cache)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     76\u001b[0m )\n\u001b[0;32m---> 78\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mbundle_reservation_check\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     79\u001b[0m \u001b[43m    \u001b[49m\u001b[43mplacement_group\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresources\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[43mBUNDLE_RESOURCE_LABEL\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.001\u001b[39;49m\u001b[43m}\u001b[49m\n\u001b[1;32m     80\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mremote\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/ray/python/ray/remote_function.py:216\u001b[0m, in \u001b[0;36mRemoteFunction.options.<locals>.FuncWrapper.remote\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mremote\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 216\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc_cls\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_remote\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mupdated_options\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/ray/python/ray/util/tracing/tracing_helper.py:307\u001b[0m, in \u001b[0;36m_tracing_task_invocation.<locals>._invocation_remote_span\u001b[0;34m(self, args, kwargs, *_args, **_kwargs)\u001b[0m\n\u001b[1;32m    306\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_ray_trace_ctx\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m kwargs\n\u001b[0;32m--> 307\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    309\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_ray_trace_ctx\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m kwargs\n",
      "File \u001b[0;32m~/Documents/ray/python/ray/remote_function.py:400\u001b[0m, in \u001b[0;36mRemoteFunction._remote\u001b[0;34m(self, args, kwargs, **task_options)\u001b[0m\n\u001b[1;32m    398\u001b[0m     invocation \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decorator(invocation)\n\u001b[0;32m--> 400\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minvocation\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/ray/python/ray/remote_function.py:375\u001b[0m, in \u001b[0;36mRemoteFunction._remote.<locals>.invocation\u001b[0;34m(args, kwargs)\u001b[0m\n\u001b[1;32m    372\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m (\n\u001b[1;32m    373\u001b[0m         \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_is_cross_language\n\u001b[1;32m    374\u001b[0m     ), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCross language remote function cannot be executed locally.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 375\u001b[0m object_refs \u001b[38;5;241m=\u001b[39m \u001b[43mworker\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcore_worker\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msubmit_task\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    376\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_language\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    377\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_function_descriptor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    378\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlist_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    379\u001b[0m \u001b[43m    \u001b[49m\u001b[43mname\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    380\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_returns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    381\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresources\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    382\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    383\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretry_exceptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    384\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretry_exception_allowlist\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    385\u001b[0m \u001b[43m    \u001b[49m\u001b[43mscheduling_strategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    386\u001b[0m \u001b[43m    \u001b[49m\u001b[43mworker\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdebugger_breakpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    387\u001b[0m \u001b[43m    \u001b[49m\u001b[43mserialized_runtime_env_info\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    388\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    389\u001b[0m \u001b[38;5;66;03m# Reset worker's debug context from the last \"remote\" command\u001b[39;00m\n\u001b[1;32m    390\u001b[0m \u001b[38;5;66;03m# (which applies only to this .remote call).\u001b[39;00m\n",
      "File \u001b[0;32mpython/ray/_raylet.pyx:1511\u001b[0m, in \u001b[0;36mray._raylet.CoreWorker.submit_task\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: submit_task() takes exactly 11 positional arguments (12 given)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mTuneError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [22]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m experiment_results \u001b[38;5;241m=\u001b[39m \u001b[43mtune\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mPPO\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m                    \u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# training config params\u001b[39;49;00m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mconfig_PPO\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m                              \u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# Stopping criteria whichever occurs first: average reward over training episodes, or ...\u001b[39;49;00m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mepisode_reward_mean\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m400\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# stop if achieve 400 out of max 500\u001b[39;49;00m\n\u001b[1;32m      8\u001b[0m \u001b[43m          \u001b[49m\u001b[38;5;66;43;03m# \"training_iteration\": 200,  # stop if achieved 200 episodes\u001b[39;49;00m\n\u001b[1;32m      9\u001b[0m \u001b[43m          \u001b[49m\u001b[38;5;66;43;03m# \"timesteps_total\": 100000,  # stop if achieved 100,000 timesteps\u001b[39;49;00m\n\u001b[1;32m     10\u001b[0m \u001b[43m          \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m                    \u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m#redirect logs instead of default ~/ray_results/\u001b[39;49;00m\n\u001b[1;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlocal_dir\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mrelative_checkpoint_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m#relative path\u001b[39;49;00m\n\u001b[1;32m     14\u001b[0m \u001b[43m         \u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# set frequency saving checkpoints >= evaulation_interval\u001b[39;49;00m\n\u001b[1;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcheckpoint_freq\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcheckpoint_at_end\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m         \u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# Reduce logging messages\u001b[39;49;00m\n\u001b[1;32m     20\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m###############\u001b[39;49;00m\n\u001b[1;32m     21\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# Note about Ray Tune verbosity.\u001b[39;49;00m\n\u001b[1;32m     22\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# Screen verbosity in Ray Tune is defined as verbose = 0, 1, 2, or 3, where:\u001b[39;49;00m\n\u001b[1;32m     23\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# 0 = silent\u001b[39;49;00m\n\u001b[1;32m     24\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# 1 = only status updates, no logging messages\u001b[39;49;00m\n\u001b[1;32m     25\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# 2 = status and brief trial results, includes logging messages\u001b[39;49;00m\n\u001b[1;32m     26\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# 3 = status and detailed trial results, includes logging messages\u001b[39;49;00m\n\u001b[1;32m     27\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# Defaults to 3.\u001b[39;49;00m\n\u001b[1;32m     28\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m###############                          \u001b[39;49;00m\n\u001b[1;32m     29\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[43m                              \u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# Define what we are comparing for, when we search for the\u001b[39;49;00m\n\u001b[1;32m     32\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# \"best\" checkpoint at the end.\u001b[39;49;00m\n\u001b[1;32m     33\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetric\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mepisode_reward_mean\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[43m                              \u001b[49m\n\u001b[1;32m     36\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining completed.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Documents/ray/python/ray/tune/tune.py:722\u001b[0m, in \u001b[0;36mrun\u001b[0;34m(run_or_experiment, name, metric, mode, stop, time_budget_s, config, resources_per_trial, num_samples, local_dir, search_alg, scheduler, keep_checkpoints_num, checkpoint_score_attr, checkpoint_freq, checkpoint_at_end, verbose, progress_reporter, log_to_file, trial_name_creator, trial_dirname_creator, sync_config, export_formats, max_failures, fail_fast, restore, server_port, resume, reuse_actors, trial_executor, raise_on_failed_trial, callbacks, max_concurrent_trials, _experiment_checkpoint_dir, _remote)\u001b[0m\n\u001b[1;32m    715\u001b[0m progress_reporter\u001b[38;5;241m.\u001b[39msetup(\n\u001b[1;32m    716\u001b[0m     start_time\u001b[38;5;241m=\u001b[39mtune_start,\n\u001b[1;32m    717\u001b[0m     total_samples\u001b[38;5;241m=\u001b[39msearch_alg\u001b[38;5;241m.\u001b[39mtotal_samples,\n\u001b[1;32m    718\u001b[0m     metric\u001b[38;5;241m=\u001b[39mmetric,\n\u001b[1;32m    719\u001b[0m     mode\u001b[38;5;241m=\u001b[39mmode,\n\u001b[1;32m    720\u001b[0m )\n\u001b[1;32m    721\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m runner\u001b[38;5;241m.\u001b[39mis_finished() \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msignal\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[0;32m--> 722\u001b[0m     \u001b[43mrunner\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    723\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m has_verbosity(Verbosity\u001b[38;5;241m.\u001b[39mV1_EXPERIMENT):\n\u001b[1;32m    724\u001b[0m         _report_progress(runner, progress_reporter)\n",
      "File \u001b[0;32m~/Documents/ray/python/ray/tune/execution/trial_runner.py:872\u001b[0m, in \u001b[0;36mTrialRunner.step\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    869\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m next_trial:\n\u001b[1;32m    870\u001b[0m     logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGot new trial to run: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnext_trial\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 872\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_wait_and_handle_event\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnext_trial\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    874\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stop_experiment_if_needed()\n\u001b[1;32m    876\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/Documents/ray/python/ray/tune/execution/trial_runner.py:851\u001b[0m, in \u001b[0;36mTrialRunner._wait_and_handle_event\u001b[0;34m(self, next_trial)\u001b[0m\n\u001b[1;32m    849\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    850\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 851\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m TuneError(traceback\u001b[38;5;241m.\u001b[39mformat_exc())\n",
      "\u001b[0;31mTuneError\u001b[0m: Traceback (most recent call last):\n  File \"/Users/christy/Documents/ray/python/ray/tune/execution/trial_runner.py\", line 815, in _wait_and_handle_event\n    event = self.trial_executor.get_next_executor_event(\n  File \"/Users/christy/Documents/ray/python/ray/tune/execution/ray_trial_executor.py\", line 910, in get_next_executor_event\n    self._stage_and_update_status(live_trials)\n  File \"/Users/christy/Documents/ray/python/ray/tune/execution/ray_trial_executor.py\", line 299, in _stage_and_update_status\n    if not self._pg_manager.stage_trial_pg(trial):\n  File \"/Users/christy/Documents/ray/python/ray/tune/execution/placement_groups.py\", line 447, in stage_trial_pg\n    return self._stage_pgf_pg(pgf)\n  File \"/Users/christy/Documents/ray/python/ray/tune/execution/placement_groups.py\", line 461, in _stage_pgf_pg\n    self._staging_futures[pg.ready()] = (pgf, pg)\n  File \"/Users/christy/Documents/ray/python/ray/util/placement_group.py\", line 78, in ready\n    return bundle_reservation_check.options(\n  File \"/Users/christy/Documents/ray/python/ray/remote_function.py\", line 216, in remote\n    return func_cls._remote(args=args, kwargs=kwargs, **updated_options)\n  File \"/Users/christy/Documents/ray/python/ray/util/tracing/tracing_helper.py\", line 307, in _invocation_remote_span\n    return method(self, args, kwargs, *_args, **_kwargs)\n  File \"/Users/christy/Documents/ray/python/ray/remote_function.py\", line 400, in _remote\n    return invocation(args, kwargs)\n  File \"/Users/christy/Documents/ray/python/ray/remote_function.py\", line 375, in invocation\n    object_refs = worker.core_worker.submit_task(\n  File \"python/ray/_raylet.pyx\", line 1511, in ray._raylet.CoreWorker.submit_task\nTypeError: submit_task() takes exactly 11 positional arguments (12 given)\n"
     ]
    }
   ],
   "source": [
    "experiment_results = tune.run(\"PPO\", \n",
    "                    \n",
    "    # training config params\n",
    "    config = config_PPO.to_dict(),\n",
    "                              \n",
    "    # Stopping criteria whichever occurs first: average reward over training episodes, or ...\n",
    "    stop={\"episode_reward_mean\": 400, # stop if achieve 400 out of max 500\n",
    "          # \"training_iteration\": 200,  # stop if achieved 200 episodes\n",
    "          # \"timesteps_total\": 100000,  # stop if achieved 100,000 timesteps\n",
    "          },  \n",
    "                    \n",
    "    #redirect logs instead of default ~/ray_results/\n",
    "    local_dir = relative_checkpoint_dir, #relative path\n",
    "         \n",
    "    # set frequency saving checkpoints >= evaulation_interval\n",
    "    checkpoint_freq = 2,  \n",
    "    checkpoint_at_end=True,\n",
    "         \n",
    "    # Reduce logging messages\n",
    "    ###############\n",
    "    # Note about Ray Tune verbosity.\n",
    "    # Screen verbosity in Ray Tune is defined as verbose = 0, 1, 2, or 3, where:\n",
    "    # 0 = silent\n",
    "    # 1 = only status updates, no logging messages\n",
    "    # 2 = status and brief trial results, includes logging messages\n",
    "    # 3 = status and detailed trial results, includes logging messages\n",
    "    # Defaults to 3.\n",
    "    ###############                          \n",
    "    verbose = 2,\n",
    "                              \n",
    "    # Define what we are comparing for, when we search for the\n",
    "    # \"best\" checkpoint at the end.\n",
    "    metric=\"episode_reward_mean\",\n",
    "    mode=\"max\",\n",
    "                              \n",
    "    )\n",
    "\n",
    "print(\"Training completed.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc331b91-dcb8-4225-8e30-94415c6b0c11",
   "metadata": {
    "tags": []
   },
   "source": [
    "<br>\n",
    "\n",
    "Scroll through the output in the cell above.  Look for a single table row that looks like this: <br>\n",
    "\n",
    "<img src=\"./images/ppo_cartpole_tune_output.png\"></img>\n",
    "\n",
    "What this telling you is that Tune ran your experiment.  It was terminated when Reward reached 409.14, which satisified the stopping criteria of Reward >= 400, out of an environment max reward possible of 500."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0fd024c",
   "metadata": {},
   "source": [
    "## Evaluate a RLlib Policy <a class=\"anchor\" id=\"eval_rllib\"></a>\n",
    "\n",
    "RLlib policies can be evaluated by:\n",
    "<ul>\n",
    "    <li>Calling RLlib Algorithm API method .evaluate()*</li>\n",
    "    <li>Examining <b>Ray Tune</b> experiment results </li>\n",
    "    <li>Visualizing training progress in <b>TensorBoard</b></li>\n",
    "    </ul>\n",
    "\n",
    "*RLlib algorithm objects can be examined manually using, for example <i>ppo_algo.evaluate()</i>, but it gives the same info you already saw in the single episode .train() output.\n",
    "\n",
    "**Ray Tune experiment results** <br>\n",
    "First, let's start by looking at the experiment results object. How long did training take?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bb3c46c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get RLlib default stats\n",
    "stats = experiment_results.stats()\n",
    "secs = stats[\"timestamp\"] - stats[\"start_time\"]\n",
    "print(f'{secs:7.2f} seconds, {secs/60.0:7.2f} minutes')\n",
    "\n",
    "# Typically takes about 67 seconds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "793cad71",
   "metadata": {},
   "source": [
    "**Ray Tune experiment trial results**\n",
    "\n",
    "Read all the experiment trials into a single pandas dataframe.  The dataframe will have 1 row per trial.\n",
    "\n",
    "Below, we see a dataframe with only 1 row because Tune only ran 1 trial.  (Because we did not specify a hyperparameter space to search for tuning.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99483c98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read Tune experiment results in a pandas dataframe\n",
    "df = experiment_results.results_df\n",
    "\n",
    "print(f\"df.shape: {df.shape}\")  #Only 1 trial\n",
    "print(f\"df.columns: {df.columns}\")\n",
    "df.iloc[:,0:8].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dacd6b1e",
   "metadata": {},
   "source": [
    "For how many episodes did training run?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15cdff25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get number of episodes for the 1st trial\n",
    "df.iloc[0,:].episodes_this_iter  \n",
    "\n",
    "# Answer is 8 episodes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0ed7e6a",
   "metadata": {},
   "source": [
    "What were the best parameter values?  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53a815bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Watch out, the following output is long because there are many parameters!\n",
    "\n",
    "# experiment_results.get_best_config(metric=\"episode_reward_mean\", mode=\"mean\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35fd62d4",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Visualize the training progress in TensorBoard\n",
    "\n",
    "RLlib automatically creates logs for your trained RLlib models that can be visualized in TensorBoard.  To visualize the performance of your RL model:\n",
    "\n",
    "<ol>\n",
    "    <li>Open a terminal</li>\n",
    "    <li><i><b>cd</b></i> into the logdir path from the above cell's output.</li>\n",
    "    <li><i><b>ls</b></i></li>\n",
    "    <li>You should see files that look like: checkpoint_NNNNNN</li>\n",
    "    <li>To be able to compare all your experiments, cd one dir level up.\n",
    "    <li><i><b>cd ..</b></i>  \n",
    "    <li><i><b>tensorboard --logdir . </b></i></li>\n",
    "    <li>Look at the url in the message, and open it in a browser</li>\n",
    "        </ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6a04eff",
   "metadata": {},
   "source": [
    "#### Screenshot of Tensorboard\n",
    "\n",
    "TensorBoard will give you many pages of charts.  Below displaying just Train/Eval mean and min rewards.\n",
    "\n",
    "The charts below are showing \"sample efficiency\", the number of training steps it took to achieve a certain level of performance.\n",
    "\n",
    "<b>Train Performance:</b> <br>\n",
    "\n",
    "---\n",
    "<img src=\"./images/ppo_cartpole_training_rewards.png\" width=\"80%\" />\n",
    "\n",
    "<b>Eval Performance:</b> <br>\n",
    "<img src=\"./images/ppo_cartpole_eval_rewards.png\" width=\"80%\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf198368",
   "metadata": {},
   "source": [
    "## Reload RLlib model from checkpoint and run inference <a class=\"anchor\" id=\"reload_rllib\"></a>\n",
    "\n",
    "We want to reload the desired RLlib model from checkpoint file and then run the model inference mode on the environment it was trained on.  \n",
    "\n",
    "You will need:\n",
    "<ul>\n",
    "    <li>Your <b>algorithm's config class</b> and exact same <a href=\"#intro_rllib_api\">config settings you used to train your model.</a></li>\n",
    "    <li>Name of the <b>environment</b> you used to train the model.</li>\n",
    "    <li>Path to the desired <b>checkpoint</b> file you want to use to restore the model.</li>\n",
    "    </ul>\n",
    "\n",
    "#### Step 1. Find the best model checkpoint file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82b94d2c-9c28-411a-ad9d-ab46357b5664",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get best checkpoint path\n",
    "logdir = experiment_results.get_best_logdir(metric=\"evaluation_reward_mean\", mode=\"max\")\n",
    "print(logdir)\n",
    "\n",
    "# Get last checkpoint path\n",
    "checkpoint = experiment_results.get_last_checkpoint()\n",
    "print(f\"\\n{checkpoint}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc9f109b-8618-4f2a-8484-2a0b62890d6e",
   "metadata": {},
   "source": [
    "#### Step 2. Re-initialize an already-trained algorithm object from the checkpoint file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b933a724-7840-4fb7-bf0c-0b64e627c183",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new Algorithm and restore its state from the last checkpoint.\n",
    "\n",
    "# create an empty Algorithm\n",
    "algo = config_PPO.build(env=env_name)\n",
    "\n",
    "# restore the agent from the checkpoint\n",
    "algo.restore(checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b47e4a55-fbe1-439e-9a5f-76818a2289e5",
   "metadata": {},
   "source": [
    "#### Step 3. Play and render the game as a video\n",
    "\n",
    "Now we want to record a video of the trained model doing inference in the environment it was trained on.\n",
    "\n",
    "Gym includes video recording and saving capability in its wrapper <i>`gym.wrappers.RecordVideo`</i>, so we will import and use that method to wrap the <i>`gym.make()`</i> method.\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "üëç During inference, call the RLlib API method <b>compute_single_action()</b>, which uses the trained <i>`policy`</i> (RL word for trained model) to calculate actions for the entire number of time steps in 1 <i>`rollout`</i> (RLlib word for episode during inference). \n",
    "</div>\n",
    "\n",
    "Execute the cell below, and you will see a video of the rollouts, so you can verify visually that the agent is starting from a near perfect score.\n",
    "\n",
    "Note that the CartPole environment's perfect score is 500.  Since we trained our model to around 400, the restored agent should appear very stable. \n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03f9f3bb-9ea2-471d-98d2-19632a79ce80",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gym.wrappers import RecordVideo\n",
    "\n",
    "#############\n",
    "## Create the env to do inference on\n",
    "#############\n",
    "# Try this first to make sure it is working\n",
    "# env = gym.make(env_name)\n",
    "# Once you have confirmed it works, wrap the method\n",
    "# RecordVideo() takes as input a path name where to store the video\n",
    "# RecordVideo() includes its own render step, records, and saves the video.\n",
    "env = RecordVideo(gym.make(env_name), \"videos\", )\n",
    "obs = env.reset()\n",
    "\n",
    "#############\n",
    "## Use the restored model and run it in inference mode\n",
    "## You will see a pop-up video rendering for about 10 seconds\n",
    "#############\n",
    "num_episodes_during_inference = 1\n",
    "num_episodes = 0\n",
    "episode_reward = 0.0\n",
    "done = False\n",
    "\n",
    "while num_episodes < num_episodes_during_inference:\n",
    "    # Compute an action (`a`).\n",
    "    a = algo.compute_single_action(observation=obs)\n",
    "    # Send the computed action `a` to the env.\n",
    "    obs, reward, done, _ = env.step(a)\n",
    "    episode_reward += reward\n",
    "    \n",
    "    # Is the episode `done`? -> Reset.\n",
    "    if done:\n",
    "        print(f\"Episode done: reward = {episode_reward}\")\n",
    "        obs = env.reset()\n",
    "        num_episodes += 1\n",
    "        episode_reward = 0.0\n",
    "        break\n",
    "        \n",
    "env.close()\n",
    "\n",
    "# The restored agent manages to achieve a perfect score during the 1 episode rollout.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36eb7940-bb3c-4899-a7fa-66a676ba30bd",
   "metadata": {},
   "source": [
    "<br>\n",
    "<b>Play and share your video .mp4 file with others if you want.</b>\n",
    "<br><br>\n",
    "\n",
    "Show a video player in the notebook, and play the video you just recorded!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b3aa827",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Video\n",
    "\n",
    "cart_pole_video='videos/rl-video-episode-0.mp4'\n",
    "Video(cart_pole_video, width=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40b3830e-d0ae-4794-85b2-4b245faef04d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shut down Ray if you are done\n",
    "if ray.is_initialized():\n",
    "    ray.shutdown()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "829fbd96",
   "metadata": {},
   "source": [
    "### Summary\n",
    "\n",
    "TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3bb74fe",
   "metadata": {},
   "source": [
    "### Exercises\n",
    "\n",
    "1. How would you choose another algorithm to train Cart Pole?  Hint:  Look at the [RLlib algorithm doc page](https://docs.ray.io/en/master/rllib/rllib-algorithms.html).  How would you change the choice of RLlib algorithm from <b>PPO to DQN</b>?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1511893b",
   "metadata": {},
   "source": [
    "### References\n",
    "\n",
    "1. [OpenAI Gym Environments](https://www.gymlibrary.ml/)\n",
    "2. [Ray doc page](https://docs.ray.io/en/latest/)\n",
    "3. [Rllib github](https://github.com/ray-project/ray/tree/master/rllib)\n",
    "4. [RLlib Algorithms doc page](https://docs.ray.io/en/master/rllib/rllib-algorithms.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aa00b0e-9136-4d52-9977-b1845064b60e",
   "metadata": {},
   "source": [
    "üìñ [Back to Table of Contents](./ex_00_rllib_notebooks_table_of_contents.ipynb)<br>\n",
    "‚û° [Next notebook](./ex_02_create_multiagent_rllib_env.ipynb) <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9a924cb-af94-43dd-9e4e-4fab1b648302",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
