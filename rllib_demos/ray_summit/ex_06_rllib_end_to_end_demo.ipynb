{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "89a2f968-af93-4759-b6dc-ba5e5dcd3332",
   "metadata": {},
   "source": [
    "# Notebook 06. End-to-end demo: Learning a Multiplayer Game using RLlib, Ray Tune, and Ray Serve\n",
    "\n",
    "¬© 2019-2022, Anyscale. All Rights Reserved <br>\n",
    "üìñ [Back to Table of Contents](./ex_00_rllib_notebooks_table_of_contents.ipynb)<br>\n",
    "‚¨ÖÔ∏è [Previous notebook](./ex_05_rllib_and_ray_serve.ipynb) <br>\n",
    "\n",
    "\n",
    "In this notebook, you will learn how to:\n",
    "* Recycle our multi-player game from a previous notebook in this tutorial\n",
    "* The game will be interrupted in the middle of an episode by an in-game item sale (a power-up is offered to both players at a price determined by a trained RecSys model served via Ray Serve)\n",
    "* A user model decides whether to buy the item or not\n",
    "* The game continues with or without the bought item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "bc0e78d6-61fb-4e8f-bb54-d09080a6a776",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gym: 0.21.0\n",
      "ray: 3.0.0.dev0\n"
     ]
    }
   ],
   "source": [
    "# Import required packages.\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas\n",
    "import requests\n",
    "import time\n",
    "\n",
    "import ray\n",
    "from ray import serve\n",
    "from ray import tune\n",
    "from ray.rllib.algorithms.crr import CRRConfig\n",
    "from ray.rllib.algorithms.ppo import PPOConfig\n",
    "from ray.rllib.examples.env.random_env import RandomEnv\n",
    "\n",
    "from multi_agent_arena.multi_agent_arena import MultiAgentArena\n",
    "\n",
    "\n",
    "print(f\"gym: {gym.__version__}\")\n",
    "print(f\"ray: {ray.__version__}\")\n",
    "\n",
    "# !ale-import-roms --import-from-pkg atari_py.atari_roms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d78f7eb-936c-47a1-922a-9bd4beab2e3b",
   "metadata": {},
   "source": [
    "## Modifying our Game\n",
    "\n",
    "So far, we have been using our own custom `MultiAgentEnv` sub-class to define our game and asked RLlib to train two policies (one for each agent/player in the game) on how to play the game close to optimal.\n",
    "\n",
    "In this end-to-end example, we would like to extend this idea and include an in-game power-up (item) sale in the middle of the episode.\n",
    "The type of the offered item is fixed and always the same for both players. Buying it will allow the respectve agent to move twice as fast as before.\n",
    "Remember that each episode had a fixed number of timesteps (configurable via the `timestep_limit` constructor argument). We will now add some logic such that the game will pause after half of this number of timesteps and ask the in the to \n",
    "\n",
    "<img src=\"images/multi_agent_arena_3.png\" width=800 />\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1aafd4eb-cc44-4acf-9f19-2f7e4d30957d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<ray.rllib.algorithms.ppo.ppo.PPOConfig at 0x7fbdb9faecd0>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use this simple script to generate some RecSys (price recommender) offline data:\n",
    "\n",
    "dummy_config = PPOConfig().environment(env=RandomEnv, env_config={\n",
    "    # Observation space: agent1 total reward, agent2 total reward\n",
    "    \"observation_space\": gym.spaces.Box(-100, 100.0, (2, ), np.float32),\n",
    "    # Price for the offered item (between $0 and $100).\n",
    "    \"action_space\": gym.spaces.Box(0.0, 100.0, (1,), np.float32),\n",
    "    \"reward_space\": gym.spaces.Box(0.0, 1.0, (), np.float32),\n",
    "    \"p_done\": 0.0,\n",
    "    # One-step episode len:\n",
    "    # reset() -> obs=game state\n",
    "    # step(action=recommended price) -> reward=bought or not + done?\n",
    "    \"max_episode_len\": 1,\n",
    "}).offline_data(output=\"offline_rl_data\")\n",
    "\n",
    "# Uncomment to train and generate the json output.\n",
    "\"\"\"\n",
    "algo = dummy_config.build()\n",
    "\n",
    "for _ in range(4):\n",
    "    algo.train()\n",
    "\"\"\"\n",
    "\n",
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "460ec786-dc51-45b1-a788-c7f8e83c7970",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>obs</th>\n",
       "      <th>actions</th>\n",
       "      <th>prev_actions</th>\n",
       "      <th>rewards</th>\n",
       "      <th>prev_rewards</th>\n",
       "      <th>dones</th>\n",
       "      <th>t</th>\n",
       "      <th>eps_id</th>\n",
       "      <th>unroll_id</th>\n",
       "      <th>agent_index</th>\n",
       "      <th>action_prob</th>\n",
       "      <th>action_logp</th>\n",
       "      <th>action_dist_inputs</th>\n",
       "      <th>advantages</th>\n",
       "      <th>value_targets</th>\n",
       "      <th>new_obs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SampleBatch</td>\n",
       "      <td>[[4.8118495941, -2.3441574574], [2.9057257175,...</td>\n",
       "      <td>[[1.0178880692], [0.9851945043], [-0.418107837...</td>\n",
       "      <td>[[1.0178880692], [0.9851945043], [-0.418107837...</td>\n",
       "      <td>[0.9583539367, 0.4122557342, 0.8586096168, 0.2...</td>\n",
       "      <td>[0.9583539367, 0.4122557342, 0.8586096168, 0.2...</td>\n",
       "      <td>[True, True, True, True, True, True, True, Tru...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1473058350, 1302222902, 1192158565, 138371370...</td>\n",
       "      <td>[26600, 26601, 26602, 26603, 26604, 26605, 266...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0.3876596391, 0.29237189890000004, 0.33490023...</td>\n",
       "      <td>[-0.9476275444000001, -1.2297286987, -1.093922...</td>\n",
       "      <td>[[0.4882035851, -0.16736510400000001], [0.1870...</td>\n",
       "      <td>[0.4965085387, -0.0309310257, 0.33864313360000...</td>\n",
       "      <td>[0.9583539367, 0.4122557342, 0.8586096168, 0.2...</td>\n",
       "      <td>[[4.8118495941, -2.3441574574], [2.9057257175,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SampleBatch</td>\n",
       "      <td>[[-2.5245726109, -6.7920980453], [2.82340765, ...</td>\n",
       "      <td>[[0.6935230494], [0.5013949871], [-0.285651296...</td>\n",
       "      <td>[[0.6935230494], [0.5013949871], [-0.285651296...</td>\n",
       "      <td>[0.556561172, 0.2602818906, 0.5748660564, 0.44...</td>\n",
       "      <td>[0.556561172, 0.2602818906, 0.5748660564, 0.44...</td>\n",
       "      <td>[True, True, True, True, True, True, True, Tru...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1548731360, 1146866576, 868049208, 1059559815...</td>\n",
       "      <td>[26800, 26801, 26802, 26803, 26804, 26805, 268...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0.2448666096, 0.2235761732, 0.5428897738, 0.1...</td>\n",
       "      <td>[-1.4070416689, -1.4980031252, -0.6108490229, ...</td>\n",
       "      <td>[[-0.2137183249, 0.226452902], [-0.5752148628,...</td>\n",
       "      <td>[0.0268349648, -0.23261126880000002, 0.1082856...</td>\n",
       "      <td>[0.556561172, 0.2602818906, 0.5748660564, 0.44...</td>\n",
       "      <td>[[-2.5245726109, -6.7920980453], [2.82340765, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SampleBatch</td>\n",
       "      <td>[[-8.8964138031, -2.5550217628], [5.9735541344...</td>\n",
       "      <td>[[-0.6430661082], [-0.5806437731], [-0.9546555...</td>\n",
       "      <td>[[-0.6430661082], [-0.5806437731], [-0.9546555...</td>\n",
       "      <td>[0.1523697376, 0.3082717061, 0.1305330247, 0.7...</td>\n",
       "      <td>[0.1523697376, 0.3082717061, 0.1305330247, 0.7...</td>\n",
       "      <td>[True, True, True, True, True, True, True, Tru...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1576878179, 231362190, 98657002, 1156300994, ...</td>\n",
       "      <td>[27000, 27001, 27002, 27003, 27004, 27005, 270...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0.36427187920000004, 0.4829119146, 0.42937782...</td>\n",
       "      <td>[-1.0098547935, -0.7279210091, -0.845418036000...</td>\n",
       "      <td>[[-0.2679643631, 0.0238414705], [-0.2201347053...</td>\n",
       "      <td>[-0.3653242588, -0.1588068306, -0.3405982852, ...</td>\n",
       "      <td>[0.1523697376, 0.3082717061, 0.1305330247, 0.7...</td>\n",
       "      <td>[[-8.8964138031, -2.5550217628], [5.9735541344...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>SampleBatch</td>\n",
       "      <td>[[-9.1011753082, 3.4522781372], [-9.4919176102...</td>\n",
       "      <td>[[-1.2645549774], [0.5220996141], [1.653925538...</td>\n",
       "      <td>[[-1.2645549774], [0.5220996141], [1.653925538...</td>\n",
       "      <td>[0.820802331, 0.5500279665, 0.9657452106000001...</td>\n",
       "      <td>[0.820802331, 0.5500279665, 0.9657452106000001...</td>\n",
       "      <td>[True, True, True, True, True, True, True, Tru...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1646655329, 1720896215, 524391847, 1017520255...</td>\n",
       "      <td>[27200, 27201, 27202, 27203, 27204, 27205, 272...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0.3244321644, 0.2451513559, 0.102826900800000...</td>\n",
       "      <td>[-1.1256787777, -1.4058794975, -2.274708271, -...</td>\n",
       "      <td>[[-0.5836541057, -0.0488789529], [-0.446589678...</td>\n",
       "      <td>[0.2631308436, 0.0150480866, 0.431158185, 0.21...</td>\n",
       "      <td>[0.820802331, 0.5500279665, 0.9657452106000001...</td>\n",
       "      <td>[[-9.1011753082, 3.4522781372], [-9.4919176102...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>SampleBatch</td>\n",
       "      <td>[[4.7014288902, 0.4653792679], [-5.8722491264,...</td>\n",
       "      <td>[[-0.3971097469], [-0.8547632098], [0.50789344...</td>\n",
       "      <td>[[-0.3971097469], [-0.8547632098], [0.50789344...</td>\n",
       "      <td>[0.8241453171, 0.5656263828, 0.6364953518, 0.1...</td>\n",
       "      <td>[0.8241453171, 0.5656263828, 0.6364953518, 0.1...</td>\n",
       "      <td>[True, True, True, True, True, True, True, Tru...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[125557271, 1190899551, 660656716, 1153393864,...</td>\n",
       "      <td>[27400, 27401, 27402, 27403, 27404, 27405, 274...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0.36774969100000005, 0.2262004018, 0.30153703...</td>\n",
       "      <td>[-1.0003527403, -1.4863339663000001, -1.198862...</td>\n",
       "      <td>[[0.0219877884, -0.0077851862], [0.1896262765,...</td>\n",
       "      <td>[0.3771229088, 0.060254335400000004, 0.1244024...</td>\n",
       "      <td>[0.8241453171, 0.5656263828, 0.6364953518, 0.1...</td>\n",
       "      <td>[[4.7014288902, 0.4653792679], [-5.8722491264,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          type                                                obs  \\\n",
       "0  SampleBatch  [[4.8118495941, -2.3441574574], [2.9057257175,...   \n",
       "1  SampleBatch  [[-2.5245726109, -6.7920980453], [2.82340765, ...   \n",
       "2  SampleBatch  [[-8.8964138031, -2.5550217628], [5.9735541344...   \n",
       "3  SampleBatch  [[-9.1011753082, 3.4522781372], [-9.4919176102...   \n",
       "4  SampleBatch  [[4.7014288902, 0.4653792679], [-5.8722491264,...   \n",
       "\n",
       "                                             actions  \\\n",
       "0  [[1.0178880692], [0.9851945043], [-0.418107837...   \n",
       "1  [[0.6935230494], [0.5013949871], [-0.285651296...   \n",
       "2  [[-0.6430661082], [-0.5806437731], [-0.9546555...   \n",
       "3  [[-1.2645549774], [0.5220996141], [1.653925538...   \n",
       "4  [[-0.3971097469], [-0.8547632098], [0.50789344...   \n",
       "\n",
       "                                        prev_actions  \\\n",
       "0  [[1.0178880692], [0.9851945043], [-0.418107837...   \n",
       "1  [[0.6935230494], [0.5013949871], [-0.285651296...   \n",
       "2  [[-0.6430661082], [-0.5806437731], [-0.9546555...   \n",
       "3  [[-1.2645549774], [0.5220996141], [1.653925538...   \n",
       "4  [[-0.3971097469], [-0.8547632098], [0.50789344...   \n",
       "\n",
       "                                             rewards  \\\n",
       "0  [0.9583539367, 0.4122557342, 0.8586096168, 0.2...   \n",
       "1  [0.556561172, 0.2602818906, 0.5748660564, 0.44...   \n",
       "2  [0.1523697376, 0.3082717061, 0.1305330247, 0.7...   \n",
       "3  [0.820802331, 0.5500279665, 0.9657452106000001...   \n",
       "4  [0.8241453171, 0.5656263828, 0.6364953518, 0.1...   \n",
       "\n",
       "                                        prev_rewards  \\\n",
       "0  [0.9583539367, 0.4122557342, 0.8586096168, 0.2...   \n",
       "1  [0.556561172, 0.2602818906, 0.5748660564, 0.44...   \n",
       "2  [0.1523697376, 0.3082717061, 0.1305330247, 0.7...   \n",
       "3  [0.820802331, 0.5500279665, 0.9657452106000001...   \n",
       "4  [0.8241453171, 0.5656263828, 0.6364953518, 0.1...   \n",
       "\n",
       "                                               dones  \\\n",
       "0  [True, True, True, True, True, True, True, Tru...   \n",
       "1  [True, True, True, True, True, True, True, Tru...   \n",
       "2  [True, True, True, True, True, True, True, Tru...   \n",
       "3  [True, True, True, True, True, True, True, Tru...   \n",
       "4  [True, True, True, True, True, True, True, Tru...   \n",
       "\n",
       "                                                   t  \\\n",
       "0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "2  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "3  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "4  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "\n",
       "                                              eps_id  \\\n",
       "0  [1473058350, 1302222902, 1192158565, 138371370...   \n",
       "1  [1548731360, 1146866576, 868049208, 1059559815...   \n",
       "2  [1576878179, 231362190, 98657002, 1156300994, ...   \n",
       "3  [1646655329, 1720896215, 524391847, 1017520255...   \n",
       "4  [125557271, 1190899551, 660656716, 1153393864,...   \n",
       "\n",
       "                                           unroll_id  \\\n",
       "0  [26600, 26601, 26602, 26603, 26604, 26605, 266...   \n",
       "1  [26800, 26801, 26802, 26803, 26804, 26805, 268...   \n",
       "2  [27000, 27001, 27002, 27003, 27004, 27005, 270...   \n",
       "3  [27200, 27201, 27202, 27203, 27204, 27205, 272...   \n",
       "4  [27400, 27401, 27402, 27403, 27404, 27405, 274...   \n",
       "\n",
       "                                         agent_index  \\\n",
       "0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "2  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "3  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "4  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "\n",
       "                                         action_prob  \\\n",
       "0  [0.3876596391, 0.29237189890000004, 0.33490023...   \n",
       "1  [0.2448666096, 0.2235761732, 0.5428897738, 0.1...   \n",
       "2  [0.36427187920000004, 0.4829119146, 0.42937782...   \n",
       "3  [0.3244321644, 0.2451513559, 0.102826900800000...   \n",
       "4  [0.36774969100000005, 0.2262004018, 0.30153703...   \n",
       "\n",
       "                                         action_logp  \\\n",
       "0  [-0.9476275444000001, -1.2297286987, -1.093922...   \n",
       "1  [-1.4070416689, -1.4980031252, -0.6108490229, ...   \n",
       "2  [-1.0098547935, -0.7279210091, -0.845418036000...   \n",
       "3  [-1.1256787777, -1.4058794975, -2.274708271, -...   \n",
       "4  [-1.0003527403, -1.4863339663000001, -1.198862...   \n",
       "\n",
       "                                  action_dist_inputs  \\\n",
       "0  [[0.4882035851, -0.16736510400000001], [0.1870...   \n",
       "1  [[-0.2137183249, 0.226452902], [-0.5752148628,...   \n",
       "2  [[-0.2679643631, 0.0238414705], [-0.2201347053...   \n",
       "3  [[-0.5836541057, -0.0488789529], [-0.446589678...   \n",
       "4  [[0.0219877884, -0.0077851862], [0.1896262765,...   \n",
       "\n",
       "                                          advantages  \\\n",
       "0  [0.4965085387, -0.0309310257, 0.33864313360000...   \n",
       "1  [0.0268349648, -0.23261126880000002, 0.1082856...   \n",
       "2  [-0.3653242588, -0.1588068306, -0.3405982852, ...   \n",
       "3  [0.2631308436, 0.0150480866, 0.431158185, 0.21...   \n",
       "4  [0.3771229088, 0.060254335400000004, 0.1244024...   \n",
       "\n",
       "                                       value_targets  \\\n",
       "0  [0.9583539367, 0.4122557342, 0.8586096168, 0.2...   \n",
       "1  [0.556561172, 0.2602818906, 0.5748660564, 0.44...   \n",
       "2  [0.1523697376, 0.3082717061, 0.1305330247, 0.7...   \n",
       "3  [0.820802331, 0.5500279665, 0.9657452106000001...   \n",
       "4  [0.8241453171, 0.5656263828, 0.6364953518, 0.1...   \n",
       "\n",
       "                                             new_obs  \n",
       "0  [[4.8118495941, -2.3441574574], [2.9057257175,...  \n",
       "1  [[-2.5245726109, -6.7920980453], [2.82340765, ...  \n",
       "2  [[-8.8964138031, -2.5550217628], [5.9735541344...  \n",
       "3  [[-9.1011753082, 3.4522781372], [-9.4919176102...  \n",
       "4  [[4.7014288902, 0.4653792679], [-5.8722491264,...  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's first take a look at some of this (JSON) data using pandas:\n",
    "json_file = \"offline_rl_data/in_game_item_price_recsys.json\"\n",
    "dataframe = pandas.read_json(json_file, lines=True)  # don't forget lines=True -> Each line in the json is one \"rollout\" of 4 timesteps.\n",
    "dataframe.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5f39d301-c849-4c25-bbd4-cb0a5d05e59e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<ray.rllib.algorithms.crr.crr.CRRConfig at 0x7fbdbb294b20>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "crr_config = CRRConfig()\n",
    "\n",
    "crr_config.environment(\n",
    "    env=None,\n",
    "    observation_space=dummy_config.env_config[\"observation_space\"],\n",
    "    action_space=dummy_config.env_config[\"action_space\"],\n",
    ")\n",
    "\n",
    "crr_config.offline_data(\n",
    "    input_=\"dataset\",\n",
    "    input_config={\n",
    "        # If you feel daring here, use the `pendulum_beginner.json` file instead of the expert one here.\n",
    "        # You may need to train a little longer, then, in order to get a decent policy.\n",
    "        # But since you have the actual Pendulum environment available for evaluation, you should be able\n",
    "        # to perfectly stop learning once a good episode reward (> -300.0) has been reached.\n",
    "        \"paths\": os.path.join(os.getcwd(), \"offline_rl_data/in_game_item_price_recsys.json\"),\n",
    "        \"format\": \"json\",\n",
    "    },\n",
    "    actions_in_input_normalized=True,\n",
    ")\n",
    "\n",
    "crr_config.framework(\"torch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "132acdd0-15b2-4aa9-98ef-b7f94685c74c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-08-04 13:05:38 (running for 00:00:51.91)<br>Memory usage on this node: 10.9/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/16 CPUs, 0/0 GPUs, 0.0/5.4 GiB heap, 0.0/2.0 GiB objects<br>Result logdir: /Users/sven/Dropbox/Projects/ray-summit-2022-training/ray-rllib/results/CRR<br>Number of trials: 1/1 (1 TERMINATED)<br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(CRR pid=28988)\u001b[0m Checking /Users/sven/Dropbox/Projects/ray-summit-2022-training/ray-rllib/offline_rl_data/in_game_item_price_recsys.json ...\n",
      "\u001b[2m\u001b[36m(CRR pid=28988)\u001b[0m fpath=/Users/sven/Dropbox/Projects/ray-summit-2022-training/ray-rllib/offline_rl_data/in_game_item_price_recsys.json ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(CRR pid=28988)\u001b[0m 2022-08-04 13:04:56,530\tWARNING deprecation.py:47 -- DeprecationWarning: `min_iter_time_s` has been deprecated. Use `min_time_s_per_iteration` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(CRR pid=28988)\u001b[0m 2022-08-04 13:04:56,531\tWARNING deprecation.py:47 -- DeprecationWarning: `on_trainer_init(trainer, **kwargs)` has been deprecated. Use `on_algorithm_init(algorithm, **kwargs)` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(CRR pid=28988)\u001b[0m 2022-08-04 13:04:56,531\tINFO algorithm.py:332 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(CRR pid=28988)\u001b[0m 2022-08-04 13:04:56,618\tWARNING read_api.py:260 -- The number of blocks in this dataset (1) limits its parallelism to 1 concurrent tasks. This is much less than the number of available CPU slots in the cluster. Use `.repartition(n)` to increase the number of dataset blocks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(CRR pid=28988)\u001b[0m [dataset]: Run `pip install tqdm` to enable progress reporting.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(RolloutWorker pid=28995)\u001b[0m 2022-08-04 13:05:07,244\tWARNING deprecation.py:47 -- DeprecationWarning: `on_trainer_init(trainer, **kwargs)` has been deprecated. Use `on_algorithm_init(algorithm, **kwargs)` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=28995)\u001b[0m 2022-08-04 13:05:07,250\tWARNING deprecation.py:47 -- DeprecationWarning: `on_trainer_init(trainer, **kwargs)` has been deprecated. Use `on_algorithm_init(algorithm, **kwargs)` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=28997)\u001b[0m 2022-08-04 13:05:07,244\tWARNING deprecation.py:47 -- DeprecationWarning: `on_trainer_init(trainer, **kwargs)` has been deprecated. Use `on_algorithm_init(algorithm, **kwargs)` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=28997)\u001b[0m 2022-08-04 13:05:07,250\tWARNING deprecation.py:47 -- DeprecationWarning: `on_trainer_init(trainer, **kwargs)` has been deprecated. Use `on_algorithm_init(algorithm, **kwargs)` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=28996)\u001b[0m 2022-08-04 13:05:07,244\tWARNING deprecation.py:47 -- DeprecationWarning: `on_trainer_init(trainer, **kwargs)` has been deprecated. Use `on_algorithm_init(algorithm, **kwargs)` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=28996)\u001b[0m 2022-08-04 13:05:07,250\tWARNING deprecation.py:47 -- DeprecationWarning: `on_trainer_init(trainer, **kwargs)` has been deprecated. Use `on_algorithm_init(algorithm, **kwargs)` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=28998)\u001b[0m 2022-08-04 13:05:07,244\tWARNING deprecation.py:47 -- DeprecationWarning: `on_trainer_init(trainer, **kwargs)` has been deprecated. Use `on_algorithm_init(algorithm, **kwargs)` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=28998)\u001b[0m 2022-08-04 13:05:07,250\tWARNING deprecation.py:47 -- DeprecationWarning: `on_trainer_init(trainer, **kwargs)` has been deprecated. Use `on_algorithm_init(algorithm, **kwargs)` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(CRR pid=28988)\u001b[0m 2022-08-04 13:05:07,335\tWARNING deprecation.py:47 -- DeprecationWarning: `on_trainer_init(trainer, **kwargs)` has been deprecated. Use `on_algorithm_init(algorithm, **kwargs)` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(CRR pid=28988)\u001b[0m 2022-08-04 13:05:07,336\tWARNING deprecation.py:47 -- DeprecationWarning: `on_trainer_init(trainer, **kwargs)` has been deprecated. Use `on_algorithm_init(algorithm, **kwargs)` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(CRR pid=28988)\u001b[0m 2022-08-04 13:05:07,350\tINFO trainable.py:160 -- Trainable.setup took 10.820 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n",
      "\u001b[2m\u001b[36m(CRR pid=28988)\u001b[0m 2022-08-04 13:05:07,350\tWARNING util.py:65 -- Install gputil for GPU system monitoring.\n",
      "\u001b[2m\u001b[36m(CRR pid=28988)\u001b[0m 2022-08-04 13:05:07,383\tWARNING deprecation.py:47 -- DeprecationWarning: `concat_samples` has been deprecated. Use `concat_samples() from rllib.policy.sample_batch` instead. This will raise an error in the future!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(RolloutWorker pid=28995)\u001b[0m DatasetReader 1 has 1, samples.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=28996)\u001b[0m DatasetReader 2 has 2, samples.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=28997)\u001b[0m DatasetReader 3 has 2, samples.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=28998)\u001b[0m DatasetReader 4 has 2, samples.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-04 13:05:17,990\tWARNING util.py:216 -- The `process_trial_result` operation took 0.608 s, which may be a performance bottleneck.\n",
      "2022-08-04 13:05:38,389\tINFO tune.py:737 -- Total run time: 52.12 seconds (51.89 seconds for the tuning loop).\n"
     ]
    }
   ],
   "source": [
    "results = tune.run(\n",
    "    # Registered name for the CRR Algorithm.\n",
    "    \"CRR\",\n",
    "    # Use our config -> converted to python dict.\n",
    "    config=crr_config.to_dict(),\n",
    "    # Stopping criteria -> As we are learning from dummy data, just train for a few iterations.\n",
    "    stop={\n",
    "        \"training_iteration\": 3,\n",
    "    },\n",
    "    # Create checkpoint every iteration.\n",
    "    checkpoint_freq=1,\n",
    "    local_dir=\"results\",\n",
    "    verbose=1,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "57d74ff1-5429-4535-abd4-c6d762a48a35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last checkpoint from training: <ray.air.checkpoint.Checkpoint object at 0x7fbdbda90460>\n"
     ]
    }
   ],
   "source": [
    "# Get the best trial (there is only one) and last checkpoint.\n",
    "best_trial = results.get_best_trial()\n",
    "last_checkpoint = results.get_last_checkpoint(trial=best_trial)\n",
    "print(f\"Last checkpoint from training: {last_checkpoint}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "bccbadcf-57c1-434a-9ed4-724f6a45997e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(ServeController pid=29159)\u001b[0m INFO 2022-08-04 13:17:29,549 controller 29159 checkpoint_path.py:17 - Using RayInternalKVStore for controller checkpoint and recovery.\n",
      "\u001b[2m\u001b[36m(ServeController pid=29159)\u001b[0m INFO 2022-08-04 13:17:29,551 controller 29159 http_state.py:115 - Starting HTTP proxy with name 'SERVE_CONTROLLER_ACTOR:jIZYdn:SERVE_PROXY_ACTOR-node:127.0.0.1-0' on node 'node:127.0.0.1-0' listening on '127.0.0.1:8000'\n",
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=29160)\u001b[0m INFO:     Started server process [29160]\n",
      "/var/folders/j4/brrn254576lgnbqqtp5p1z280000gn/T/ipykernel_5126/735497702.py:23: UserWarning: From /var/folders/j4/brrn254576lgnbqqtp5p1z280000gn/T/ipykernel_5126/735497702.py:23: deploy (from ray.serve.deployment) is deprecated and will be removed in a future version Please see https://docs.ray.io/en/latest/serve/index.html\n",
      "  ServeModel.deploy(crr_config, last_checkpoint)\n",
      "\u001b[2m\u001b[36m(ServeController pid=29159)\u001b[0m INFO 2022-08-04 13:17:31,504 controller 29159 deployment_state.py:1280 - Adding 1 replicas to deployment 'ServeModel'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(ServeModel pid=29161)\u001b[0m Checking /Users/sven/Dropbox/Projects/ray-summit-2022-training/ray-rllib/offline_rl_data/in_game_item_price_recsys.json ...\n",
      "\u001b[2m\u001b[36m(ServeModel pid=29161)\u001b[0m fpath=/Users/sven/Dropbox/Projects/ray-summit-2022-training/ray-rllib/offline_rl_data/in_game_item_price_recsys.json ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(ServeModel pid=29161)\u001b[0m 2022-08-04 13:17:40,634\tWARNING deprecation.py:47 -- DeprecationWarning: `min_iter_time_s` has been deprecated. Use `min_time_s_per_iteration` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(ServeModel pid=29161)\u001b[0m 2022-08-04 13:17:40,635\tWARNING deprecation.py:47 -- DeprecationWarning: `on_trainer_init(trainer, **kwargs)` has been deprecated. Use `on_algorithm_init(algorithm, **kwargs)` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(ServeModel pid=29161)\u001b[0m 2022-08-04 13:17:40,635\tINFO algorithm.py:332 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(ServeModel pid=29161)\u001b[0m 2022-08-04 13:17:40,907\tWARNING read_api.py:260 -- The number of blocks in this dataset (1) limits its parallelism to 1 concurrent tasks. This is much less than the number of available CPU slots in the cluster. Use `.repartition(n)` to increase the number of dataset blocks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(RolloutWorker pid=29171)\u001b[0m DatasetReader 4 has 2, samples.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=29169)\u001b[0m DatasetReader 2 has 2, samples.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=29170)\u001b[0m DatasetReader 3 has 2, samples.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=29168)\u001b[0m DatasetReader 1 has 1, samples.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(RolloutWorker pid=29169)\u001b[0m 2022-08-04 13:17:51,397\tWARNING deprecation.py:47 -- DeprecationWarning: `on_trainer_init(trainer, **kwargs)` has been deprecated. Use `on_algorithm_init(algorithm, **kwargs)` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=29169)\u001b[0m 2022-08-04 13:17:51,398\tWARNING deprecation.py:47 -- DeprecationWarning: `on_trainer_init(trainer, **kwargs)` has been deprecated. Use `on_algorithm_init(algorithm, **kwargs)` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=29170)\u001b[0m 2022-08-04 13:17:51,397\tWARNING deprecation.py:47 -- DeprecationWarning: `on_trainer_init(trainer, **kwargs)` has been deprecated. Use `on_algorithm_init(algorithm, **kwargs)` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=29170)\u001b[0m 2022-08-04 13:17:51,398\tWARNING deprecation.py:47 -- DeprecationWarning: `on_trainer_init(trainer, **kwargs)` has been deprecated. Use `on_algorithm_init(algorithm, **kwargs)` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=29171)\u001b[0m 2022-08-04 13:17:51,397\tWARNING deprecation.py:47 -- DeprecationWarning: `on_trainer_init(trainer, **kwargs)` has been deprecated. Use `on_algorithm_init(algorithm, **kwargs)` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=29171)\u001b[0m 2022-08-04 13:17:51,398\tWARNING deprecation.py:47 -- DeprecationWarning: `on_trainer_init(trainer, **kwargs)` has been deprecated. Use `on_algorithm_init(algorithm, **kwargs)` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=29168)\u001b[0m 2022-08-04 13:17:51,397\tWARNING deprecation.py:47 -- DeprecationWarning: `on_trainer_init(trainer, **kwargs)` has been deprecated. Use `on_algorithm_init(algorithm, **kwargs)` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=29168)\u001b[0m 2022-08-04 13:17:51,398\tWARNING deprecation.py:47 -- DeprecationWarning: `on_trainer_init(trainer, **kwargs)` has been deprecated. Use `on_algorithm_init(algorithm, **kwargs)` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(ServeModel pid=29161)\u001b[0m 2022-08-04 13:17:51,452\tWARNING deprecation.py:47 -- DeprecationWarning: `on_trainer_init(trainer, **kwargs)` has been deprecated. Use `on_algorithm_init(algorithm, **kwargs)` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(ServeModel pid=29161)\u001b[0m 2022-08-04 13:17:51,453\tWARNING deprecation.py:47 -- DeprecationWarning: `on_trainer_init(trainer, **kwargs)` has been deprecated. Use `on_algorithm_init(algorithm, **kwargs)` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(ServeModel pid=29161)\u001b[0m 2022-08-04 13:17:51,465\tINFO trainable.py:160 -- Trainable.setup took 10.834 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n",
      "\u001b[2m\u001b[36m(ServeModel pid=29161)\u001b[0m 2022-08-04 13:17:51,466\tWARNING util.py:65 -- Install gputil for GPU system monitoring.\n",
      "\u001b[2m\u001b[36m(ServeModel pid=29161)\u001b[0m 2022-08-04 13:17:51,492\tINFO trainable.py:654 -- Restored on 127.0.0.1 from checkpoint: /var/folders/j4/brrn254576lgnbqqtp5p1z280000gn/T/checkpoint_tmp_0m3qjtnl\n",
      "\u001b[2m\u001b[36m(ServeModel pid=29161)\u001b[0m 2022-08-04 13:17:51,492\tINFO trainable.py:663 -- Current state after restoring: {'_iteration': 3, '_timesteps_total': None, '_time_total': 30.137645483016968, '_episodes_total': 0}\n"
     ]
    }
   ],
   "source": [
    "# Call `serve.start()` to get \n",
    "serve.start()\n",
    "\n",
    "\n",
    "@serve.deployment(route_prefix=\"/in-game-recommendations\")\n",
    "class ServeModel:\n",
    "    def __init__(self, config, checkpoint) -> None:\n",
    "        # Create new algo from scratch.\n",
    "        self.algo = config.build()\n",
    "        # Restore state of algo to a already trained one (using a checkpoint).\n",
    "        self.algo.restore(checkpoint)\n",
    "\n",
    "    async def __call__(self, request):\n",
    "        json_input = await request.json()\n",
    "        # Extract observation from input.\n",
    "        obs = json_input[\"observation\"]\n",
    "        # Translate obs back to np.arrays.\n",
    "        np_obs = np.array(obs)\n",
    "        action = self.algo.compute_single_action(np_obs, explore=False)\n",
    "        return {\"action\": action}\n",
    "\n",
    "\n",
    "ServeModel.deploy(crr_config, last_checkpoint)\n",
    "    \n",
    "# That's it: Deployment created!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "e4fe9b6e-13b7-4609-99d6-09676177e4d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "55.382530212402344"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=29160)\u001b[0m INFO 2022-08-04 14:10:29,662 http_proxy 127.0.0.1 http_proxy.py:316 - GET /in-game-recommendations 200 4.0ms\n",
      "\u001b[2m\u001b[36m(ServeModel pid=29161)\u001b[0m INFO 2022-08-04 14:10:29,661 ServeModel ServeModel#xPKpku replica.py:467 - HANDLE __call__ OK 1.2ms\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Convenience function to send action requests to the service.\n",
    "def get_price(rewards1, rewards2):\n",
    "    obs = np.array([rewards1, rewards2])\n",
    "    # Convert numpy array to list (needed for http transfer).\n",
    "    obs = obs.tolist()\n",
    "    resp = requests.get(\n",
    "        \"http://localhost:8000/in-game-recommendations\", json={\"observation\": obs}\n",
    "    )\n",
    "    response_json = resp.json()\n",
    "    price = response_json[\"action\"][0]\n",
    "    return price\n",
    "\n",
    "# Test our deployment\n",
    "get_price(0.0, -10.0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "15093b3c-52ee-4a8b-a3cf-e9821176db36",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiAgentArenaWithItemSale(MultiAgentArena):\n",
    "    def __init__(self, config=None):\n",
    "        super().__init__(config=config)\n",
    "        \n",
    "        self.sell_item_at_ts = self.timestep_limit // 2\n",
    "\n",
    "    def reset(self):\n",
    "        obs = super().reset()\n",
    "        self.agent1_moves_first = False\n",
    "        self.agent2_double_speed = False\n",
    "        return obs\n",
    "\n",
    "    def step(self, action: dict):\n",
    "        # Increase our time steps counter by 1.\n",
    "        self.timesteps += 1\n",
    "        # An episode is \"done\" when we reach the time step limit.\n",
    "        is_done = self.timesteps >= self.timestep_limit\n",
    "\n",
    "        ######################\n",
    "        # NEW BEHAVIOR\n",
    "        ######################\n",
    "        # It's time to do the item sale.\n",
    "        price_agent1_item = price_agent2_item = 0.0\n",
    "        if self.timesteps == self.sell_item_at_ts:\n",
    "            # Send a price request to our price service.\n",
    "            price_agent1_item = get_price(self.agent1_R, self.agent2_R)\n",
    "            price_agent2_item = get_price(self.agent2_R, self.agent1_R)\n",
    "            \n",
    "            # User model agent1: User of agent1 buys if item price < 50.0.\n",
    "            if price_agent1_item < 50.0:\n",
    "                print(\"User1 bought power-up!\")\n",
    "                time.sleep(1.0)\n",
    "                self.agent1_moves_first = True\n",
    "            # User model agent2: User of agent2 buys if item price < 45.0.\n",
    "            if price_agent2_item < 45.0:\n",
    "                print(\"User2 bought power-up!\")\n",
    "                time.sleep(1.0)\n",
    "                self.agent2_double_speed = True\n",
    "        \n",
    "        # Who moves first?\n",
    "        # events = [collision|agent1_new_field]\n",
    "        if self.agent1_moves_first:\n",
    "            events = self._move(self.agent1_pos, action[\"agent1\"], is_agent1=True)\n",
    "            events |= self._move(self.agent2_pos, action[\"agent2\"], is_agent1=False)\n",
    "            # Agent2 is allowed to move twice (double the speed).\n",
    "            if self.agent2_double_speed:\n",
    "                events |= self._move(self.agent2_pos, action[\"agent2\"], is_agent1=False)\n",
    "        else:\n",
    "            events = self._move(self.agent2_pos, action[\"agent2\"], is_agent1=False)\n",
    "            # Agent2 is allowed to move twice (double the speed).\n",
    "            if self.agent2_double_speed:\n",
    "                events |= self._move(self.agent2_pos, action[\"agent2\"], is_agent1=False)\n",
    "            events |= self._move(self.agent1_pos, action[\"agent1\"], is_agent1=True)\n",
    "\n",
    "        # Determine rewards based on the collected events AND on the prices paid:\n",
    "        r1 = -1.0 if \"collision\" in events else 1.0 if \"agent1_new_field\" in events else -0.5\n",
    "        r2 = 1.0 if \"collision\" in events else -0.1\n",
    "        r1 -= price_agent1_item / 10.0\n",
    "        r2 -= price_agent2_item / 10.0\n",
    "        self.agent1_R += r1\n",
    "        self.agent2_R += r2\n",
    "        ######################\n",
    "        # END: NEW BEHAVIOR\n",
    "        ######################\n",
    "\n",
    "        rewards = {\n",
    "            \"agent1\": r1,\n",
    "            \"agent2\": r2,\n",
    "        }\n",
    "\n",
    "        # Generate a `done` dict (per-agent and total).\n",
    "        dones = {\n",
    "            \"agent1\": is_done,\n",
    "            \"agent2\": is_done,\n",
    "            # special `__all__` key indicates that the episode is done for all agents.\n",
    "            \"__all__\": is_done,\n",
    "        }\n",
    "\n",
    "        # Useful for rendering.\n",
    "        self.collision = \"collision\" in events\n",
    "        if self.collision is True:\n",
    "            self.num_collisions += 1    \n",
    "\n",
    "        return self._get_obs(), rewards, dones, {}  # <- info dict (not needed here).\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "cb0a8cfa-e0e4-4a7f-9214-b780807c10df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32b23d9f95f243dbbe59a2f27070db37",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent1's x/y position=[2, 1]\n",
      "Agent2's x/y position=[0, 2]\n",
      "Env timesteps=6\n"
     ]
    }
   ],
   "source": [
    "env = MultiAgentArenaWithItemSale(config={\"render\": True, \"width\": 5, \"height\": 5, \"timestep_limit\": 10})\n",
    "obs = env.reset()\n",
    "\n",
    "with env.out:\n",
    "    # Agent1 moves down, Agent2 moves up.\n",
    "    obs, rewards, dones, infos = env.step(action={\"agent1\": 2, \"agent2\": 0})\n",
    "    env.render()\n",
    "\n",
    "    # Agent1 moves right, Agent2 moves left.\n",
    "    obs, rewards, dones, infos = env.step(action={\"agent1\": 1, \"agent2\": 3})\n",
    "    env.render()\n",
    "\n",
    "    # Agent1 moves right, Agent2 moves left.\n",
    "    obs, rewards, dones, infos = env.step(action={\"agent1\": 1, \"agent2\": 3})\n",
    "    env.render()\n",
    "\n",
    "    # Agent1 moves down, Agent2 moves up.\n",
    "    obs, rewards, dones, infos = env.step(action={\"agent1\": 2, \"agent2\": 0})\n",
    "    env.render()\n",
    "\n",
    "    # Agent1 moves left, Agent2 moves up.\n",
    "    obs, rewards, dones, infos = env.step(action={\"agent1\": 3, \"agent2\": 0})\n",
    "    env.render()\n",
    "\n",
    "    # Agent1 moves down, Agent2 moves up.\n",
    "    obs, rewards, dones, infos = env.step(action={\"agent1\": 2, \"agent2\": 0})\n",
    "    env.render()\n",
    "\n",
    "\n",
    "print(\"Agent1's x/y position={}\".format(env.agent1_pos))\n",
    "print(\"Agent2's x/y position={}\".format(env.agent2_pos))\n",
    "print(\"Env timesteps={}\".format(env.timesteps))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb3feb27-8976-4bcf-a38c-87562bc59809",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
