{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3o3rZytugfo6",
   "metadata": {
    "id": "3o3rZytugfo6"
   },
   "source": [
    "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/christy/AnyscaleDemos/blob/main/rllib_demos/ODSC_conference/tutorial_notebook.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://github.com/christy/AnyscaleDemos/blob/main/rllib_demos/ODSC_conference/tutorial_notebook.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />View source on GitHub</a>\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "_BD9NRNPltO8",
   "metadata": {
    "id": "_BD9NRNPltO8"
   },
   "outputs": [],
   "source": [
    "!pip install \"ray[rllib,serve]==1.12\" sklearn tensorflow gputil recsim gym==0.21"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aa06051",
   "metadata": {
    "id": "6aa06051"
   },
   "source": [
    "# Reinforcement Learning for Recommender Systems\n",
    "## From Contextual Bandits to Slate-Q\n",
    "\n",
    "\n",
    "\n",
    "<table>\n",
    "<tr>\n",
    "    <td> <img src=https://drive.google.com/uc?id=1jAhSZfGDIcdlBXd6EtfxPhBvV54F16RX style=\"width: 230px;\"/> </td>\n",
    "    <td> <img src=https://drive.google.com/uc?id=1rum1twl4g0nsPJzxDplFjknZHrYbQ3XW style=\"width: 213px;\"/> </td>\n",
    "    <td> <img src=https://drive.google.com/uc?id=1t9w6Z87vd7cgtjxAa3chlGwX_fde03Oy style=\"width: 169px;\"/> </td>\n",
    "    <td> <img src=https://drive.google.com/uc?id=1tVPSOMDzIqrSK7NHEzrYc_xSdEpe6Ibg style=\"width: 254px;\"/> </td>\n",
    "    <td> <img src=https://drive.google.com/uc?id=10kw_frXjhtaLrxYNIHlSAPgAGZ_tNbv- style=\"width: 252px;\"/> </td>\n",
    "</tr>\n",
    "</table>\n",
    "\n",
    "\n",
    "### Overview\n",
    "“Reinforcement Learning for Recommender Systems, From Contextual Bandits to Slate-Q” is a tutorial for industry researchers, domain-experts, and ML-engineers, showcasing ...\n",
    "\n",
    "1) .. how you can use RLlib to build a recommender system **simulator** for your industry applications and run Bandit algorithms and the Slate-Q algorithm against this simulator.\n",
    "\n",
    "2) .. how RLlib's offline algorithms pose solutions in case you **don't have a simulator** of your problem environment at hand.\n",
    "\n",
    "We will further explore how to deploy trained models to production using Ray Serve.\n",
    "\n",
    "During the live-coding phases, we will using a recommender system simulating environment by google's RecSim and configure and run 2 RLlib algorithms against it. We'll also demonstrate how you may use offline RL as a solution for recommender systems and how to deploy a learned policy into production.\n",
    "\n",
    "RLlib offers industry-grade scalability, a large list of algos to choose from (offline, model-based, model-free, etc..), support for TensorFlow and PyTorch, and a unified API for a variety of applications. This tutorial includes a brief introduction to provide an overview of concepts (e.g. why RL?) before proceeding to RLlib (recommender system) environments, neural network models, offline RL, student exercises, Q/A, and more. All code will be provided as .py files in a GitHub repo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "green-insertion",
   "metadata": {
    "id": "green-insertion"
   },
   "source": [
    "### Intended Audience\n",
    "* Python programmers who are interested in using RL to solve their specific industry decision making problems and who want to get started with RLlib.\n",
    "\n",
    "### Prerequisites\n",
    "* Some Python programming experience.\n",
    "* Some familiarity with machine learning.\n",
    "* *Helpful, but not required:* Experience in reinforcement learning and Ray.\n",
    "* *Helpful, but not required:* Experience with TensorFlow or PyTorch.\n",
    "\n",
    "\n",
    "### Key Takeaways\n",
    "* What is reinforcement learning and RLlib?\n",
    "* How do recommender systems work? How do we build our own?\n",
    "* How do we train RLlib's different algorithms on a recommender system problem?\n",
    "* How do I deploy an already trained policy into production using Ray Serve.\n",
    "\n",
    "\n",
    "### Tutorial Outline\n",
    "\n",
    "1. Reinforcement learning (RL) in a nutshell.\n",
    "1. How to formulate any problem as an RL-solvable one?\n",
    "1. Recommender systems - How they work.\n",
    "1. Why you should use RLlib.\n",
    "\n",
    "(10min break)\n",
    "\n",
    "1. [Google RecSim - Build your own recom sys simulator.](#recsim)\n",
    "1. [Dissecting the \"long term satisfaction\" (LTE) environment.](#dissecting_lte)\n",
    "1. [Using a contextual Bandit algorithm with RLlib and starting our first training run on the LTE env.](#rllib)\n",
    "1. [What did the Bandit learn?](#bandit_results)\n",
    "1. [Intro to Slate-Q.](#slateq)\n",
    "1. [Starting a Slate-Q training run.](#slateq_experiment)\n",
    "\n",
    "(10min break)\n",
    "\n",
    "1. [Analyzing the results of the SlateQ run.](#slateq_results)\n",
    "1. [Ray Serve example: How can we deploy a trained policy into our production environment?](#ray_serve)\n",
    "\n",
    "\n",
    "### Other Recommended Readings\n",
    "* [Intro to RLlib: Example Environments](https://medium.com/distributed-computing-with-ray/intro-to-rllib-example-environments-3a113f532c70)\n",
    "* [Reinforcement Learning with RLlib in the Unity Game Engine](https://medium.com/distributed-computing-with-ray/reinforcement-learning-with-rllib-in-the-unity-game-engine-1a98080a7c0d)\n",
    "\n",
    "<td> <img src=https://drive.google.com/uc?id=1skpzZvJPWLSbGg0xj1obuQegKc0YfwZO width=400> </td>\n",
    "\n",
    "* [Attention Nets and More with RLlib's Trajectory View API](https://medium.com/distributed-computing-with-ray/attention-nets-and-more-with-rllibs-trajectory-view-api-d326339a6e65)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9f7e21f-f3de-4bad-a3a7-4bbd0b015559",
   "metadata": {
    "id": "e9f7e21f-f3de-4bad-a3a7-4bbd0b015559"
   },
   "source": [
    "# Let's start!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "930deb27-e739-4507-bc24-e39ded9caeb4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "930deb27-e739-4507-bc24-e39ded9caeb4",
    "outputId": "c5fae381-70b2-4f56-e1cf-46c6e4b65755"
   },
   "outputs": [],
   "source": [
    "# Let's get started with some basic imports.\n",
    "\n",
    "import ray  # .. of course\n",
    "from ray import serve\n",
    "from ray import tune\n",
    "\n",
    "from collections import OrderedDict\n",
    "import gym  # RL environments and action/observation spaces\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas\n",
    "from pprint import pprint\n",
    "import re\n",
    "import recsim  # google's RecSim package.\n",
    "import requests\n",
    "from scipy.interpolate import make_interp_spline, BSpline\n",
    "from scipy.stats import linregress, sem\n",
    "from starlette.requests import Request\n",
    "import tree  # dm_tree\n",
    "\n",
    "!python --version\n",
    "print(f\"ray: {ray.__version__}\")\n",
    "import tensorflow as tf\n",
    "print(f\"tf: {tf.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90f010a6-7ee3-49b3-814a-2b9455c66c8f",
   "metadata": {
    "id": "90f010a6-7ee3-49b3-814a-2b9455c66c8f"
   },
   "source": [
    "<a id='recsim'></a>\n",
    "## Introducing google RecSim\n",
    "\n",
    "<td> <img src=https://drive.google.com/uc?id=1B9zX184GMYZ6_oLR8p16tSJnzIHofk1g width=600/> </td>\n",
    "\n",
    "<a href=\"https://github.com/google-research/recsim\">Google's RecSim package</a> offers a flexible way for you to <a href=\"https://github.com/google-research/recsim/blob/master/recsim/colab/RecSim_Developing_an_Environment.ipynb\">define the different building blocks of a recommender system</a>:\n",
    "\n",
    "\n",
    "- User model (how do users change their preferences when having been faced with, selected, and consumed certain items?).\n",
    "- Document model: Features of documents and how do documents get pre-selected/sampled.\n",
    "- Reward functions.\n",
    "\n",
    "RLlib comes with 3 off-the-shelf RecSim environments that are ready for training (with RLlib):\n",
    "* Long Term Satisfaction (<- the \"env\" we will use in this tutorial)\n",
    "* Interest Evolution\n",
    "* Interest Exploration\n",
    "\n",
    "<a id='dissecting_lte'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3363126-0f38-4f92-a031-1ea791b9a747",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c3363126-0f38-4f92-a031-1ea791b9a747",
    "outputId": "75ecf413-fc95-4596-de9b-2add9fe3ea5b"
   },
   "outputs": [],
   "source": [
    "# Import the built-in RecSim exapmle environment: \"Long Term Satisfaction\", ready to be trained by RLlib.\n",
    "from ray.rllib.examples.env.recommender_system_envs_with_recsim import LongTermSatisfactionRecSimEnv\n",
    "\n",
    "# Create a RecSim instance using the following config parameters (very similar to what we used above in our own recommender system env):\n",
    "lts_10_1_env = LongTermSatisfactionRecSimEnv({\n",
    "    \"num_candidates\": 10,  # The number of possible documents/videos/candidates that we can recommend\n",
    "    \"slate_size\": 1, # The number of recommendations that we will be making\n",
    "    # Set to False for re-using the same candidate doecuments each timestep.\n",
    "    \"resample_documents\": False,\n",
    "    # Convert MultiDiscrete actions to Discrete (flatten action space).\n",
    "    # e.g. slate_size=2 and num_candidates=10 -> MultiDiscrete([10, 10]) -> Discrete(100)  # 10x10\n",
    "    \"convert_to_discrete_action_space\": True,\n",
    "})\n",
    "\n",
    "# What are our spaces?\n",
    "print(f\"observation space = {lts_10_1_env.observation_space}\")\n",
    "print(f\"action space = {lts_10_1_env.action_space}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3eac27a-bc9a-47dd-b3f9-cffd3e590e3b",
   "metadata": {
    "id": "a3eac27a-bc9a-47dd-b3f9-cffd3e590e3b"
   },
   "source": [
    "Let's make use of our knowledge on the gym.Env API and call our new environment's `reset()` and `step()` methods.\n",
    "First: `reset()` to receive the initial observation in a new episode/trajectory/session:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "913d34a6-fac6-4436-b1d5-9292ebf88006",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "913d34a6-fac6-4436-b1d5-9292ebf88006",
    "outputId": "3b4635e1-599b-4a44-be17-2b9f46eb279e"
   },
   "outputs": [],
   "source": [
    "# Start a new episode and look at initial observation.\n",
    "obs = lts_10_1_env.reset()\n",
    "pprint(obs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9857197-f79f-4f9d-8e3f-68eee20daf94",
   "metadata": {
    "id": "e9857197-f79f-4f9d-8e3f-68eee20daf94"
   },
   "source": [
    "Now let's play RL agent ourselves and recommend some items (pick some actions) via the environment's `step()` method:\n",
    "\n",
    "**Task:** Execute the following cell a couple of times chosing different actions (from 0 - 9) to be sent into the environment's `step()` method. Each time, look at the returned next observation, reward, and `done` flag and write down what you find interesting about the dynamics and observations of this environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f10b83e-993f-4c59-8e13-4e1074bfd7af",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1f10b83e-993f-4c59-8e13-4e1074bfd7af",
    "outputId": "e672004b-73d9-42ce-e22a-8e5a07cfe8cb"
   },
   "outputs": [],
   "source": [
    "# Let's send our first action (1-slate back into the env) using the env's `step()` method.\n",
    "action = 3  # Discrete(10): 0-9 are all valid actions\n",
    "\n",
    "# This method returns 4 items:\n",
    "# - next observation (after having applied the action)\n",
    "# - reward (after having applied the action)\n",
    "# - `done` flag; if True, the episode is terminated and the environment needs to be `reset()` again.\n",
    "# - info dict (we'll ignore this)\n",
    "next_obs, reward, done, _ = lts_10_1_env.step(action)\n",
    "\n",
    "# Print out the next observation.\n",
    "# We expect the \"doc\" and \"user\" items to be the same as in the previous observation\n",
    "# b/c we set \"resample_documents\" to False.\n",
    "pprint(next_obs)\n",
    "# Print out rewards and the vlaue of the `done` flag.\n",
    "print(f\"reward = {reward:.2f}; done = {done}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "444c7982-b372-4fe9-806a-18a29101c094",
   "metadata": {
    "id": "444c7982-b372-4fe9-806a-18a29101c094",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "\n",
    "### What have we learnt from experimenting with the environment?\n",
    "\n",
    "* User's state (if any) is hidden to agent (not part of observation).\n",
    "* Episodes seem to last at least n timesteps -> user seems to have some time budget to spend.\n",
    "* User always seems to click, no matter what we recommend.\n",
    "* Reward seems to be always identical to the \"engagement\" value (of the clicked item). These values range somewhere between 0.0 and 20.0+.\n",
    "* Weak suspicion: If we always recommend the item with the highest feature value, rewards seem to taper off over time - in most of the episodes.\n",
    "* Weak suspicion: If we always recommend the item with the lowest feature value, rewards seem to increase over time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a05210c-69ea-4c09-acf8-831fffca5f8c",
   "metadata": {
    "id": "8a05210c-69ea-4c09-acf8-831fffca5f8c"
   },
   "source": [
    "### What the environment actually does under the hood\n",
    "\n",
    "Let's take a quick look at a pre-configured RecSim environment: \"Long Term Satisfaction\".\n",
    "\n",
    "<td> <img src=https://drive.google.com/uc?id=14BHvEMPrXWGuW6gZVFMuJnEJnuvkwhUu width=1200/> </td>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5958ff84-f7d0-45c9-aa43-b807243b8452",
   "metadata": {
    "id": "5958ff84-f7d0-45c9-aa43-b807243b8452"
   },
   "source": [
    "Now that we know, that there is a double objective built into the env (a. sweetness -> engagement; b. sweetness -> unhappyness; unhappyness -> low engagement), let's make this effect a tiny bit stronger by slightly modifying the environment. As said above, the effect is very weak and almost not measurable, which is a problem on the env's side. We can use this following `gym.ObservationWrapper` class in the cell below to \"fix\" that problem:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "375a469e-aa46-4038-9df7-02cabccdad50",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "375a469e-aa46-4038-9df7-02cabccdad50",
    "outputId": "d6db808d-98d6-4fa9-c50a-9555bade705a"
   },
   "outputs": [],
   "source": [
    "# Modifying wrapper around the LTS (Long Term Satisfaction) env:\n",
    "# - allows us to tweak the user model (and thus: reward behavior)\n",
    "# - adds user's current satisfaction value to observation\n",
    "\n",
    "class LTSWithStrongerDissatisfactionEffect(gym.ObservationWrapper):\n",
    "\n",
    "    def __init__(self, env):\n",
    "        # Tweak incoming environment.\n",
    "        env.environment._user_model._user_sampler._state_parameters.update({\n",
    "            \"sensitivity\": 0.058,\n",
    "            \"time_budget\": 120,\n",
    "            \"choc_stddev\": 0.1,\n",
    "            \"kale_stddev\": 0.1,\n",
    "            #\"innovation_stddev\": 0.01,\n",
    "            #\"choc_mean\": 1.25,\n",
    "            #\"kale_mean\": 1.0,\n",
    "            #\"memory_discount\": 0.9,\n",
    "        })\n",
    "\n",
    "        super().__init__(env)\n",
    "\n",
    "        # Adjust observation space.\n",
    "        if \"response\" in self.observation_space.spaces:\n",
    "            self.observation_space.spaces[\"user\"] = gym.spaces.Box(0.0, 1.0, (1, ), dtype=np.float32)\n",
    "            for r in self.observation_space[\"response\"]:\n",
    "                if \"engagement\" in r.spaces:\n",
    "                    r.spaces[\"watch_time\"] = r.spaces[\"engagement\"]\n",
    "                    del r.spaces[\"engagement\"]\n",
    "                    break\n",
    "\n",
    "    def observation(self, observation):\n",
    "        if \"response\" in self.observation_space.spaces:\n",
    "            observation[\"user\"] = np.array([self.env.environment._user_model._user_state.satisfaction])\n",
    "            for r in observation[\"response\"]:\n",
    "                if \"engagement\" in r:\n",
    "                    r[\"watch_time\"] = r[\"engagement\"]\n",
    "                    del r[\"engagement\"]\n",
    "        return observation\n",
    "\n",
    "\n",
    "# Add the wrapping around \n",
    "tune.register_env(\"modified_lts\", lambda env_config: LTSWithStrongerDissatisfactionEffect(LongTermSatisfactionRecSimEnv(env_config)))\n",
    "\n",
    "print(\"ok; registered the string 'modified_lts' to be used in RLlib configs (see below)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbce752b-68a3-4a84-aada-97810039e4e8",
   "metadata": {
    "id": "dbce752b-68a3-4a84-aada-97810039e4e8"
   },
   "source": [
    "Now that we have a stronger effect of the user's satisfaction value on the long-term rewards, we may be able to measure this effect reliably\n",
    "using the following utility code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b1f1ab8-6b08-47c7-9dfa-3fe9bd672c14",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3b1f1ab8-6b08-47c7-9dfa-3fe9bd672c14",
    "outputId": "7540fffe-4ff8-4995-8714-b577c499fcb5"
   },
   "outputs": [],
   "source": [
    "# This cell should help you with your own analysis of the two above \"suspicions\":\n",
    "# Always chosing the highest/lowest-valued action will lead to a decrease/increase in rewards over the course of an episode.\n",
    "modified_lts_10_1_env = LTSWithStrongerDissatisfactionEffect(lts_10_1_env)\n",
    "\n",
    "# Capture slopes of all trendlines over all episodes.\n",
    "slopes = []\n",
    "# Run 1000 episodes.\n",
    "for _ in range(1000):\n",
    "    obs = modified_lts_10_1_env.reset()  # Reset environment to get initial observation:\n",
    "\n",
    "    # Compute actions that pick doc with highest/lowest feature value.\n",
    "    action_sweetest = np.argmax([value for _, value in obs[\"doc\"].items()])\n",
    "    action_kaleiest = np.argmin([value for _, value in obs[\"doc\"].items()])\n",
    "\n",
    "    # Play one episode.\n",
    "    done = False\n",
    "    rewards = []\n",
    "    while not done:\n",
    "        #action = action_sweetest\n",
    "        action = action_kaleiest\n",
    "        #action = np.random.choice([action_kaleiest, action_sweetest])\n",
    "\n",
    "        obs, reward, done, _ = modified_lts_10_1_env.step(action)\n",
    "        rewards.append(reward)\n",
    "\n",
    "    # Create linear model of rewards over time.\n",
    "    reward_linreg = linregress(np.array((range(len(rewards)))), np.array(rewards))\n",
    "    slopes.append(reward_linreg.slope)\n",
    "\n",
    "print(np.mean(slopes))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be848212-87b8-4eb1-9353-74e09ae72310",
   "metadata": {
    "id": "be848212-87b8-4eb1-9353-74e09ae72310"
   },
   "source": [
    "## Measuring random baseline of our environment\n",
    "\n",
    "In the cells above, we created a new environment instance (`lts_10_1_env`). As we have seen above, in order to start \"walking\" through a recommender system episode, we need to perform `reset()` and then several `step()` calls (with different actions) until the returned `done` flag is True.\n",
    "\n",
    "Let's find out how well a randomly acting agent performs in this environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "spatial-geography",
   "metadata": {
    "id": "spatial-geography"
   },
   "outputs": [],
   "source": [
    "# Function that measures and outputs the random baseline reward.\n",
    "# This is the expected accumulated reward per episode, if we act randomly (recommend random items) at each time step.\n",
    "def measure_random_performance_for_env(env, episodes=1000, verbose=False):\n",
    "\n",
    "    # Reset the env.\n",
    "    env.reset()\n",
    "\n",
    "    # Number of episodes already done.\n",
    "    num_episodes = 0\n",
    "    # Current episode's accumulated reward.\n",
    "    episode_reward = 0.0\n",
    "    # Collect all episode rewards here to be able to calculate a random baseline reward.\n",
    "    episode_rewards = []\n",
    "\n",
    "    # Enter while loop (to step through the episode).\n",
    "    while num_episodes < episodes:\n",
    "        # Produce a random action.\n",
    "        action = env.action_space.sample()\n",
    "\n",
    "        # Send the action to the env's `step()` method to receive: obs, reward, done, and info.\n",
    "        obs, reward, done, _ = env.step(action)\n",
    "        episode_reward += reward\n",
    "\n",
    "        # Check, whether the episde is done, if yes, reset and increase episode counter.\n",
    "        if done:\n",
    "            if verbose:\n",
    "                print(f\"Episode done - accumulated reward={episode_reward}\")\n",
    "            elif num_episodes % 100 == 0:\n",
    "                print(f\" {num_episodes} \", end=\"\")\n",
    "            elif num_episodes % 10 == 0:\n",
    "                print(\".\", end=\"\")\n",
    "            num_episodes += 1\n",
    "            env.reset()\n",
    "            episode_rewards.append(episode_reward)\n",
    "            episode_reward = 0.0\n",
    "\n",
    "    # Print out and return mean episode reward (and standard error of the mean).\n",
    "    env_mean_random_reward = np.mean(episode_rewards)\n",
    "\n",
    "    print(f\"\\n\\nMean episode reward when acting randomly: {env_mean_random_reward:.2f}+/-{sem(episode_rewards):.2f}\")\n",
    "\n",
    "    return env_mean_random_reward, sem(episode_rewards)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e6e63e6-d030-4a45-af5b-ab88eaef3969",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0e6e63e6-d030-4a45-af5b-ab88eaef3969",
    "outputId": "212896b4-4ed4-41f2-c53c-5cd18b837744"
   },
   "outputs": [],
   "source": [
    "%time\n",
    "# Let's create a somewhat tougher version of this with 20 candidates (instead of 10) and a slate-size of 2.\n",
    "# We'll also keep using our wrapper from above to strengthen the dissatisfaction effect on the engagement:\n",
    "lts_20_2_env = LTSWithStrongerDissatisfactionEffect(LongTermSatisfactionRecSimEnv(config={\n",
    "    \"num_candidates\": 20,\n",
    "    \"slate_size\": 2,  # MultiDiscrete([20, 20]) -> Discrete(400)\n",
    "    \"resample_documents\": True,\n",
    "    # Convert to Discrete action space.\n",
    "    \"convert_to_discrete_action_space\": True,\n",
    "    # Wrap observations for RLlib bandit: Only changes dict keys (\"item\" instead of \"doc\").\n",
    "    \"wrap_for_bandits\": True,\n",
    "}))\n",
    "\n",
    "lts_20_2_env_mean_random_reward, _ = measure_random_performance_for_env(lts_20_2_env, episodes=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b20ac75-f3e6-4975-a209-2bf110b4ee13",
   "metadata": {
    "id": "5b20ac75-f3e6-4975-a209-2bf110b4ee13"
   },
   "source": [
    "# Plugging in RLlib\n",
    "<a id='rllib'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a76f02f-ef66-484d-8a1a-074a6e25c84a",
   "metadata": {
    "id": "8a76f02f-ef66-484d-8a1a-074a6e25c84a"
   },
   "source": [
    "## Picking an RLlib algorithm (\"Trainer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65aa24b2-ac17-44a3-b7b1-274ce2f50a87",
   "metadata": {
    "id": "65aa24b2-ac17-44a3-b7b1-274ce2f50a87"
   },
   "source": [
    "https://docs.ray.io/en/master/rllib-algorithms.html#available-algorithms-overview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0194b33a-e031-49ce-9ff2-b32e328f9955",
   "metadata": {
    "id": "0194b33a-e031-49ce-9ff2-b32e328f9955"
   },
   "source": [
    "\n",
    "\n",
    "<td> <img src=https://drive.google.com/uc?id=1CvhB59H2PsmKeFy-4hZuhRVHxymOH0hd /> </td>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62b1b0b3-ec96-41c0-9d5b-93db1c5ce021",
   "metadata": {
    "id": "62b1b0b3-ec96-41c0-9d5b-93db1c5ce021"
   },
   "source": [
    "### Trying a \"Contextual n-armed Bandit\" on our environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a26e094-9887-4fc6-88b6-d1448e931526",
   "metadata": {
    "id": "4a26e094-9887-4fc6-88b6-d1448e931526"
   },
   "outputs": [],
   "source": [
    "# In order to use one of the above algorithms, you may instantiate its associated Trainer class.\n",
    "# For example, to import a Bandit Trainer w/ Upper Confidence Bound (UCB) exploration, do:\n",
    "\n",
    "from ray.rllib.agents.bandit import BanditLinUCBTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0911f212-523e-4a75-846d-342dd2a681a6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0911f212-523e-4a75-846d-342dd2a681a6",
    "outputId": "cf904666-3fc2-4a83-8147-28c9121aafde"
   },
   "outputs": [],
   "source": [
    "# Configuration dicts for RLlib Trainers.\n",
    "# Where are the default configuration dicts stored?\n",
    "\n",
    "# E.g. Bandit algorithms:\n",
    "from ray.rllib.agents.bandit.bandit import DEFAULT_CONFIG as BANDIT_DEFAULT_CONFIG\n",
    "print(f\"Bandit's default config is:\")\n",
    "pprint(BANDIT_DEFAULT_CONFIG)\n",
    "\n",
    "# DQN algorithm:\n",
    "#from ray.rllib.agents.dqn import DEFAULT_CONFIG as DQN_DEFAULT_CONFIG\n",
    "#print(f\"DQN's default config is:\")\n",
    "#pprint(DQN_DEFAULT_CONFIG)\n",
    "\n",
    "# Common (all algorithms).\n",
    "#from ray.rllib.agents.trainer import COMMON_CONFIG\n",
    "#print(f\"RLlib Trainer's default config is:\")\n",
    "#pprint(COMMON_CONFIG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9bd9775-f2bb-41d9-8ff6-20be9abd68db",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c9bd9775-f2bb-41d9-8ff6-20be9abd68db",
    "outputId": "5af8d60a-6770-4008-a0cd-ddbc1965b8a8"
   },
   "outputs": [],
   "source": [
    "bandit_config = {\n",
    "    \"env\": \"modified_lts\",\n",
    "    \"env_config\": {\n",
    "        \"num_candidates\": 20,  # 20x19 = ~400 unique slates (arms)\n",
    "        \"slate_size\": 2,\n",
    "        \"resample_documents\": True,\n",
    "\n",
    "        # Bandit-specific flags:\n",
    "        \"convert_to_discrete_action_space\": True,\n",
    "        # Convert \"doc\" key into \"item\" key.\n",
    "        \"wrap_for_bandits\": True,\n",
    "        # Use consistent seeds for the environment ...\n",
    "        \"seed\": 0,\n",
    "    },\n",
    "    # ... and the Trainer itself.\n",
    "    \"seed\": 0,\n",
    "\n",
    "    # The following settings are affecting the reporting only:\n",
    "    # ---\n",
    "    # Generate a result dict every single time step.\n",
    "    \"timesteps_per_iteration\": 1,\n",
    "    # Report rewards as smoothed mean over this many episodes.\n",
    "    \"metrics_num_episodes_for_smoothing\": 200,\n",
    "}\n",
    "\n",
    "# Create the RLlib Trainer using above config.\n",
    "bandit_trainer = BanditLinUCBTrainer(config=bandit_config)\n",
    "bandit_trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46a22cc0-0efb-40be-85fe-720e62a7a419",
   "metadata": {
    "id": "46a22cc0-0efb-40be-85fe-720e62a7a419"
   },
   "source": [
    "#### Running a single training iteration, by calling the `.train()` method:\n",
    "\n",
    "One iteration for most algos involves:\n",
    "\n",
    "1. Sampling from the environment(s)\n",
    "1. Using the sampled data (observations, actions taken, rewards) to update the policy model (e.g. a neural network), such that it would pick better actions in the future, leading to higher rewards.\n",
    "\n",
    "Let's try it out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddd18251-2a1a-4822-8744-ca6df4a14787",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ddd18251-2a1a-4822-8744-ca6df4a14787",
    "outputId": "74abdd34-7dcb-4ada-ccf2-2bdbb7687a91"
   },
   "outputs": [],
   "source": [
    "# Perform single `.train()` call.\n",
    "result = bandit_trainer.train()\n",
    "# Erase config dict from result (for better overview).\n",
    "del result[\"config\"]\n",
    "# Print out training iteration results.\n",
    "pprint(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d6f089b-e0cc-47de-af9b-dc05a71e102f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 513
    },
    "id": "6d6f089b-e0cc-47de-af9b-dc05a71e102f",
    "outputId": "e5368ef3-78d2-4973-c6b1-1dc6e7f9191d"
   },
   "outputs": [],
   "source": [
    "# Train for n more iterations (timesteps) and collect n-arm rewards.\n",
    "rewards = []\n",
    "for i in range(3000):\n",
    "    # Run a single timestep in the environment and update\n",
    "    # the model immediately on the received reward.\n",
    "    result = bandit_trainer.train()\n",
    "    # Extract reward from results.\n",
    "    #rewards.extend(result[\"hist_stats\"][\"episode_reward\"]\n",
    "    rewards.append(result[\"episode_reward_mean\"])\n",
    "    if i % 500 == 0:\n",
    "        print(f\" {i} \", end=\"\")\n",
    "    elif i % 100 == 0:\n",
    "        print(\".\", end=\"\")\n",
    "\n",
    "# Plot per-timestep (episode) rewards.\n",
    "plt.figure(figsize=(10,7))\n",
    "start_at = 0\n",
    "smoothing_win = 200\n",
    "x = list(range(start_at, len(rewards)))\n",
    "y = [np.nanmean(rewards[max(i - smoothing_win, 0):i + 1]) for i in range(start_at, len(rewards))]\n",
    "plt.plot(x, y)\n",
    "plt.title(\"Mean reward\")\n",
    "plt.xlabel(\"Time/Training steps\")\n",
    "\n",
    "# Add mean random baseline reward (red line).\n",
    "plt.axhline(y=lts_20_2_env_mean_random_reward, color=\"r\", linestyle=\"-\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b77c3cf-226c-4cab-82c6-7d8a54bfe9ea",
   "metadata": {
    "id": "8b77c3cf-226c-4cab-82c6-7d8a54bfe9ea"
   },
   "source": [
    "<a id='bandit_results'></a>\n",
    "### What does our trained Bandit actually recommend?\n",
    "\n",
    "The first method of the RLlib Trainer API we used above was `train()`.\n",
    "We'll now use another method of the Trainer, `compute_single_action(input_dict={})`.\n",
    "It takes a input_dict keyword arg, into which you may pass a single (unbatched!) observation to receive an action for:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eb62acb-0a16-4412-949a-4851201620bf",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6eb62acb-0a16-4412-949a-4851201620bf",
    "outputId": "e12959d4-ae02-4081-8343-b27ad5c50a9e"
   },
   "outputs": [],
   "source": [
    "# Let's see what items our bandit recommends now that it has been trained and achieves good (>> random) rewards.\n",
    "obs = lts_20_2_env.reset()\n",
    "\n",
    "# Run a single episode.\n",
    "done = False\n",
    "while not done:\n",
    "    # Pass the single (unbatched) observation into the `compute_single_action` method of our Trainer.\n",
    "    # This is one way to perform inference on a learned policy.\n",
    "    action = bandit_trainer.compute_single_action(input_dict={\"obs\": obs})\n",
    "    feat_value_of_action = obs[\"item\"][action][0]\n",
    "    max_choc_feat = obs['item'][np.argmax(obs[\"item\"])][0]\n",
    "\n",
    "    # Print out the picked document's feature value and compare that to the highest possible feature value.\n",
    "    print(f\"action's feature value={feat_value_of_action}; max-choc-feature={max_choc_feat}; \")\n",
    "\n",
    "    # Apply the computed action in the environment and continue.\n",
    "    obs, r, done, _ = lts_20_2_env.step(action)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9355c1b-f0f7-4690-a7fe-332b01a651c4",
   "metadata": {
    "id": "c9355c1b-f0f7-4690-a7fe-332b01a651c4"
   },
   "source": [
    "### Ok, Bandits want Chocolate! :)\n",
    "#### Why is that?\n",
    "\n",
    "<td> <img src=https://drive.google.com/uc?id=1qOMMI80Wn8p1jbqEdtM-vnIA9wO1KX1y width=1000/> </td>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84291b69-050f-489a-b822-239294bb3a2e",
   "metadata": {
    "id": "84291b69-050f-489a-b822-239294bb3a2e"
   },
   "source": [
    "### Recap: Advantages and Disatvantages of Bandits:\n",
    "#### Advantages:\n",
    "* Very fast\n",
    "* Very sample-efficient\n",
    "* Easy to understand learning process\n",
    "\n",
    "#### Disadvantages\n",
    "* Need immediate reward (not capable of solving long-horizon credit assignment problem)\n",
    "* Models user -> If > 1 user, must train separate bandit per user\n",
    "* Not able to handle components of MultiDiscrete action space separately (works only on flattened Discrete action space)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "108a830b-cc5f-454c-b986-75c59d40df89",
   "metadata": {
    "id": "108a830b-cc5f-454c-b986-75c59d40df89"
   },
   "source": [
    "<a id='slateq'></a>\n",
    "# Switching to Slate-Q\n",
    "\n",
    "\n",
    "https://docs.ray.io/en/master/rllib-algorithms.html#available-algorithms-overview\n",
    "<td> <img src=https://drive.google.com/uc?id=1oEVD57X1MD7Z7D3roOz0kWj-hUUWydwe width=800/> </td>\n",
    "\n",
    "The RLlib team has implemented the Slate-Q algorithm from Google - designed for k-slate, long time horizon, and dynamic user recommendation problems. \n",
    "<br>\n",
    "\n",
    "Slate-Q Paper: https://arxiv.org/pdf/1812.02353.pdf <br>\n",
    "Author video about Slate-Q: https://slideslive.com/38917655/reinforcement-learning-in-recommender-systems-some-challenges\n",
    "<br><br>\n",
    "\n",
    "<td> <img src=https://drive.google.com/uc?id=1iG1-lMP6jwh1rgodQoTcO5l67MgHy7qc width=1000/> </td>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32828a91-2da7-4f5f-9374-d12f32ec0b87",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "32828a91-2da7-4f5f-9374-d12f32ec0b87",
    "outputId": "c5da9bf4-f048-4050-a449-188dbc2d87bd"
   },
   "outputs": [],
   "source": [
    "from ray.rllib.agents.slateq import SlateQTrainer, DEFAULT_CONFIG\n",
    "\n",
    "slateq_config = {\n",
    "    \"env\": \"modified_lts\",\n",
    "    \"env_config\": {\n",
    "        \"num_candidates\": 20,  # MultiDiscrete([20, 20]) -> no flattening necessary (see `convert_to_discrete_action_space=False` below)\n",
    "        \"slate_size\": 2,\n",
    "        \"resample_documents\": True,\n",
    "        \"wrap_for_bandits\": False,  # SlateQ != Bandit (will keep \"doc\" key, instead of \"items\")\n",
    "        \"convert_to_discrete_action_space\": False,  # SlateQ handles MultiDiscrete action spaces (slate recommendations).\n",
    "    },\n",
    "    # Setup exploratory behavior: Implemented as \"epsilon greedy\" strategy:\n",
    "    # Act randomly `e` percent of the time; `e` gets reduced from 1.0 to almost 0.0 over\n",
    "    # the course of `epsilon_timesteps`.\n",
    "    \"exploration_config\": {\n",
    "        #\"warmup_timesteps\": 20000,  # default\n",
    "        \n",
    "        # Use Ray Tune to run 3 parallel tuning trials\n",
    "        # \"epsilon_timesteps\": tune.grid_search([40000, 2000, 3000]),  # default: 250000\n",
    "        \n",
    "        # Do not use Ray Tune\n",
    "        \"epsilon_timesteps\": 60000\n",
    "    },\n",
    "    #\"learning_starts\": 20000,  # default\n",
    "    \"target_network_update_freq\": 3200,\n",
    "\n",
    "    # Report rewards as smoothed mean over this many episodes.\n",
    "    \"metrics_num_episodes_for_smoothing\": 200,\n",
    "}\n",
    "\n",
    "# Instantiate the Trainer object using the exact same config as in our Bandit experiment above.\n",
    "slateq_trainer = SlateQTrainer(config=slateq_config)\n",
    "slateq_trainer\n",
    "\n",
    "# # You can change timesteps_total here to see more tuning\n",
    "# tune.run(\"SlateQ\", config=slateq_config, stop={\"timesteps_total\":1000, \n",
    "#                                                \"training_iteration\":5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0845c458-d656-417e-b3d8-60596650c2bb",
   "metadata": {
    "id": "0845c458-d656-417e-b3d8-60596650c2bb"
   },
   "outputs": [],
   "source": [
    "# Optional - View the default configs of slateq\n",
    "# DEFAULT_CONFIG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95395f1a-31c6-4933-b09a-d06959ad5714",
   "metadata": {
    "id": "95395f1a-31c6-4933-b09a-d06959ad5714"
   },
   "source": [
    "<a id='slateq_experiment'></a>\n",
    "Now that we have confirmed we have setup the Trainer correctly, let's call `train()` on it several times:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc47c75f-4f6f-4806-995e-80ec974cfd86",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dc47c75f-4f6f-4806-995e-80ec974cfd86",
    "outputId": "636becd4-2f93-463e-f8c2-9b4843af1513"
   },
   "outputs": [],
   "source": [
    "# See reward progress with time\n",
    "# Run `train()` n times. Repeatedly call `train()` now to see rewards increase.\n",
    "for _ in range(60):\n",
    "    results = slateq_trainer.train()\n",
    "    print(f\"Iteration={slateq_trainer.iteration}; ts={results['timesteps_total']}: R(\\\"return\\\")={results['episode_reward_mean']}\")\n",
    "    \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b86aecb-90ce-4be1-91a2-5c5391ab6adf",
   "metadata": {
    "id": "7b86aecb-90ce-4be1-91a2-5c5391ab6adf"
   },
   "source": [
    "------------------\n",
    "## 10 min break :)\n",
    "\n",
    "while Slate-Q is (hopefully) leaning\n",
    "\n",
    "------------------\n",
    "\n",
    "<a id='slateq_results'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34bc1113-bd5e-45f5-bd8e-93931b8cee0b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "34bc1113-bd5e-45f5-bd8e-93931b8cee0b",
    "outputId": "a53677eb-4887-42c8-b61e-617b0224c2fb"
   },
   "outputs": [],
   "source": [
    "# Let's see what items our SlateQ recommends now that it has been lightly trained\n",
    "lts_20_2_env = LTSWithStrongerDissatisfactionEffect(LongTermSatisfactionRecSimEnv(config=slateq_config[\"env_config\"]))\n",
    "\n",
    "obs = lts_20_2_env.reset()\n",
    "done = False\n",
    "while not done:\n",
    "    action = slateq_trainer.compute_single_action(input_dict={\"obs\": obs})\n",
    "    feat_value_of_action = obs[\"doc\"][str(action[0])][0]\n",
    "    max_feat_action = np.argmax([value for _, value in obs[\"doc\"].items()])\n",
    "    max_choc_feat = obs['doc'][str(max_feat_action)][0]\n",
    "    # Print out the picked document's feature value and compare that to the highest possible feature value.\n",
    "    print(f\"action's feature value={feat_value_of_action} max-choc-feature={max_choc_feat}\")\n",
    "    \n",
    "    obs, r, done, _ = lts_20_2_env.step(action)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f409efcd-9c5c-4d91-a1ae-121b1b2fa698",
   "metadata": {
    "id": "f409efcd-9c5c-4d91-a1ae-121b1b2fa698"
   },
   "source": [
    "### Optional\n",
    "\n",
    "Feel free to play around with the following code in order to learn how RLlib - under the hood - calculates actions from the environment's observations using the SlateQ Policy and its NN models inside our Trainer object):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2b62d74-6392-453a-b25f-f8cbc90009d6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b2b62d74-6392-453a-b25f-f8cbc90009d6",
    "outputId": "bae3a3b8-2882-4eee-b6f1-3487bce38130"
   },
   "outputs": [],
   "source": [
    "# To get the policy inside the Trainer, use `Trainer.get_policy([policy ID]=\"default_policy\")`:\n",
    "policy = slateq_trainer.get_policy()\n",
    "print(f\"Our Policy right now is: {policy}\")\n",
    "\n",
    "# To get to the model inside any policy, do:\n",
    "model = policy.model\n",
    "#print(f\"Our Policy's model is: {model}\")\n",
    "\n",
    "# Print out the policy's action and observation spaces.\n",
    "print(f\"Our Policy's observation space is: {policy.observation_space}\\n\")\n",
    "print(f\"Our Policy's action space is: {policy.action_space}\\n\")\n",
    "\n",
    "# Produce a random obervation (B=1; batch of size 1).\n",
    "obs = lts_20_2_env.observation_space.sample()\n",
    "\n",
    "# tf-specific code: Use tf1.Session().\n",
    "sess = policy.get_session()\n",
    "\n",
    "# Get the action logits (as torch tensor).\n",
    "with sess.graph.as_default():\n",
    "    q_values_per_candidate = model.q_value_head([\n",
    "        np.expand_dims(obs[\"user\"], 0),\n",
    "        np.expand_dims(np.concatenate([value for value in obs[\"doc\"].values()]), 0),\n",
    "    ])\n",
    "print(f\"q_values_per_candidate={sess.run(q_values_per_candidate)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de603d14-f0cb-4363-a72b-8f147c094071",
   "metadata": {
    "id": "de603d14-f0cb-4363-a72b-8f147c094071"
   },
   "source": [
    "### Stop the RLlib Trainer\n",
    "\n",
    "In order to release all resources from a Trainer, you can use a Trainer's `stop()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "737dca4f-942f-4fda-abcc-0052263a103b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "737dca4f-942f-4fda-abcc-0052263a103b",
    "outputId": "da8a0b45-6af8-432e-a0ff-3300c256fc80"
   },
   "outputs": [],
   "source": [
    "# In order to release resources that a Trainer uses, you can call its `stop()` method:\n",
    "slateq_trainer.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecce74b8-20ed-43c5-ad88-54a2dec32f71",
   "metadata": {
    "id": "ecce74b8-20ed-43c5-ad88-54a2dec32f71",
    "tags": []
   },
   "source": [
    "### Recap: Advantages and Disadvantages of SlateQ:\n",
    "#### Advantages:\n",
    "* Decomposes MultiDiscrete action space (better understanding of items inside a k-slate)\n",
    "* Handles long-horizon credit assignment better than bandits (Q-learning)\n",
    "* Handles > 1 user problems\n",
    "* Sample efficient (due to replay buffer + off-policy DQN-style learning)\n",
    "\n",
    "#### Disadvantages\n",
    "* Uses larger (deep) model(s): One Q-value NN head per candidate\n",
    "* Slower and heavier feel to it\n",
    "* Requires careful hyperparameter-tuning, e.g. exploration timesteps."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e00f8e5a-d8a8-451d-bb97-b2000dbb2f9d",
   "metadata": {
    "id": "e00f8e5a-d8a8-451d-bb97-b2000dbb2f9d"
   },
   "source": [
    "## Time for Q&A\n",
    "\n",
    "...\n",
    "\n",
    "## Thank you for listening and participating!\n",
    "\n",
    "### Here are a couple of links that you may find useful.\n",
    "\n",
    "- The <a href=\"https://github.com/sven1977/rllib_tutorials/tree/main/production_rl_2022\">github repo of Sven's tutorials</a>.\n",
    "- <a href=\"https://docs.ray.io/en/latest/rllib/index.html\">RLlib's documentation main page</a>.\n",
    "- <a href=\"http://discuss.ray.io\">Our discourse forum</a> to ask questions on Ray and its libraries.\n",
    "- Our <a href=\"https://forms.gle/9TSdDYUgxYs8SA9e8\">Slack channel</a> for interacting with other Ray RLlib users.\n",
    "- The <a href=\"https://github.com/ray-project/ray/blob/master/rllib/examples/\">RLlib examples scripts folder</a> with tons of examples on how to do different stuff with RLlib.\n",
    "- A <a href=\"https://medium.com/distributed-computing-with-ray/reinforcement-learning-with-rllib-in-the-unity-game-engine-1a98080a7c0d\">blog post on training with RLlib inside a Unity3D environment</a>.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "u6ej2NZcIvrv",
   "metadata": {
    "id": "u6ej2NZcIvrv"
   },
   "source": [
    "# Shutdown Ray if you are finished"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfac2bd7-8a01-4a89-a899-c5a33b7ada4c",
   "metadata": {
    "id": "dfac2bd7-8a01-4a89-a899-c5a33b7ada4c"
   },
   "outputs": [],
   "source": [
    "# if you are done with Ray\n",
    "if ray.is_initialized():\n",
    "    ray.shutdown()\n",
    "\n",
    "# # note this is how to start ray manually on your laptop\n",
    "# ray.init()\n",
    "\n",
    "# # note this is how to start ray on a cloud\n",
    "# ray.init(anyscale_cluster_name, [cluster_env | runtime_env] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf9e69d7-fb7c-4b34-8768-dfe423fa61c9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Copy of tutorial_notebook.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
