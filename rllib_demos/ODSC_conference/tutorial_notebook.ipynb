{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/christy/AnyscaleDemos/blob/main/rllib_demos/ODSC_conference/tutorial_notebook.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://github.com/christy/AnyscaleDemos/blob/main/rllib_demos/ODSC_conference/tutorial_notebook.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />View source on GitHub</a>\n",
        "  </td>\n",
        "</table>"
      ],
      "metadata": {
        "id": "3o3rZytugfo6"
      },
      "id": "3o3rZytugfo6"
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install \"ray[rllib,serve]==1.12\" sklearn tensorflow gputil recsim gym==0.21"
      ],
      "metadata": {
        "id": "_BD9NRNPltO8"
      },
      "id": "_BD9NRNPltO8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "6aa06051",
      "metadata": {
        "id": "6aa06051"
      },
      "source": [
        "# Reinforcement Learning for Recommender Systems\n",
        "## From Contextual Bandits to Slate-Q\n",
        "\n",
        "\n",
        "\n",
        "<table>\n",
        "<tr>\n",
        "    <td> <img src=https://drive.google.com/uc?id=1jAhSZfGDIcdlBXd6EtfxPhBvV54F16RX style=\"width: 230px;\"/> </td>\n",
        "    <td> <img src=https://drive.google.com/uc?id=1rum1twl4g0nsPJzxDplFjknZHrYbQ3XW style=\"width: 213px;\"/> </td>\n",
        "    <td> <img src=https://drive.google.com/uc?id=1t9w6Z87vd7cgtjxAa3chlGwX_fde03Oy style=\"width: 169px;\"/> </td>\n",
        "    <td> <img src=https://drive.google.com/uc?id=1tVPSOMDzIqrSK7NHEzrYc_xSdEpe6Ibg style=\"width: 254px;\"/> </td>\n",
        "    <td> <img src=https://drive.google.com/uc?id=10kw_frXjhtaLrxYNIHlSAPgAGZ_tNbv- style=\"width: 252px;\"/> </td>\n",
        "</tr>\n",
        "</table>\n",
        "\n",
        "\n",
        "### Overview\n",
        "“Reinforcement Learning for Recommender Systems, From Contextual Bandits to Slate-Q” is a tutorial for industry researchers, domain-experts, and ML-engineers, showcasing ...\n",
        "\n",
        "1) .. how you can use RLlib to build a recommender system **simulator** for your industry applications and run Bandit algorithms and the Slate-Q algorithm against this simulator.\n",
        "\n",
        "2) .. how RLlib's offline algorithms pose solutions in case you **don't have a simulator** of your problem environment at hand.\n",
        "\n",
        "We will further explore how to deploy trained models to production using Ray Serve.\n",
        "\n",
        "During the live-coding phases, we will using a recommender system simulating environment by google's RecSim and configure and run 2 RLlib algorithms against it. We'll also demonstrate how you may use offline RL as a solution for recommender systems and how to deploy a learned policy into production.\n",
        "\n",
        "RLlib offers industry-grade scalability, a large list of algos to choose from (offline, model-based, model-free, etc..), support for TensorFlow and PyTorch, and a unified API for a variety of applications. This tutorial includes a brief introduction to provide an overview of concepts (e.g. why RL?) before proceeding to RLlib (recommender system) environments, neural network models, offline RL, student exercises, Q/A, and more. All code will be provided as .py files in a GitHub repo."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "green-insertion",
      "metadata": {
        "id": "green-insertion"
      },
      "source": [
        "### Intended Audience\n",
        "* Python programmers who are interested in using RL to solve their specific industry decision making problems and who want to get started with RLlib.\n",
        "\n",
        "### Prerequisites\n",
        "* Some Python programming experience.\n",
        "* Some familiarity with machine learning.\n",
        "* *Helpful, but not required:* Experience in reinforcement learning and Ray.\n",
        "* *Helpful, but not required:* Experience with TensorFlow or PyTorch.\n",
        "\n",
        "\n",
        "### Key Takeaways\n",
        "* What is reinforcement learning and RLlib?\n",
        "* How do recommender systems work? How do we build our own?\n",
        "* How do we train RLlib's different algorithms on a recommender system problem?\n",
        "* How do I deploy an already trained policy into production using Ray Serve.\n",
        "\n",
        "\n",
        "### Tutorial Outline\n",
        "\n",
        "1. Reinforcement learning (RL) in a nutshell.\n",
        "1. How to formulate any problem as an RL-solvable one?\n",
        "1. Recommender systems - How they work.\n",
        "1. Why you should use RLlib.\n",
        "\n",
        "(10min break)\n",
        "\n",
        "1. [Google RecSim - Build your own recom sys simulator.](#recsim)\n",
        "1. [Dissecting the \"long term satisfaction\" (LTE) environment.](#dissecting_lte)\n",
        "1. [Using a contextual Bandit algorithm with RLlib and starting our first training run on the LTE env.](#rllib)\n",
        "1. [What did the Bandit learn?](#bandit_results)\n",
        "1. [Intro to Slate-Q.](#slateq)\n",
        "1. [Starting a Slate-Q training run.](#slateq_experiment)\n",
        "\n",
        "(10min break)\n",
        "\n",
        "1. [Analyzing the results of the SlateQ run.](#slateq_results)\n",
        "1. [Ray Serve example: How can we deploy a trained policy into our production environment?](#ray_serve)\n",
        "\n",
        "\n",
        "### Other Recommended Readings\n",
        "* [Intro to RLlib: Example Environments](https://medium.com/distributed-computing-with-ray/intro-to-rllib-example-environments-3a113f532c70)\n",
        "* [Reinforcement Learning with RLlib in the Unity Game Engine](https://medium.com/distributed-computing-with-ray/reinforcement-learning-with-rllib-in-the-unity-game-engine-1a98080a7c0d)\n",
        "\n",
        "<td> <img src=https://drive.google.com/uc?id=1skpzZvJPWLSbGg0xj1obuQegKc0YfwZO width=400> </td>\n",
        "\n",
        "* [Attention Nets and More with RLlib's Trajectory View API](https://medium.com/distributed-computing-with-ray/attention-nets-and-more-with-rllibs-trajectory-view-api-d326339a6e65)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e9f7e21f-f3de-4bad-a3a7-4bbd0b015559",
      "metadata": {
        "id": "e9f7e21f-f3de-4bad-a3a7-4bbd0b015559"
      },
      "source": [
        "# Let's start!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "930deb27-e739-4507-bc24-e39ded9caeb4",
      "metadata": {
        "id": "930deb27-e739-4507-bc24-e39ded9caeb4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c5fae381-70b2-4f56-e1cf-46c6e4b65755"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Python 3.9.12\r\n",
            "ray: 1.12.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/Users/christy/mambaforge/envs/rllib/lib/python3.9/site-packages/jax/_src/lib/__init__.py:33: UserWarning: JAX on Mac ARM machines is experimental and minimally tested. Please see https://github.com/google/jax/issues/5501 in the event of problems.\n",
            "  warnings.warn(\"JAX on Mac ARM machines is experimental and minimally tested. \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf: 2.7.0\n"
          ]
        }
      ],
      "source": [
        "# Let's get started with some basic imports.\n",
        "\n",
        "import ray  # .. of course\n",
        "from ray import serve\n",
        "from ray import tune\n",
        "\n",
        "from collections import OrderedDict\n",
        "import gym  # RL environments and action/observation spaces\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "import pandas\n",
        "from pprint import pprint\n",
        "import re\n",
        "import recsim  # google's RecSim package.\n",
        "import requests\n",
        "from scipy.interpolate import make_interp_spline, BSpline\n",
        "from scipy.stats import linregress, sem\n",
        "from starlette.requests import Request\n",
        "import tree  # dm_tree\n",
        "\n",
        "!python --version\n",
        "print(f\"ray: {ray.__version__}\")\n",
        "import tensorflow as tf\n",
        "print(f\"tf: {tf.__version__}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "90f010a6-7ee3-49b3-814a-2b9455c66c8f",
      "metadata": {
        "id": "90f010a6-7ee3-49b3-814a-2b9455c66c8f"
      },
      "source": [
        "<a id='recsim'></a>\n",
        "## Introducing google RecSim\n",
        "\n",
        "<td> <img src=https://drive.google.com/uc?id=1B9zX184GMYZ6_oLR8p16tSJnzIHofk1g width=600/> </td>\n",
        "\n",
        "<a href=\"https://github.com/google-research/recsim\">Google's RecSim package</a> offers a flexible way for you to <a href=\"https://github.com/google-research/recsim/blob/master/recsim/colab/RecSim_Developing_an_Environment.ipynb\">define the different building blocks of a recommender system</a>:\n",
        "\n",
        "\n",
        "- User model (how do users change their preferences when having been faced with, selected, and consumed certain items?).\n",
        "- Document model: Features of documents and how do documents get pre-selected/sampled.\n",
        "- Reward functions.\n",
        "\n",
        "RLlib comes with 3 off-the-shelf RecSim environments that are ready for training (with RLlib):\n",
        "* Long Term Satisfaction (<- the \"env\" we will use in this tutorial)\n",
        "* Interest Evolution\n",
        "* Interest Exploration\n",
        "\n",
        "<a id='dissecting_lte'></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "c3363126-0f38-4f92-a031-1ea791b9a747",
      "metadata": {
        "id": "c3363126-0f38-4f92-a031-1ea791b9a747",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "75ecf413-fc95-4596-de9b-2add9fe3ea5b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "observation space = Dict(user:Box([], [], (0,), float32), doc:Dict(0:Box([0.], [1.], (1,), float32), 1:Box([0.], [1.], (1,), float32), 2:Box([0.], [1.], (1,), float32), 3:Box([0.], [1.], (1,), float32), 4:Box([0.], [1.], (1,), float32), 5:Box([0.], [1.], (1,), float32), 6:Box([0.], [1.], (1,), float32), 7:Box([0.], [1.], (1,), float32), 8:Box([0.], [1.], (1,), float32), 9:Box([0.], [1.], (1,), float32)), response:Tuple(Dict(click:Discrete(2), engagement:Box(0.0, 100.0, (), float32))))\n",
            "action space = Discrete(10)\n"
          ]
        }
      ],
      "source": [
        "# Import the built-in RecSim exapmle environment: \"Long Term Satisfaction\", ready to be trained by RLlib.\n",
        "from ray.rllib.examples.env.recommender_system_envs_with_recsim import LongTermSatisfactionRecSimEnv\n",
        "\n",
        "# Create a RecSim instance using the following config parameters (very similar to what we used above in our own recommender system env):\n",
        "lts_10_1_env = LongTermSatisfactionRecSimEnv({\n",
        "    \"num_candidates\": 10,  # The number of possible documents/videos/candidates that we can recommend\n",
        "    \"slate_size\": 1, # The number of recommendations that we will be making\n",
        "    # Set to False for re-using the same candidate doecuments each timestep.\n",
        "    \"resample_documents\": False,\n",
        "    # Convert MultiDiscrete actions to Discrete (flatten action space).\n",
        "    # e.g. slate_size=2 and num_candidates=10 -> MultiDiscrete([10, 10]) -> Discrete(100)  # 10x10\n",
        "    \"convert_to_discrete_action_space\": True,\n",
        "})\n",
        "\n",
        "# What are our spaces?\n",
        "print(f\"observation space = {lts_10_1_env.observation_space}\")\n",
        "print(f\"action space = {lts_10_1_env.action_space}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a3eac27a-bc9a-47dd-b3f9-cffd3e590e3b",
      "metadata": {
        "id": "a3eac27a-bc9a-47dd-b3f9-cffd3e590e3b"
      },
      "source": [
        "Let's make use of our knowledge on the gym.Env API and call our new environment's `reset()` and `step()` methods.\n",
        "First: `reset()` to receive the initial observation in a new episode/trajectory/session:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "913d34a6-fac6-4436-b1d5-9292ebf88006",
      "metadata": {
        "id": "913d34a6-fac6-4436-b1d5-9292ebf88006",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3b4635e1-599b-4a44-be17-2b9f46eb279e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OrderedDict([('user', array([], dtype=float32)),\n",
            "             ('doc',\n",
            "              {'0': array([0.5488135], dtype=float32),\n",
            "               '1': array([0.71518934], dtype=float32),\n",
            "               '2': array([0.60276335], dtype=float32),\n",
            "               '3': array([0.5448832], dtype=float32),\n",
            "               '4': array([0.4236548], dtype=float32),\n",
            "               '5': array([0.6458941], dtype=float32),\n",
            "               '6': array([0.4375872], dtype=float32),\n",
            "               '7': array([0.891773], dtype=float32),\n",
            "               '8': array([0.96366274], dtype=float32),\n",
            "               '9': array([0.3834415], dtype=float32)}),\n",
            "             ('response',\n",
            "              (OrderedDict([('click', 1),\n",
            "                            ('engagement',\n",
            "                             array(62.39338, dtype=float32))]),))])\n"
          ]
        }
      ],
      "source": [
        "# Start a new episode and look at initial observation.\n",
        "obs = lts_10_1_env.reset()\n",
        "pprint(obs)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e9857197-f79f-4f9d-8e3f-68eee20daf94",
      "metadata": {
        "id": "e9857197-f79f-4f9d-8e3f-68eee20daf94"
      },
      "source": [
        "Now let's play RL agent ourselves and recommend some items (pick some actions) via the environment's `step()` method:\n",
        "\n",
        "**Task:** Execute the following cell a couple of times chosing different actions (from 0 - 9) to be sent into the environment's `step()` method. Each time, look at the returned next observation, reward, and `done` flag and write down what you find interesting about the dynamics and observations of this environment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "1f10b83e-993f-4c59-8e13-4e1074bfd7af",
      "metadata": {
        "id": "1f10b83e-993f-4c59-8e13-4e1074bfd7af",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e672004b-73d9-42ce-e22a-8e5a07cfe8cb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OrderedDict([('user', array([], dtype=float32)),\n",
            "             ('doc',\n",
            "              {'0': array([0.5488135], dtype=float32),\n",
            "               '1': array([0.71518934], dtype=float32),\n",
            "               '2': array([0.60276335], dtype=float32),\n",
            "               '3': array([0.5448832], dtype=float32),\n",
            "               '4': array([0.4236548], dtype=float32),\n",
            "               '5': array([0.6458941], dtype=float32),\n",
            "               '6': array([0.4375872], dtype=float32),\n",
            "               '7': array([0.891773], dtype=float32),\n",
            "               '8': array([0.96366274], dtype=float32),\n",
            "               '9': array([0.3834415], dtype=float32)}),\n",
            "             ('response',\n",
            "              ({'click': 1, 'engagement': array(3.2838342, dtype=float32)},))])\n",
            "reward = 3.28; done = False\n"
          ]
        }
      ],
      "source": [
        "# Let's send our first action (1-slate back into the env) using the env's `step()` method.\n",
        "action = 3  # Discrete(10): 0-9 are all valid actions\n",
        "\n",
        "# This method returns 4 items:\n",
        "# - next observation (after having applied the action)\n",
        "# - reward (after having applied the action)\n",
        "# - `done` flag; if True, the episode is terminated and the environment needs to be `reset()` again.\n",
        "# - info dict (we'll ignore this)\n",
        "next_obs, reward, done, _ = lts_10_1_env.step(action)\n",
        "\n",
        "# Print out the next observation.\n",
        "# We expect the \"doc\" and \"user\" items to be the same as in the previous observation\n",
        "# b/c we set \"resample_documents\" to False.\n",
        "pprint(next_obs)\n",
        "# Print out rewards and the vlaue of the `done` flag.\n",
        "print(f\"reward = {reward:.2f}; done = {done}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "444c7982-b372-4fe9-806a-18a29101c094",
      "metadata": {
        "jp-MarkdownHeadingCollapsed": true,
        "tags": [],
        "id": "444c7982-b372-4fe9-806a-18a29101c094"
      },
      "source": [
        "\n",
        "### What have we learnt from experimenting with the environment?\n",
        "\n",
        "* User's state (if any) is hidden to agent (not part of observation).\n",
        "* Episodes seem to last at least n timesteps -> user seems to have some time budget to spend.\n",
        "* User always seems to click, no matter what we recommend.\n",
        "* Reward seems to be always identical to the \"engagement\" value (of the clicked item). These values range somewhere between 0.0 and 20.0+.\n",
        "* Weak suspicion: If we always recommend the item with the highest feature value, rewards seem to taper off over time - in most of the episodes.\n",
        "* Weak suspicion: If we always recommend the item with the lowest feature value, rewards seem to increase over time."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8a05210c-69ea-4c09-acf8-831fffca5f8c",
      "metadata": {
        "id": "8a05210c-69ea-4c09-acf8-831fffca5f8c"
      },
      "source": [
        "### What the environment actually does under the hood\n",
        "\n",
        "Let's take a quick look at a pre-configured RecSim environment: \"Long Term Satisfaction\".\n",
        "\n",
        "<td> <img src=https://drive.google.com/uc?id=14BHvEMPrXWGuW6gZVFMuJnEJnuvkwhUu width=1200/> </td>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5958ff84-f7d0-45c9-aa43-b807243b8452",
      "metadata": {
        "id": "5958ff84-f7d0-45c9-aa43-b807243b8452"
      },
      "source": [
        "Now that we know, that there is a double objective built into the env (a. sweetness -> engagement; b. sweetness -> unhappyness; unhappyness -> low engagement), let's make this effect a tiny bit stronger by slightly modifying the environment. As said above, the effect is very weak and almost not measurable, which is a problem on the env's side. We can use this following `gym.ObservationWrapper` class in the cell below to \"fix\" that problem:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "375a469e-aa46-4038-9df7-02cabccdad50",
      "metadata": {
        "id": "375a469e-aa46-4038-9df7-02cabccdad50",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d6db808d-98d6-4fa9-c50a-9555bade705a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ok; registered the string 'modified_lts' to be used in RLlib configs (see below)\n"
          ]
        }
      ],
      "source": [
        "# Modifying wrapper around the LTS (Long Term Satisfaction) env:\n",
        "# - allows us to tweak the user model (and thus: reward behavior)\n",
        "# - adds user's current satisfaction value to observation\n",
        "\n",
        "class LTSWithStrongerDissatisfactionEffect(gym.ObservationWrapper):\n",
        "\n",
        "    def __init__(self, env):\n",
        "        # Tweak incoming environment.\n",
        "        env.environment._user_model._user_sampler._state_parameters.update({\n",
        "            \"sensitivity\": 0.058,\n",
        "            \"time_budget\": 120,\n",
        "            \"choc_stddev\": 0.1,\n",
        "            \"kale_stddev\": 0.1,\n",
        "            #\"innovation_stddev\": 0.01,\n",
        "            #\"choc_mean\": 1.25,\n",
        "            #\"kale_mean\": 1.0,\n",
        "            #\"memory_discount\": 0.9,\n",
        "        })\n",
        "\n",
        "        super().__init__(env)\n",
        "\n",
        "        # Adjust observation space.\n",
        "        if \"response\" in self.observation_space.spaces:\n",
        "            self.observation_space.spaces[\"user\"] = gym.spaces.Box(0.0, 1.0, (1, ), dtype=np.float32)\n",
        "            for r in self.observation_space[\"response\"]:\n",
        "                if \"engagement\" in r.spaces:\n",
        "                    r.spaces[\"watch_time\"] = r.spaces[\"engagement\"]\n",
        "                    del r.spaces[\"engagement\"]\n",
        "                    break\n",
        "\n",
        "    def observation(self, observation):\n",
        "        if \"response\" in self.observation_space.spaces:\n",
        "            observation[\"user\"] = np.array([self.env.environment._user_model._user_state.satisfaction])\n",
        "            for r in observation[\"response\"]:\n",
        "                if \"engagement\" in r:\n",
        "                    r[\"watch_time\"] = r[\"engagement\"]\n",
        "                    del r[\"engagement\"]\n",
        "        return observation\n",
        "\n",
        "\n",
        "# Add the wrapping around \n",
        "tune.register_env(\"modified_lts\", lambda env_config: LTSWithStrongerDissatisfactionEffect(LongTermSatisfactionRecSimEnv(env_config)))\n",
        "\n",
        "print(\"ok; registered the string 'modified_lts' to be used in RLlib configs (see below)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dbce752b-68a3-4a84-aada-97810039e4e8",
      "metadata": {
        "id": "dbce752b-68a3-4a84-aada-97810039e4e8"
      },
      "source": [
        "Now that we have a stronger effect of the user's satisfaction value on the long-term rewards, we may be able to measure this effect reliably\n",
        "using the following utility code:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "3b1f1ab8-6b08-47c7-9dfa-3fe9bd672c14",
      "metadata": {
        "id": "3b1f1ab8-6b08-47c7-9dfa-3fe9bd672c14",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7540fffe-4ff8-4995-8714-b577c499fcb5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.000578828452688054\n"
          ]
        }
      ],
      "source": [
        "# This cell should help you with your own analysis of the two above \"suspicions\":\n",
        "# Always chosing the highest/lowest-valued action will lead to a decrease/increase in rewards over the course of an episode.\n",
        "modified_lts_10_1_env = LTSWithStrongerDissatisfactionEffect(lts_10_1_env)\n",
        "\n",
        "# Capture slopes of all trendlines over all episodes.\n",
        "slopes = []\n",
        "# Run 1000 episodes.\n",
        "for _ in range(1000):\n",
        "    obs = modified_lts_10_1_env.reset()  # Reset environment to get initial observation:\n",
        "\n",
        "    # Compute actions that pick doc with highest/lowest feature value.\n",
        "    action_sweetest = np.argmax([value for _, value in obs[\"doc\"].items()])\n",
        "    action_kaleiest = np.argmin([value for _, value in obs[\"doc\"].items()])\n",
        "\n",
        "    # Play one episode.\n",
        "    done = False\n",
        "    rewards = []\n",
        "    while not done:\n",
        "        #action = action_sweetest\n",
        "        action = action_kaleiest\n",
        "        #action = np.random.choice([action_kaleiest, action_sweetest])\n",
        "\n",
        "        obs, reward, done, _ = modified_lts_10_1_env.step(action)\n",
        "        rewards.append(reward)\n",
        "\n",
        "    # Create linear model of rewards over time.\n",
        "    reward_linreg = linregress(np.array((range(len(rewards)))), np.array(rewards))\n",
        "    slopes.append(reward_linreg.slope)\n",
        "\n",
        "print(np.mean(slopes))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "be848212-87b8-4eb1-9353-74e09ae72310",
      "metadata": {
        "id": "be848212-87b8-4eb1-9353-74e09ae72310"
      },
      "source": [
        "## Measuring random baseline of our environment\n",
        "\n",
        "In the cells above, we created a new environment instance (`lts_10_1_env`). As we have seen above, in order to start \"walking\" through a recommender system episode, we need to perform `reset()` and then several `step()` calls (with different actions) until the returned `done` flag is True.\n",
        "\n",
        "Let's find out how well a randomly acting agent performs in this environment:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "spatial-geography",
      "metadata": {
        "id": "spatial-geography"
      },
      "outputs": [],
      "source": [
        "# Function that measures and outputs the random baseline reward.\n",
        "# This is the expected accumulated reward per episode, if we act randomly (recommend random items) at each time step.\n",
        "def measure_random_performance_for_env(env, episodes=1000, verbose=False):\n",
        "\n",
        "    # Reset the env.\n",
        "    env.reset()\n",
        "\n",
        "    # Number of episodes already done.\n",
        "    num_episodes = 0\n",
        "    # Current episode's accumulated reward.\n",
        "    episode_reward = 0.0\n",
        "    # Collect all episode rewards here to be able to calculate a random baseline reward.\n",
        "    episode_rewards = []\n",
        "\n",
        "    # Enter while loop (to step through the episode).\n",
        "    while num_episodes < episodes:\n",
        "        # Produce a random action.\n",
        "        action = env.action_space.sample()\n",
        "\n",
        "        # Send the action to the env's `step()` method to receive: obs, reward, done, and info.\n",
        "        obs, reward, done, _ = env.step(action)\n",
        "        episode_reward += reward\n",
        "\n",
        "        # Check, whether the episde is done, if yes, reset and increase episode counter.\n",
        "        if done:\n",
        "            if verbose:\n",
        "                print(f\"Episode done - accumulated reward={episode_reward}\")\n",
        "            elif num_episodes % 100 == 0:\n",
        "                print(f\" {num_episodes} \", end=\"\")\n",
        "            elif num_episodes % 10 == 0:\n",
        "                print(\".\", end=\"\")\n",
        "            num_episodes += 1\n",
        "            env.reset()\n",
        "            episode_rewards.append(episode_reward)\n",
        "            episode_reward = 0.0\n",
        "\n",
        "    # Print out and return mean episode reward (and standard error of the mean).\n",
        "    env_mean_random_reward = np.mean(episode_rewards)\n",
        "\n",
        "    print(f\"\\n\\nMean episode reward when acting randomly: {env_mean_random_reward:.2f}+/-{sem(episode_rewards):.2f}\")\n",
        "\n",
        "    return env_mean_random_reward, sem(episode_rewards)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "0e6e63e6-d030-4a45-af5b-ab88eaef3969",
      "metadata": {
        "id": "0e6e63e6-d030-4a45-af5b-ab88eaef3969",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "212896b4-4ed4-41f2-c53c-5cd18b837744"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 1 µs, sys: 0 ns, total: 1 µs\n",
            "Wall time: 2.15 µs\n",
            " 0 ......... 100 ......... 200 ......... 300 ......... 400 ......... 500 ......... 600 ......... 700 ......... 800 ......... 900 .........\n",
            "\n",
            "Mean episode reward when acting randomly: 1157.58+/-0.37\n"
          ]
        }
      ],
      "source": [
        "%time\n",
        "# Let's create a somewhat tougher version of this with 20 candidates (instead of 10) and a slate-size of 2.\n",
        "# We'll also keep using our wrapper from above to strengthen the dissatisfaction effect on the engagement:\n",
        "lts_20_2_env = LTSWithStrongerDissatisfactionEffect(LongTermSatisfactionRecSimEnv(config={\n",
        "    \"num_candidates\": 20,\n",
        "    \"slate_size\": 2,  # MultiDiscrete([20, 20]) -> Discrete(400)\n",
        "    \"resample_documents\": True,\n",
        "    # Convert to Discrete action space.\n",
        "    \"convert_to_discrete_action_space\": True,\n",
        "    # Wrap observations for RLlib bandit: Only changes dict keys (\"item\" instead of \"doc\").\n",
        "    \"wrap_for_bandits\": True,\n",
        "}))\n",
        "\n",
        "lts_20_2_env_mean_random_reward, _ = measure_random_performance_for_env(lts_20_2_env, episodes=1000)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5b20ac75-f3e6-4975-a209-2bf110b4ee13",
      "metadata": {
        "id": "5b20ac75-f3e6-4975-a209-2bf110b4ee13"
      },
      "source": [
        "# Plugging in RLlib\n",
        "<a id='rllib'></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8a76f02f-ef66-484d-8a1a-074a6e25c84a",
      "metadata": {
        "id": "8a76f02f-ef66-484d-8a1a-074a6e25c84a"
      },
      "source": [
        "## Picking an RLlib algorithm (\"Trainer\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "65aa24b2-ac17-44a3-b7b1-274ce2f50a87",
      "metadata": {
        "id": "65aa24b2-ac17-44a3-b7b1-274ce2f50a87"
      },
      "source": [
        "https://docs.ray.io/en/master/rllib-algorithms.html#available-algorithms-overview"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0194b33a-e031-49ce-9ff2-b32e328f9955",
      "metadata": {
        "id": "0194b33a-e031-49ce-9ff2-b32e328f9955"
      },
      "source": [
        "\n",
        "\n",
        "<td> <img src=https://drive.google.com/uc?id=1CvhB59H2PsmKeFy-4hZuhRVHxymOH0hd /> </td>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "62b1b0b3-ec96-41c0-9d5b-93db1c5ce021",
      "metadata": {
        "id": "62b1b0b3-ec96-41c0-9d5b-93db1c5ce021"
      },
      "source": [
        "### Trying a \"Contextual n-armed Bandit\" on our environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "4a26e094-9887-4fc6-88b6-d1448e931526",
      "metadata": {
        "id": "4a26e094-9887-4fc6-88b6-d1448e931526"
      },
      "outputs": [],
      "source": [
        "# In order to use one of the above algorithms, you may instantiate its associated Trainer class.\n",
        "# For example, to import a Bandit Trainer w/ Upper Confidence Bound (UCB) exploration, do:\n",
        "\n",
        "from ray.rllib.agents.bandit import BanditLinUCBTrainer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "0911f212-523e-4a75-846d-342dd2a681a6",
      "metadata": {
        "id": "0911f212-523e-4a75-846d-342dd2a681a6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cf904666-3fc2-4a83-8147-28c9121aafde"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bandit's default config is:\n",
            "{'_disable_action_flattening': False,\n",
            " '_disable_execution_plan_api': False,\n",
            " '_disable_preprocessor_api': False,\n",
            " '_fake_gpus': False,\n",
            " '_tf_policy_handles_more_than_one_loss': False,\n",
            " 'action_space': None,\n",
            " 'actions_in_input_normalized': False,\n",
            " 'always_attach_evaluation_results': False,\n",
            " 'batch_mode': 'truncate_episodes',\n",
            " 'callbacks': <class 'ray.rllib.agents.callbacks.DefaultCallbacks'>,\n",
            " 'clip_actions': False,\n",
            " 'clip_rewards': None,\n",
            " 'collect_metrics_timeout': -1,\n",
            " 'compress_observations': False,\n",
            " 'create_env_on_driver': False,\n",
            " 'custom_eval_function': None,\n",
            " 'custom_resources_per_worker': {},\n",
            " 'disable_env_checking': False,\n",
            " 'eager_max_retraces': 20,\n",
            " 'eager_tracing': False,\n",
            " 'env': None,\n",
            " 'env_config': {},\n",
            " 'env_task_fn': None,\n",
            " 'evaluation_config': {},\n",
            " 'evaluation_duration': 10,\n",
            " 'evaluation_duration_unit': 'episodes',\n",
            " 'evaluation_interval': None,\n",
            " 'evaluation_num_episodes': -1,\n",
            " 'evaluation_num_workers': 0,\n",
            " 'evaluation_parallel_to_training': False,\n",
            " 'exploration_config': {'type': 'StochasticSampling'},\n",
            " 'explore': True,\n",
            " 'extra_python_environs_for_driver': {},\n",
            " 'extra_python_environs_for_worker': {},\n",
            " 'fake_sampler': False,\n",
            " 'framework': 'torch',\n",
            " 'gamma': 0.99,\n",
            " 'horizon': None,\n",
            " 'ignore_worker_failures': False,\n",
            " 'in_evaluation': False,\n",
            " 'input': 'sampler',\n",
            " 'input_config': {},\n",
            " 'input_evaluation': ['is', 'wis'],\n",
            " 'keep_per_episode_custom_metrics': False,\n",
            " 'local_tf_session_args': {'inter_op_parallelism_threads': 8,\n",
            "                           'intra_op_parallelism_threads': 8},\n",
            " 'log_level': 'WARN',\n",
            " 'log_sys_usage': True,\n",
            " 'logger_config': None,\n",
            " 'lr': 0.0001,\n",
            " 'metrics_episode_collection_timeout_s': 180,\n",
            " 'metrics_num_episodes_for_smoothing': 100,\n",
            " 'metrics_smoothing_episodes': -1,\n",
            " 'min_iter_time_s': -1,\n",
            " 'min_sample_timesteps_per_reporting': None,\n",
            " 'min_time_s_per_reporting': None,\n",
            " 'min_train_timesteps_per_reporting': None,\n",
            " 'model': {'_disable_action_flattening': False,\n",
            "           '_disable_preprocessor_api': False,\n",
            "           '_time_major': False,\n",
            "           '_use_default_native_models': False,\n",
            "           'attention_dim': 64,\n",
            "           'attention_head_dim': 32,\n",
            "           'attention_init_gru_gate_bias': 2.0,\n",
            "           'attention_memory_inference': 50,\n",
            "           'attention_memory_training': 50,\n",
            "           'attention_num_heads': 1,\n",
            "           'attention_num_transformer_units': 1,\n",
            "           'attention_position_wise_mlp_dim': 32,\n",
            "           'attention_use_n_prev_actions': 0,\n",
            "           'attention_use_n_prev_rewards': 0,\n",
            "           'conv_activation': 'relu',\n",
            "           'conv_filters': None,\n",
            "           'custom_action_dist': None,\n",
            "           'custom_model': None,\n",
            "           'custom_model_config': {},\n",
            "           'custom_preprocessor': None,\n",
            "           'dim': 84,\n",
            "           'fcnet_activation': 'tanh',\n",
            "           'fcnet_hiddens': [256, 256],\n",
            "           'framestack': True,\n",
            "           'free_log_std': False,\n",
            "           'grayscale': False,\n",
            "           'lstm_cell_size': 256,\n",
            "           'lstm_use_prev_action': False,\n",
            "           'lstm_use_prev_action_reward': -1,\n",
            "           'lstm_use_prev_reward': False,\n",
            "           'max_seq_len': 20,\n",
            "           'no_final_linear': False,\n",
            "           'post_fcnet_activation': 'relu',\n",
            "           'post_fcnet_hiddens': [],\n",
            "           'use_attention': False,\n",
            "           'use_lstm': False,\n",
            "           'vf_share_layers': True,\n",
            "           'zero_mean': True},\n",
            " 'monitor': -1,\n",
            " 'multiagent': {'count_steps_by': 'env_steps',\n",
            "                'observation_fn': None,\n",
            "                'policies': {},\n",
            "                'policies_to_train': None,\n",
            "                'policy_map_cache': None,\n",
            "                'policy_map_capacity': 100,\n",
            "                'policy_mapping_fn': None,\n",
            "                'replay_mode': 'independent'},\n",
            " 'no_done_at_end': False,\n",
            " 'normalize_actions': True,\n",
            " 'num_cpus_for_driver': 1,\n",
            " 'num_cpus_per_worker': 1,\n",
            " 'num_envs_per_worker': 1,\n",
            " 'num_gpus': 0,\n",
            " 'num_gpus_per_worker': 0,\n",
            " 'num_workers': 0,\n",
            " 'observation_filter': 'NoFilter',\n",
            " 'observation_space': None,\n",
            " 'optimizer': {},\n",
            " 'output': None,\n",
            " 'output_compress_columns': ['obs', 'new_obs'],\n",
            " 'output_config': {},\n",
            " 'output_max_file_size': 67108864,\n",
            " 'placement_strategy': 'PACK',\n",
            " 'postprocess_inputs': False,\n",
            " 'preprocessor_pref': 'deepmind',\n",
            " 'record_env': False,\n",
            " 'remote_env_batch_wait_ms': 0,\n",
            " 'remote_worker_envs': False,\n",
            " 'render_env': False,\n",
            " 'rollout_fragment_length': 1,\n",
            " 'sample_async': False,\n",
            " 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>,\n",
            " 'seed': None,\n",
            " 'shuffle_buffer_size': 0,\n",
            " 'simple_optimizer': -1,\n",
            " 'soft_horizon': False,\n",
            " 'synchronize_filters': True,\n",
            " 'tf_session_args': {'allow_soft_placement': True,\n",
            "                     'device_count': {'CPU': 1},\n",
            "                     'gpu_options': {'allow_growth': True},\n",
            "                     'inter_op_parallelism_threads': 2,\n",
            "                     'intra_op_parallelism_threads': 2,\n",
            "                     'log_device_placement': False},\n",
            " 'timesteps_per_iteration': 100,\n",
            " 'train_batch_size': 1}\n"
          ]
        }
      ],
      "source": [
        "# Configuration dicts for RLlib Trainers.\n",
        "# Where are the default configuration dicts stored?\n",
        "\n",
        "# E.g. Bandit algorithms:\n",
        "from ray.rllib.agents.bandit.bandit import DEFAULT_CONFIG as BANDIT_DEFAULT_CONFIG\n",
        "print(f\"Bandit's default config is:\")\n",
        "pprint(BANDIT_DEFAULT_CONFIG)\n",
        "\n",
        "# DQN algorithm:\n",
        "#from ray.rllib.agents.dqn import DEFAULT_CONFIG as DQN_DEFAULT_CONFIG\n",
        "#print(f\"DQN's default config is:\")\n",
        "#pprint(DQN_DEFAULT_CONFIG)\n",
        "\n",
        "# Common (all algorithms).\n",
        "#from ray.rllib.agents.trainer import COMMON_CONFIG\n",
        "#print(f\"RLlib Trainer's default config is:\")\n",
        "#pprint(COMMON_CONFIG)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "c9bd9775-f2bb-41d9-8ff6-20be9abd68db",
      "metadata": {
        "id": "c9bd9775-f2bb-41d9-8ff6-20be9abd68db",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5af8d60a-6770-4008-a0cd-ddbc1965b8a8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-04-19 16:29:22,126\tWARNING rollout_worker.py:498 -- We've added a module for checking environments that are used in experiments. It will cause your environment to fail if your environment is not set upcorrectly. You can disable check env by setting `disable_env_checking` to True in your experiment config dictionary. You can run the environment checking module standalone by calling ray.rllib.utils.check_env(env).\n",
            "2022-04-19 16:29:22,127\tWARNING env.py:120 -- Your env doesn't have a .spec.max_episode_steps attribute. This is fine if you have set 'horizon' in your config dictionary, or `soft_horizon`. However, if you haven't, 'horizon' will default to infinity, and your environment will not be reset.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BanditLinUCBTrainer"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "bandit_config = {\n",
        "    \"env\": \"modified_lts\",\n",
        "    \"env_config\": {\n",
        "        \"num_candidates\": 20,  # 20x19 = ~400 unique slates (arms)\n",
        "        \"slate_size\": 2,\n",
        "        \"resample_documents\": True,\n",
        "\n",
        "        # Bandit-specific flags:\n",
        "        \"convert_to_discrete_action_space\": True,\n",
        "        # Convert \"doc\" key into \"item\" key.\n",
        "        \"wrap_for_bandits\": True,\n",
        "        # Use consistent seeds for the environment ...\n",
        "        \"seed\": 0,\n",
        "    },\n",
        "    # ... and the Trainer itself.\n",
        "    \"seed\": 0,\n",
        "\n",
        "    # The following settings are affecting the reporting only:\n",
        "    # ---\n",
        "    # Generate a result dict every single time step.\n",
        "    \"timesteps_per_iteration\": 1,\n",
        "    # Report rewards as smoothed mean over this many episodes.\n",
        "    \"metrics_num_episodes_for_smoothing\": 200,\n",
        "}\n",
        "\n",
        "# Create the RLlib Trainer using above config.\n",
        "bandit_trainer = BanditLinUCBTrainer(config=bandit_config)\n",
        "bandit_trainer"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "46a22cc0-0efb-40be-85fe-720e62a7a419",
      "metadata": {
        "id": "46a22cc0-0efb-40be-85fe-720e62a7a419"
      },
      "source": [
        "#### Running a single training iteration, by calling the `.train()` method:\n",
        "\n",
        "One iteration for most algos involves:\n",
        "\n",
        "1. Sampling from the environment(s)\n",
        "1. Using the sampled data (observations, actions taken, rewards) to update the policy model (e.g. a neural network), such that it would pick better actions in the future, leading to higher rewards.\n",
        "\n",
        "Let's try it out:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "ddd18251-2a1a-4822-8744-ca6df4a14787",
      "metadata": {
        "id": "ddd18251-2a1a-4822-8744-ca6df4a14787",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "74abdd34-7dcb-4ada-ccf2-2bdbb7687a91"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-04-19 16:29:22,158\tWARNING bandit_torch_policy.py:48 -- The env did not report `regret` values in its `info` return, ignoring.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'agent_timesteps_total': 1,\n",
            " 'custom_metrics': {},\n",
            " 'date': '2022-04-19_16-29-22',\n",
            " 'done': False,\n",
            " 'episode_len_mean': nan,\n",
            " 'episode_media': {},\n",
            " 'episode_reward_max': nan,\n",
            " 'episode_reward_mean': nan,\n",
            " 'episode_reward_min': nan,\n",
            " 'episodes_this_iter': 0,\n",
            " 'episodes_total': 0,\n",
            " 'experiment_id': '98d61bd5b71341e5ab91d1efcfb0220e',\n",
            " 'hist_stats': {'episode_lengths': [], 'episode_reward': []},\n",
            " 'hostname': 'Christys-MacBook-Pro.local',\n",
            " 'info': {'learner': {'default_policy': {'learner_stats': {'update_latency': 0.0008881092071533203}}},\n",
            "          'num_agent_steps_sampled': 1,\n",
            "          'num_agent_steps_trained': 1,\n",
            "          'num_steps_sampled': 1,\n",
            "          'num_steps_trained': 1,\n",
            "          'num_steps_trained_this_iter': 1},\n",
            " 'iterations_since_restore': 1,\n",
            " 'node_ip': '127.0.0.1',\n",
            " 'num_healthy_workers': 0,\n",
            " 'off_policy_estimator': {},\n",
            " 'perf': {'cpu_util_percent': 29.7, 'ram_util_percent': 80.6},\n",
            " 'pid': 51656,\n",
            " 'policy_reward_max': {},\n",
            " 'policy_reward_mean': {},\n",
            " 'policy_reward_min': {},\n",
            " 'sampler_perf': {},\n",
            " 'time_since_restore': 0.011373758316040039,\n",
            " 'time_this_iter_s': 0.011373758316040039,\n",
            " 'time_total_s': 0.011373758316040039,\n",
            " 'timers': {'learn_throughput': 833.858,\n",
            "            'learn_time_ms': 1.199,\n",
            "            'load_throughput': 10305.415,\n",
            "            'load_time_ms': 0.097,\n",
            "            'sample_throughput': 43.533,\n",
            "            'sample_time_ms': 22.971},\n",
            " 'timestamp': 1650410962,\n",
            " 'timesteps_since_restore': 1,\n",
            " 'timesteps_this_iter': 1,\n",
            " 'timesteps_total': 1,\n",
            " 'training_iteration': 1,\n",
            " 'trial_id': 'default',\n",
            " 'warmup_time': 0.037197113037109375}\n"
          ]
        }
      ],
      "source": [
        "# Perform single `.train()` call.\n",
        "result = bandit_trainer.train()\n",
        "# Erase config dict from result (for better overview).\n",
        "del result[\"config\"]\n",
        "# Print out training iteration results.\n",
        "pprint(result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "6d6f089b-e0cc-47de-af9b-dc05a71e102f",
      "metadata": {
        "id": "6d6f089b-e0cc-47de-af9b-dc05a71e102f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 513
        },
        "outputId": "e5368ef3-78d2-4973-c6b1-1dc6e7f9191d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " 0 .... 500 .... 1000 .... 1500 .... 2000 .... 2500 ...."
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/var/folders/0g/jfs_l_113_356_c0rfp4jd8c0000gn/T/ipykernel_51656/4138643203.py:20: RuntimeWarning: Mean of empty slice\n",
            "  y = [np.nanmean(rewards[max(i - smoothing_win, 0):i + 1]) for i in range(start_at, len(rewards))]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 720x504 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "# Train for n more iterations (timesteps) and collect n-arm rewards.\n",
        "rewards = []\n",
        "for i in range(3000):\n",
        "    # Run a single timestep in the environment and update\n",
        "    # the model immediately on the received reward.\n",
        "    result = bandit_trainer.train()\n",
        "    # Extract reward from results.\n",
        "    #rewards.extend(result[\"hist_stats\"][\"episode_reward\"]\n",
        "    rewards.append(result[\"episode_reward_mean\"])\n",
        "    if i % 500 == 0:\n",
        "        print(f\" {i} \", end=\"\")\n",
        "    elif i % 100 == 0:\n",
        "        print(\".\", end=\"\")\n",
        "\n",
        "# Plot per-timestep (episode) rewards.\n",
        "plt.figure(figsize=(10,7))\n",
        "start_at = 0\n",
        "smoothing_win = 200\n",
        "x = list(range(start_at, len(rewards)))\n",
        "y = [np.nanmean(rewards[max(i - smoothing_win, 0):i + 1]) for i in range(start_at, len(rewards))]\n",
        "plt.plot(x, y)\n",
        "plt.title(\"Mean reward\")\n",
        "plt.xlabel(\"Time/Training steps\")\n",
        "\n",
        "# Add mean random baseline reward (red line).\n",
        "plt.axhline(y=lts_20_2_env_mean_random_reward, color=\"r\", linestyle=\"-\")\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8b77c3cf-226c-4cab-82c6-7d8a54bfe9ea",
      "metadata": {
        "id": "8b77c3cf-226c-4cab-82c6-7d8a54bfe9ea"
      },
      "source": [
        "<a id='bandit_results'></a>\n",
        "### What does our trained Bandit actually recommend?\n",
        "\n",
        "The first method of the RLlib Trainer API we used above was `train()`.\n",
        "We'll now use another method of the Trainer, `compute_single_action(input_dict={})`.\n",
        "It takes a input_dict keyword arg, into which you may pass a single (unbatched!) observation to receive an action for:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "6eb62acb-0a16-4412-949a-4851201620bf",
      "metadata": {
        "id": "6eb62acb-0a16-4412-949a-4851201620bf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e12959d4-ae02-4081-8343-b27ad5c50a9e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "action's feature value=0.8916457295417786; max-choc-feature=0.8916457295417786; \n",
            "action's feature value=0.9777311682701111; max-choc-feature=0.9777311682701111; \n",
            "action's feature value=0.9989755749702454; max-choc-feature=0.9989755749702454; \n",
            "action's feature value=0.8999373912811279; max-choc-feature=0.8999373912811279; \n",
            "action's feature value=0.9658776521682739; max-choc-feature=0.9658776521682739; \n",
            "action's feature value=0.9644351601600647; max-choc-feature=0.9644351601600647; \n",
            "action's feature value=0.9434471726417542; max-choc-feature=0.9434471726417542; \n",
            "action's feature value=0.974498987197876; max-choc-feature=0.974498987197876; \n",
            "action's feature value=0.9987043142318726; max-choc-feature=0.9987043142318726; \n",
            "action's feature value=0.9603764414787292; max-choc-feature=0.9603764414787292; \n",
            "action's feature value=0.9986302256584167; max-choc-feature=0.9986302256584167; \n",
            "action's feature value=0.9272821545600891; max-choc-feature=0.9272821545600891; \n",
            "action's feature value=0.9550436735153198; max-choc-feature=0.9550436735153198; \n",
            "action's feature value=0.9447901844978333; max-choc-feature=0.9447901844978333; \n",
            "action's feature value=0.9695131778717041; max-choc-feature=0.9695131778717041; \n",
            "action's feature value=0.965463399887085; max-choc-feature=0.965463399887085; \n",
            "action's feature value=0.8990983963012695; max-choc-feature=0.8990983963012695; \n",
            "action's feature value=0.9040490984916687; max-choc-feature=0.9040490984916687; \n",
            "action's feature value=0.9543625116348267; max-choc-feature=0.9543625116348267; \n",
            "action's feature value=0.9245824813842773; max-choc-feature=0.9245824813842773; \n",
            "action's feature value=0.9099351763725281; max-choc-feature=0.9099351763725281; \n",
            "action's feature value=0.9679779410362244; max-choc-feature=0.9679779410362244; \n",
            "action's feature value=0.9706671833992004; max-choc-feature=0.9706671833992004; \n",
            "action's feature value=0.995324432849884; max-choc-feature=0.995324432849884; \n",
            "action's feature value=0.9396806955337524; max-choc-feature=0.9396806955337524; \n",
            "action's feature value=0.8787536025047302; max-choc-feature=0.8787536025047302; \n",
            "action's feature value=0.9149923920631409; max-choc-feature=0.9149923920631409; \n",
            "action's feature value=0.9406102895736694; max-choc-feature=0.9406102895736694; \n",
            "action's feature value=0.9478465914726257; max-choc-feature=0.9478465914726257; \n",
            "action's feature value=0.9911457300186157; max-choc-feature=0.9911457300186157; \n",
            "action's feature value=0.8890330791473389; max-choc-feature=0.8890330791473389; \n",
            "action's feature value=0.9480608701705933; max-choc-feature=0.9480608701705933; \n",
            "action's feature value=0.9916664958000183; max-choc-feature=0.9916664958000183; \n",
            "action's feature value=0.9847033619880676; max-choc-feature=0.9847033619880676; \n",
            "action's feature value=0.9915906190872192; max-choc-feature=0.9915906190872192; \n",
            "action's feature value=0.9487980008125305; max-choc-feature=0.9487980008125305; \n",
            "action's feature value=0.9980818629264832; max-choc-feature=0.9980818629264832; \n",
            "action's feature value=0.9360242486000061; max-choc-feature=0.9360242486000061; \n",
            "action's feature value=0.9260164499282837; max-choc-feature=0.9260164499282837; \n",
            "action's feature value=0.9987164735794067; max-choc-feature=0.9987164735794067; \n",
            "action's feature value=0.8080435991287231; max-choc-feature=0.8080435991287231; \n",
            "action's feature value=0.9191829562187195; max-choc-feature=0.9191829562187195; \n",
            "action's feature value=0.9863934516906738; max-choc-feature=0.9863934516906738; \n",
            "action's feature value=0.9714748859405518; max-choc-feature=0.9714748859405518; \n",
            "action's feature value=0.9963867664337158; max-choc-feature=0.9963867664337158; \n",
            "action's feature value=0.9447032809257507; max-choc-feature=0.9447032809257507; \n",
            "action's feature value=0.9920045733451843; max-choc-feature=0.9920045733451843; \n",
            "action's feature value=0.9945886731147766; max-choc-feature=0.9945886731147766; \n",
            "action's feature value=0.9901645183563232; max-choc-feature=0.9901645183563232; \n",
            "action's feature value=0.9611482620239258; max-choc-feature=0.9611482620239258; \n",
            "action's feature value=0.8877958059310913; max-choc-feature=0.8877958059310913; \n",
            "action's feature value=0.980872392654419; max-choc-feature=0.980872392654419; \n",
            "action's feature value=0.9676306247711182; max-choc-feature=0.9676306247711182; \n",
            "action's feature value=0.7901422381401062; max-choc-feature=0.7901422381401062; \n",
            "action's feature value=0.9796108603477478; max-choc-feature=0.9796108603477478; \n",
            "action's feature value=0.9665442705154419; max-choc-feature=0.9665442705154419; \n",
            "action's feature value=0.9356783032417297; max-choc-feature=0.9356783032417297; \n",
            "action's feature value=0.971942126750946; max-choc-feature=0.971942126750946; \n",
            "action's feature value=0.9433143734931946; max-choc-feature=0.9433143734931946; \n",
            "action's feature value=0.9321243166923523; max-choc-feature=0.9321243166923523; \n",
            "action's feature value=0.8538448810577393; max-choc-feature=0.8538448810577393; \n",
            "action's feature value=0.9785804152488708; max-choc-feature=0.9785804152488708; \n",
            "action's feature value=0.9912826418876648; max-choc-feature=0.9912826418876648; \n",
            "action's feature value=0.8888525366783142; max-choc-feature=0.8888525366783142; \n",
            "action's feature value=0.9974634051322937; max-choc-feature=0.9974634051322937; \n",
            "action's feature value=0.9976392984390259; max-choc-feature=0.9976392984390259; \n",
            "action's feature value=0.9779126048088074; max-choc-feature=0.9779126048088074; \n",
            "action's feature value=0.9817178249359131; max-choc-feature=0.9817178249359131; \n",
            "action's feature value=0.9769214987754822; max-choc-feature=0.9769214987754822; \n",
            "action's feature value=0.9773058891296387; max-choc-feature=0.9773058891296387; \n",
            "action's feature value=0.9939188361167908; max-choc-feature=0.9939188361167908; \n",
            "action's feature value=0.9815660715103149; max-choc-feature=0.9815660715103149; \n",
            "action's feature value=0.9989805817604065; max-choc-feature=0.9989805817604065; \n",
            "action's feature value=0.9938614368438721; max-choc-feature=0.9938614368438721; \n",
            "action's feature value=0.9548345804214478; max-choc-feature=0.9548345804214478; \n",
            "action's feature value=0.9095875024795532; max-choc-feature=0.9095875024795532; \n",
            "action's feature value=0.9673813581466675; max-choc-feature=0.9673813581466675; \n",
            "action's feature value=0.931671142578125; max-choc-feature=0.931671142578125; \n",
            "action's feature value=0.9167065024375916; max-choc-feature=0.9167065024375916; \n",
            "action's feature value=0.9021305441856384; max-choc-feature=0.9021305441856384; \n",
            "action's feature value=0.999610185623169; max-choc-feature=0.999610185623169; \n",
            "action's feature value=0.8620670437812805; max-choc-feature=0.8620670437812805; \n",
            "action's feature value=0.9598987698554993; max-choc-feature=0.9598987698554993; \n",
            "action's feature value=0.9733729362487793; max-choc-feature=0.9733729362487793; \n",
            "action's feature value=0.9917956590652466; max-choc-feature=0.9917956590652466; \n",
            "action's feature value=0.919234573841095; max-choc-feature=0.919234573841095; \n",
            "action's feature value=0.9918598532676697; max-choc-feature=0.9918598532676697; \n",
            "action's feature value=0.9960801005363464; max-choc-feature=0.9960801005363464; \n",
            "action's feature value=0.9754700064659119; max-choc-feature=0.9754700064659119; \n",
            "action's feature value=0.9660402536392212; max-choc-feature=0.9660402536392212; \n",
            "action's feature value=0.996029257774353; max-choc-feature=0.996029257774353; \n",
            "action's feature value=0.9930408000946045; max-choc-feature=0.9930408000946045; \n",
            "action's feature value=0.9743787050247192; max-choc-feature=0.9743787050247192; \n",
            "action's feature value=0.9861087203025818; max-choc-feature=0.9861087203025818; \n",
            "action's feature value=0.9269380569458008; max-choc-feature=0.9269380569458008; \n",
            "action's feature value=0.9651040434837341; max-choc-feature=0.9651040434837341; \n",
            "action's feature value=0.9919588565826416; max-choc-feature=0.9919588565826416; \n",
            "action's feature value=0.946600079536438; max-choc-feature=0.946600079536438; \n",
            "action's feature value=0.9954290986061096; max-choc-feature=0.9954290986061096; \n",
            "action's feature value=0.916007399559021; max-choc-feature=0.916007399559021; \n",
            "action's feature value=0.9317758679389954; max-choc-feature=0.9317758679389954; \n",
            "action's feature value=0.9713598489761353; max-choc-feature=0.9713598489761353; \n",
            "action's feature value=0.9755606651306152; max-choc-feature=0.9755606651306152; \n",
            "action's feature value=0.986943781375885; max-choc-feature=0.986943781375885; \n",
            "action's feature value=0.9602279663085938; max-choc-feature=0.9602279663085938; \n",
            "action's feature value=0.8469273447990417; max-choc-feature=0.8469273447990417; \n",
            "action's feature value=0.9099560379981995; max-choc-feature=0.9099560379981995; \n",
            "action's feature value=0.9875852465629578; max-choc-feature=0.9875852465629578; \n",
            "action's feature value=0.9507820010185242; max-choc-feature=0.9507820010185242; \n",
            "action's feature value=0.9826569557189941; max-choc-feature=0.9826569557189941; \n",
            "action's feature value=0.9213010668754578; max-choc-feature=0.9213010668754578; \n",
            "action's feature value=0.9418420791625977; max-choc-feature=0.9418420791625977; \n",
            "action's feature value=0.9259132742881775; max-choc-feature=0.9259132742881775; \n",
            "action's feature value=0.9294229745864868; max-choc-feature=0.9294229745864868; \n",
            "action's feature value=0.9663825035095215; max-choc-feature=0.9663825035095215; \n",
            "action's feature value=0.9695129990577698; max-choc-feature=0.9695129990577698; \n",
            "action's feature value=0.9583300948143005; max-choc-feature=0.9583300948143005; \n",
            "action's feature value=0.9953047037124634; max-choc-feature=0.9953047037124634; \n",
            "action's feature value=0.9923339486122131; max-choc-feature=0.9923339486122131; \n",
            "action's feature value=0.9950851202011108; max-choc-feature=0.9950851202011108; \n"
          ]
        }
      ],
      "source": [
        "# Let's see what items our bandit recommends now that it has been trained and achieves good (>> random) rewards.\n",
        "obs = lts_20_2_env.reset()\n",
        "\n",
        "# Run a single episode.\n",
        "done = False\n",
        "while not done:\n",
        "    # Pass the single (unbatched) observation into the `compute_single_action` method of our Trainer.\n",
        "    # This is one way to perform inference on a learned policy.\n",
        "    action = bandit_trainer.compute_single_action(input_dict={\"obs\": obs})\n",
        "    feat_value_of_action = obs[\"item\"][action][0]\n",
        "    max_choc_feat = obs['item'][np.argmax(obs[\"item\"])][0]\n",
        "\n",
        "    # Print out the picked document's feature value and compare that to the highest possible feature value.\n",
        "    print(f\"action's feature value={feat_value_of_action}; max-choc-feature={max_choc_feat}; \")\n",
        "\n",
        "    # Apply the computed action in the environment and continue.\n",
        "    obs, r, done, _ = lts_20_2_env.step(action)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c9355c1b-f0f7-4690-a7fe-332b01a651c4",
      "metadata": {
        "id": "c9355c1b-f0f7-4690-a7fe-332b01a651c4"
      },
      "source": [
        "### Ok, Bandits want Chocolate! :)\n",
        "#### Why is that?\n",
        "\n",
        "<td> <img src=https://drive.google.com/uc?id=1qOMMI80Wn8p1jbqEdtM-vnIA9wO1KX1y width=1000/> </td>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "84291b69-050f-489a-b822-239294bb3a2e",
      "metadata": {
        "id": "84291b69-050f-489a-b822-239294bb3a2e"
      },
      "source": [
        "### Recap: Advantages and Disatvantages of Bandits:\n",
        "#### Advantages:\n",
        "* Very fast\n",
        "* Very sample-efficient\n",
        "* Easy to understand learning process\n",
        "\n",
        "#### Disadvantages\n",
        "* Need immediate reward (not capable of solving long-horizon credit assignment problem)\n",
        "* Models user -> If > 1 user, must train separate bandit per user\n",
        "* Not able to handle components of MultiDiscrete action space separately (works only on flattened Discrete action space)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "108a830b-cc5f-454c-b986-75c59d40df89",
      "metadata": {
        "id": "108a830b-cc5f-454c-b986-75c59d40df89"
      },
      "source": [
        "<a id='slateq'></a>\n",
        "# Switching to Slate-Q\n",
        "\n",
        "\n",
        "https://docs.ray.io/en/master/rllib-algorithms.html#available-algorithms-overview\n",
        "<td> <img src=https://drive.google.com/uc?id=1oEVD57X1MD7Z7D3roOz0kWj-hUUWydwe width=800/> </td>\n",
        "\n",
        "RLlib offers another algorithm - Slate-Q - designed for k-slate, long time horizon, and dynamic user recommendation problems. \n",
        "<br>\n",
        "\n",
        "Slate-Q Paper: https://arxiv.org/pdf/1812.02353.pdf <br>\n",
        "Author video about Slate-Q: https://slideslive.com/38917655/reinforcement-learning-in-recommender-systems-some-challenges\n",
        "<br><br>\n",
        "\n",
        "<td> <img src=https://drive.google.com/uc?id=1iG1-lMP6jwh1rgodQoTcO5l67MgHy7qc width=1000/> </td>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "32828a91-2da7-4f5f-9374-d12f32ec0b87",
      "metadata": {
        "id": "32828a91-2da7-4f5f-9374-d12f32ec0b87",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c5da9bf4-f048-4050-a449-188dbc2d87bd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-04-19 16:29:36,858\tWARNING rollout_worker.py:498 -- We've added a module for checking environments that are used in experiments. It will cause your environment to fail if your environment is not set upcorrectly. You can disable check env by setting `disable_env_checking` to True in your experiment config dictionary. You can run the environment checking module standalone by calling ray.rllib.utils.check_env(env).\n",
            "2022-04-19 16:29:36,858\tWARNING env.py:120 -- Your env doesn't have a .spec.max_episode_steps attribute. This is fine if you have set 'horizon' in your config dictionary, or `soft_horizon`. However, if you haven't, 'horizon' will default to infinity, and your environment will not be reset.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:From /Users/christy/mambaforge/envs/rllib/lib/python3.9/site-packages/tensorflow/python/util/deprecation.py:620: calling map_fn_v2 (from tensorflow.python.ops.map_fn) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use fn_output_signature instead\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:From /Users/christy/mambaforge/envs/rllib/lib/python3.9/site-packages/tensorflow/python/util/deprecation.py:620: calling map_fn_v2 (from tensorflow.python.ops.map_fn) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use fn_output_signature instead\n",
            "2022-04-19 16:29:36.906663: W tensorflow/core/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n",
            "/Users/christy/mambaforge/envs/rllib/lib/python3.9/site-packages/tensorflow/python/framework/indexed_slices.py:448: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"default_policy/gradients/default_policy/GatherV2_5_grad/Reshape_1:0\", shape=(None,), dtype=int64), values=Tensor(\"default_policy/gradients/default_policy/GatherV2_5_grad/Reshape:0\", shape=(None,), dtype=float32), dense_shape=Tensor(\"default_policy/gradients/default_policy/GatherV2_5_grad/Cast:0\", shape=(1,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:From /Users/christy/mambaforge/envs/rllib/lib/python3.9/site-packages/tensorflow/python/training/rmsprop.py:192: calling Ones.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:From /Users/christy/mambaforge/envs/rllib/lib/python3.9/site-packages/tensorflow/python/training/rmsprop.py:192: calling Ones.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SlateQTrainer"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "from ray.rllib.agents.slateq import SlateQTrainer, DEFAULT_CONFIG\n",
        "\n",
        "slateq_config = {\n",
        "    \"env\": \"modified_lts\",\n",
        "    \"env_config\": {\n",
        "        \"num_candidates\": 20,  # MultiDiscrete([20, 20]) -> no flattening necessary (see `convert_to_discrete_action_space=False` below)\n",
        "        \"slate_size\": 2,\n",
        "        \"resample_documents\": True,\n",
        "        \"wrap_for_bandits\": False,  # SlateQ != Bandit (will keep \"doc\" key, instead of \"items\")\n",
        "        \"convert_to_discrete_action_space\": False,  # SlateQ handles MultiDiscrete action spaces (slate recommendations).\n",
        "    },\n",
        "    # Setup exploratory behavior: Implemented as \"epsilon greedy\" strategy:\n",
        "    # Act randomly `e` percent of the time; `e` gets reduced from 1.0 to almost 0.0 over\n",
        "    # the course of `epsilon_timesteps`.\n",
        "    \"exploration_config\": {\n",
        "        #\"warmup_timesteps\": 20000,  # default\n",
        "        \n",
        "        # Use Ray Tune to run 3 parallel tuning trials\n",
        "        # \"epsilon_timesteps\": tune.grid_search([40000, 2000, 3000]),  # default: 250000\n",
        "        \n",
        "        # Do not use Ray Tune\n",
        "        \"epsilon_timesteps\": 60000\n",
        "    },\n",
        "    #\"learning_starts\": 20000,  # default\n",
        "    \"target_network_update_freq\": 3200,\n",
        "\n",
        "    # Report rewards as smoothed mean over this many episodes.\n",
        "    \"metrics_num_episodes_for_smoothing\": 200,\n",
        "}\n",
        "\n",
        "# Instantiate the Trainer object using the exact same config as in our Bandit experiment above.\n",
        "slateq_trainer = SlateQTrainer(config=slateq_config)\n",
        "slateq_trainer\n",
        "\n",
        "# # You can change timesteps_total here to see more tuning\n",
        "# tune.run(\"SlateQ\", config=slateq_config, stop={\"timesteps_total\":1000, \n",
        "#                                                \"training_iteration\":5})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "0845c458-d656-417e-b3d8-60596650c2bb",
      "metadata": {
        "id": "0845c458-d656-417e-b3d8-60596650c2bb"
      },
      "outputs": [],
      "source": [
        "# Optional - View the default configs of slateq\n",
        "# DEFAULT_CONFIG"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "95395f1a-31c6-4933-b09a-d06959ad5714",
      "metadata": {
        "id": "95395f1a-31c6-4933-b09a-d06959ad5714"
      },
      "source": [
        "<a id='slateq_experiment'></a>\n",
        "Now that we have confirmed we have setup the Trainer correctly, let's call `train()` on it several times:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "dc47c75f-4f6f-4806-995e-80ec974cfd86",
      "metadata": {
        "id": "dc47c75f-4f6f-4806-995e-80ec974cfd86",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "636becd4-2f93-463e-f8c2-9b4843af1513"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration=1; ts=20000: R(\"return\")=1157.432086834311\n",
            "Iteration=2; ts=21000: R(\"return\")=1157.1015502836303\n",
            "Iteration=3; ts=22000: R(\"return\")=1157.0025524609198\n",
            "Iteration=4; ts=23000: R(\"return\")=1157.3549628031349\n",
            "Iteration=5; ts=24000: R(\"return\")=1157.484695265295\n",
            "Iteration=6; ts=25000: R(\"return\")=1157.5591376825353\n",
            "Iteration=7; ts=26000: R(\"return\")=1157.8290496745065\n",
            "Iteration=8; ts=27000: R(\"return\")=1158.0745143004613\n",
            "Iteration=9; ts=28000: R(\"return\")=1158.2278869588997\n",
            "Iteration=10; ts=29000: R(\"return\")=1158.1244148619808\n",
            "Iteration=11; ts=30000: R(\"return\")=1158.311599684324\n",
            "Iteration=12; ts=31000: R(\"return\")=1158.6498854993938\n",
            "Iteration=13; ts=32000: R(\"return\")=1158.9165402221688\n",
            "Iteration=14; ts=33000: R(\"return\")=1159.159176009981\n",
            "Iteration=15; ts=34000: R(\"return\")=1159.5561341569012\n",
            "Iteration=16; ts=35000: R(\"return\")=1160.1353373886477\n",
            "Iteration=17; ts=36000: R(\"return\")=1160.791548739037\n",
            "Iteration=18; ts=37000: R(\"return\")=1161.1439626590516\n",
            "Iteration=19; ts=38000: R(\"return\")=1161.555863492825\n",
            "Iteration=20; ts=39000: R(\"return\")=1162.025012623414\n",
            "Iteration=21; ts=40000: R(\"return\")=1163.0207481368693\n",
            "Iteration=22; ts=41000: R(\"return\")=1163.2063636594078\n",
            "Iteration=23; ts=42000: R(\"return\")=1163.8534858446715\n",
            "Iteration=24; ts=43000: R(\"return\")=1164.418035874582\n",
            "Iteration=25; ts=44000: R(\"return\")=1164.8244950760961\n",
            "Iteration=26; ts=45000: R(\"return\")=1165.8100430151435\n",
            "Iteration=27; ts=46000: R(\"return\")=1166.4834344001702\n",
            "Iteration=28; ts=47000: R(\"return\")=1166.837475070394\n",
            "Iteration=29; ts=48000: R(\"return\")=1167.4592864957779\n",
            "Iteration=30; ts=49000: R(\"return\")=1168.0563395563718\n",
            "Iteration=31; ts=50000: R(\"return\")=1168.2683802544207\n",
            "Iteration=32; ts=51000: R(\"return\")=1168.5719177302349\n",
            "Iteration=33; ts=52000: R(\"return\")=1168.83978274326\n",
            "Iteration=34; ts=53000: R(\"return\")=1169.3110464865135\n",
            "Iteration=35; ts=54000: R(\"return\")=1170.1351242054016\n",
            "Iteration=36; ts=55000: R(\"return\")=1170.6349572470785\n",
            "Iteration=37; ts=56000: R(\"return\")=1170.730248503333\n",
            "Iteration=38; ts=57000: R(\"return\")=1171.2158934558192\n",
            "Iteration=39; ts=58000: R(\"return\")=1171.4318023417648\n",
            "Iteration=40; ts=59000: R(\"return\")=1171.2584108622675\n",
            "Iteration=41; ts=60000: R(\"return\")=1171.1282890700666\n",
            "Iteration=42; ts=61000: R(\"return\")=1171.274158236518\n",
            "Iteration=43; ts=62000: R(\"return\")=1171.4941083094857\n",
            "Iteration=44; ts=63000: R(\"return\")=1171.7040284375187\n",
            "Iteration=45; ts=64000: R(\"return\")=1171.8088803090752\n",
            "Iteration=46; ts=65000: R(\"return\")=1172.1096969284265\n",
            "Iteration=47; ts=66000: R(\"return\")=1171.8619339265224\n",
            "Iteration=48; ts=67000: R(\"return\")=1171.8950463289004\n",
            "Iteration=49; ts=68000: R(\"return\")=1171.8756135951946\n",
            "Iteration=50; ts=69000: R(\"return\")=1171.6366410099172\n",
            "Iteration=51; ts=70000: R(\"return\")=1171.3771542469956\n",
            "Iteration=52; ts=71000: R(\"return\")=1171.3716854633803\n",
            "Iteration=53; ts=72000: R(\"return\")=1171.070065082736\n",
            "Iteration=54; ts=73000: R(\"return\")=1171.0809575984536\n",
            "Iteration=55; ts=74000: R(\"return\")=1171.2830006665788\n",
            "Iteration=56; ts=75000: R(\"return\")=1171.328419957946\n",
            "Iteration=57; ts=76000: R(\"return\")=1170.9869283019375\n",
            "Iteration=58; ts=77000: R(\"return\")=1170.9153204644442\n",
            "Iteration=59; ts=78000: R(\"return\")=1170.3231712041547\n",
            "Iteration=60; ts=79000: R(\"return\")=1169.752151556073\n"
          ]
        }
      ],
      "source": [
        "# See reward progress with time\n",
        "# Run `train()` n times. Repeatedly call `train()` now to see rewards increase.\n",
        "for _ in range(60):\n",
        "    results = slateq_trainer.train()\n",
        "    print(f\"Iteration={slateq_trainer.iteration}; ts={results['timesteps_total']}: R(\\\"return\\\")={results['episode_reward_mean']}\")\n",
        "    \n",
        " "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7b86aecb-90ce-4be1-91a2-5c5391ab6adf",
      "metadata": {
        "id": "7b86aecb-90ce-4be1-91a2-5c5391ab6adf"
      },
      "source": [
        "------------------\n",
        "## 10 min break :)\n",
        "\n",
        "while Slate-Q is (hopefully) leaning\n",
        "\n",
        "------------------\n",
        "\n",
        "<a id='slateq_results'></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "34bc1113-bd5e-45f5-bd8e-93931b8cee0b",
      "metadata": {
        "id": "34bc1113-bd5e-45f5-bd8e-93931b8cee0b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a53677eb-4887-42c8-b61e-617b0224c2fb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "action's feature value=0.9437480568885803 max-choc-feature=0.978618323802948\n",
            "action's feature value=0.9883738160133362 max-choc-feature=0.9883738160133362\n",
            "action's feature value=0.9767611026763916 max-choc-feature=0.9767611026763916\n",
            "action's feature value=0.3185689449310303 max-choc-feature=0.9292961955070496\n",
            "action's feature value=0.8817353844642639 max-choc-feature=0.9621885418891907\n",
            "action's feature value=0.6180154085159302 max-choc-feature=0.9560836553573608\n",
            "action's feature value=0.9988470077514648 max-choc-feature=0.9988470077514648\n",
            "action's feature value=0.9280812740325928 max-choc-feature=0.9755215048789978\n",
            "action's feature value=0.9443724155426025 max-choc-feature=0.9443724155426025\n",
            "action's feature value=0.990338921546936 max-choc-feature=0.990338921546936\n",
            "action's feature value=0.7308558225631714 max-choc-feature=0.9527916312217712\n",
            "action's feature value=0.961936354637146 max-choc-feature=0.961936354637146\n",
            "action's feature value=0.9493188261985779 max-choc-feature=0.9774951338768005\n",
            "action's feature value=0.9729194641113281 max-choc-feature=0.9818294048309326\n",
            "action's feature value=0.8605511784553528 max-choc-feature=0.9065554738044739\n",
            "action's feature value=0.7774075865745544 max-choc-feature=0.7885454893112183\n",
            "action's feature value=0.9564056992530823 max-choc-feature=0.9594333171844482\n",
            "action's feature value=0.9591665863990784 max-choc-feature=0.9591665863990784\n",
            "action's feature value=0.9589827060699463 max-choc-feature=0.9589827060699463\n",
            "action's feature value=0.869488537311554 max-choc-feature=0.9453015327453613\n",
            "action's feature value=0.9894098043441772 max-choc-feature=0.9903450012207031\n",
            "action's feature value=0.904948353767395 max-choc-feature=0.9920112490653992\n",
            "action's feature value=0.9944007992744446 max-choc-feature=0.9944007992744446\n",
            "action's feature value=0.8967611789703369 max-choc-feature=0.96779465675354\n",
            "action's feature value=0.7400975227355957 max-choc-feature=0.9241587519645691\n",
            "action's feature value=0.9488610029220581 max-choc-feature=0.9488610029220581\n",
            "action's feature value=0.7908401489257812 max-choc-feature=0.9627703428268433\n",
            "action's feature value=0.300028920173645 max-choc-feature=0.9805801510810852\n",
            "action's feature value=0.997962236404419 max-choc-feature=0.997962236404419\n",
            "action's feature value=0.9992780089378357 max-choc-feature=0.9992780089378357\n",
            "action's feature value=0.9587409496307373 max-choc-feature=0.9762256741523743\n",
            "action's feature value=0.9799623489379883 max-choc-feature=0.9834262132644653\n",
            "action's feature value=0.8224067091941833 max-choc-feature=0.857124924659729\n",
            "action's feature value=0.8671671748161316 max-choc-feature=0.95914226770401\n",
            "action's feature value=0.7734555602073669 max-choc-feature=0.8562761545181274\n",
            "action's feature value=0.987348735332489 max-choc-feature=0.987348735332489\n",
            "action's feature value=0.9308187365531921 max-choc-feature=0.9707314372062683\n",
            "action's feature value=0.9661474227905273 max-choc-feature=0.9918903112411499\n",
            "action's feature value=0.9325612187385559 max-choc-feature=0.9758838415145874\n",
            "action's feature value=0.8975427746772766 max-choc-feature=0.9543338418006897\n",
            "action's feature value=0.8119385838508606 max-choc-feature=0.9323939681053162\n",
            "action's feature value=0.9195073843002319 max-choc-feature=0.9195073843002319\n",
            "action's feature value=0.9149707555770874 max-choc-feature=0.9631972908973694\n",
            "action's feature value=0.795590341091156 max-choc-feature=0.8667885661125183\n",
            "action's feature value=0.9804856777191162 max-choc-feature=0.9804856777191162\n",
            "action's feature value=0.9031496644020081 max-choc-feature=0.9031496644020081\n",
            "action's feature value=0.8536060452461243 max-choc-feature=0.9738187193870544\n",
            "action's feature value=0.9998085498809814 max-choc-feature=0.9998085498809814\n",
            "action's feature value=0.9384120106697083 max-choc-feature=0.9384120106697083\n",
            "action's feature value=0.7532401084899902 max-choc-feature=0.9536756873130798\n",
            "action's feature value=0.9438508749008179 max-choc-feature=0.9958152770996094\n",
            "action's feature value=0.7912274599075317 max-choc-feature=0.9692058563232422\n",
            "action's feature value=0.9750946760177612 max-choc-feature=0.9750946760177612\n",
            "action's feature value=0.9670549035072327 max-choc-feature=0.9673377871513367\n",
            "action's feature value=0.7833966612815857 max-choc-feature=0.9851086735725403\n",
            "action's feature value=0.9561231732368469 max-choc-feature=0.9979940056800842\n",
            "action's feature value=0.8268052935600281 max-choc-feature=0.9555683732032776\n",
            "action's feature value=0.983853816986084 max-choc-feature=0.983853816986084\n",
            "action's feature value=0.8668609261512756 max-choc-feature=0.9279761910438538\n",
            "action's feature value=0.8075637817382812 max-choc-feature=0.9421847462654114\n",
            "action's feature value=0.40168818831443787 max-choc-feature=0.992667019367218\n",
            "action's feature value=0.9717630743980408 max-choc-feature=0.9717630743980408\n",
            "action's feature value=0.8877008557319641 max-choc-feature=0.9792863130569458\n",
            "action's feature value=0.9960712790489197 max-choc-feature=0.9960712790489197\n",
            "action's feature value=0.9942330718040466 max-choc-feature=0.9942330718040466\n",
            "action's feature value=0.9824448823928833 max-choc-feature=0.9824448823928833\n",
            "action's feature value=0.8103020787239075 max-choc-feature=0.9492799043655396\n",
            "action's feature value=0.9440324902534485 max-choc-feature=0.9568705558776855\n",
            "action's feature value=0.9853785634040833 max-choc-feature=0.9853785634040833\n",
            "action's feature value=0.8986376523971558 max-choc-feature=0.9903685450553894\n",
            "action's feature value=0.9351605176925659 max-choc-feature=0.9980227947235107\n",
            "action's feature value=0.9376630783081055 max-choc-feature=0.9376630783081055\n",
            "action's feature value=0.898087739944458 max-choc-feature=0.9834339618682861\n",
            "action's feature value=0.6671687960624695 max-choc-feature=0.9832748770713806\n",
            "action's feature value=0.9804664850234985 max-choc-feature=0.9804664850234985\n",
            "action's feature value=0.906494140625 max-choc-feature=0.9825738668441772\n",
            "action's feature value=0.9197379946708679 max-choc-feature=0.9983548521995544\n",
            "action's feature value=0.7697890400886536 max-choc-feature=0.7697890400886536\n",
            "action's feature value=0.9985265731811523 max-choc-feature=0.9985265731811523\n",
            "action's feature value=0.827313244342804 max-choc-feature=0.9906516671180725\n",
            "action's feature value=0.7780388593673706 max-choc-feature=0.9890884160995483\n",
            "action's feature value=0.9596956372261047 max-choc-feature=0.9605224132537842\n",
            "action's feature value=0.9260265231132507 max-choc-feature=0.9260265231132507\n",
            "action's feature value=0.8996517062187195 max-choc-feature=0.9829264879226685\n",
            "action's feature value=0.8846220374107361 max-choc-feature=0.9454309940338135\n",
            "action's feature value=0.8184221386909485 max-choc-feature=0.9270205497741699\n",
            "action's feature value=0.9585323333740234 max-choc-feature=0.9585323333740234\n",
            "action's feature value=0.8444705009460449 max-choc-feature=0.9738933444023132\n",
            "action's feature value=0.8400167226791382 max-choc-feature=0.9793245196342468\n",
            "action's feature value=0.9559617638587952 max-choc-feature=0.9890880584716797\n",
            "action's feature value=0.9090927839279175 max-choc-feature=0.9670467972755432\n",
            "action's feature value=0.9465571045875549 max-choc-feature=0.9703752994537354\n",
            "action's feature value=0.8700997829437256 max-choc-feature=0.9550641179084778\n",
            "action's feature value=0.9574885368347168 max-choc-feature=0.9574885368347168\n",
            "action's feature value=0.8878343105316162 max-choc-feature=0.9229142665863037\n",
            "action's feature value=0.9832027554512024 max-choc-feature=0.9832027554512024\n",
            "action's feature value=0.998198926448822 max-choc-feature=0.998198926448822\n",
            "action's feature value=0.9414392709732056 max-choc-feature=0.9642097353935242\n",
            "action's feature value=0.9004101753234863 max-choc-feature=0.9417421221733093\n",
            "action's feature value=0.9040508270263672 max-choc-feature=0.9040508270263672\n",
            "action's feature value=0.9788569808006287 max-choc-feature=0.9835554361343384\n",
            "action's feature value=0.9334558844566345 max-choc-feature=0.9747744202613831\n",
            "action's feature value=0.8048229813575745 max-choc-feature=0.8408302664756775\n",
            "action's feature value=0.9482967257499695 max-choc-feature=0.9682117700576782\n",
            "action's feature value=0.9669559597969055 max-choc-feature=0.974723219871521\n",
            "action's feature value=0.9895250797271729 max-choc-feature=0.9895250797271729\n",
            "action's feature value=0.881066620349884 max-choc-feature=0.9818642139434814\n",
            "action's feature value=0.9736415147781372 max-choc-feature=0.987434983253479\n",
            "action's feature value=0.9674006104469299 max-choc-feature=0.9674006104469299\n",
            "action's feature value=0.9522157907485962 max-choc-feature=0.9522157907485962\n",
            "action's feature value=0.9103958606719971 max-choc-feature=0.9649279713630676\n",
            "action's feature value=0.9949018955230713 max-choc-feature=0.9949018955230713\n",
            "action's feature value=0.8948697447776794 max-choc-feature=0.9630938768386841\n",
            "action's feature value=0.9801590442657471 max-choc-feature=0.9801590442657471\n",
            "action's feature value=0.9809793829917908 max-choc-feature=0.9809793829917908\n",
            "action's feature value=0.9905391931533813 max-choc-feature=0.9905391931533813\n",
            "action's feature value=0.9747872352600098 max-choc-feature=0.9747872352600098\n",
            "action's feature value=0.8878650069236755 max-choc-feature=0.9961004257202148\n",
            "action's feature value=0.8113794326782227 max-choc-feature=0.9473085999488831\n",
            "action's feature value=0.774865448474884 max-choc-feature=0.9615751504898071\n"
          ]
        }
      ],
      "source": [
        "# Let's see what items our SlateQ recommends now that it has been lightly trained\n",
        "lts_20_2_env = LTSWithStrongerDissatisfactionEffect(LongTermSatisfactionRecSimEnv(config=slateq_config[\"env_config\"]))\n",
        "\n",
        "obs = lts_20_2_env.reset()\n",
        "done = False\n",
        "while not done:\n",
        "    action = slateq_trainer.compute_single_action(input_dict={\"obs\": obs})\n",
        "    feat_value_of_action = obs[\"doc\"][str(action[0])][0]\n",
        "    max_feat_action = np.argmax([value for _, value in obs[\"doc\"].items()])\n",
        "    max_choc_feat = obs['doc'][str(max_feat_action)][0]\n",
        "    # Print out the picked document's feature value and compare that to the highest possible feature value.\n",
        "    print(f\"action's feature value={feat_value_of_action} max-choc-feature={max_choc_feat}\")\n",
        "    \n",
        "    obs, r, done, _ = lts_20_2_env.step(action)\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f409efcd-9c5c-4d91-a1ae-121b1b2fa698",
      "metadata": {
        "id": "f409efcd-9c5c-4d91-a1ae-121b1b2fa698"
      },
      "source": [
        "### Optional\n",
        "\n",
        "Feel free to play around with the following code in order to learn how RLlib - under the hood - calculates actions from the environment's observations using the SlateQ Policy and its NN models inside our Trainer object):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "b2b62d74-6392-453a-b25f-f8cbc90009d6",
      "metadata": {
        "id": "b2b62d74-6392-453a-b25f-f8cbc90009d6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bae3a3b8-2882-4eee-b6f1-3487bce38130"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Our Policy right now is: SlateQTFPolicy\n",
            "Our Policy's observation space is: Dict(user:Box([0.], [1.], (1,), float32), doc:Dict(0:Box([0.], [1.], (1,), float32), 1:Box([0.], [1.], (1,), float32), 2:Box([0.], [1.], (1,), float32), 3:Box([0.], [1.], (1,), float32), 4:Box([0.], [1.], (1,), float32), 5:Box([0.], [1.], (1,), float32), 6:Box([0.], [1.], (1,), float32), 7:Box([0.], [1.], (1,), float32), 8:Box([0.], [1.], (1,), float32), 9:Box([0.], [1.], (1,), float32), 10:Box([0.], [1.], (1,), float32), 11:Box([0.], [1.], (1,), float32), 12:Box([0.], [1.], (1,), float32), 13:Box([0.], [1.], (1,), float32), 14:Box([0.], [1.], (1,), float32), 15:Box([0.], [1.], (1,), float32), 16:Box([0.], [1.], (1,), float32), 17:Box([0.], [1.], (1,), float32), 18:Box([0.], [1.], (1,), float32), 19:Box([0.], [1.], (1,), float32)), response:Tuple(Dict(click:Discrete(2), watch_time:Box(0.0, 100.0, (), float32)), Dict(click:Discrete(2), watch_time:Box(0.0, 100.0, (), float32))))\n",
            "\n",
            "Our Policy's action space is: MultiDiscrete([20 20])\n",
            "\n",
            "q_values_per_candidate=[[24.311869 23.904531 24.598536 24.420176 24.769163 24.116327 24.353746\n",
            "  24.460714 24.579016 24.82837  24.714819 23.961123 24.81995  24.417238\n",
            "  24.66211  24.409763 24.15305  24.503935 24.641062 24.28935 ]]\n"
          ]
        }
      ],
      "source": [
        "# To get the policy inside the Trainer, use `Trainer.get_policy([policy ID]=\"default_policy\")`:\n",
        "policy = slateq_trainer.get_policy()\n",
        "print(f\"Our Policy right now is: {policy}\")\n",
        "\n",
        "# To get to the model inside any policy, do:\n",
        "model = policy.model\n",
        "#print(f\"Our Policy's model is: {model}\")\n",
        "\n",
        "# Print out the policy's action and observation spaces.\n",
        "print(f\"Our Policy's observation space is: {policy.observation_space}\\n\")\n",
        "print(f\"Our Policy's action space is: {policy.action_space}\\n\")\n",
        "\n",
        "# Produce a random obervation (B=1; batch of size 1).\n",
        "obs = lts_20_2_env.observation_space.sample()\n",
        "\n",
        "# tf-specific code: Use tf1.Session().\n",
        "sess = policy.get_session()\n",
        "\n",
        "# Get the action logits (as torch tensor).\n",
        "with sess.graph.as_default():\n",
        "    q_values_per_candidate = model.q_value_head([\n",
        "        np.expand_dims(obs[\"user\"], 0),\n",
        "        np.expand_dims(np.concatenate([value for value in obs[\"doc\"].values()]), 0),\n",
        "    ])\n",
        "print(f\"q_values_per_candidate={sess.run(q_values_per_candidate)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "de603d14-f0cb-4363-a72b-8f147c094071",
      "metadata": {
        "id": "de603d14-f0cb-4363-a72b-8f147c094071"
      },
      "source": [
        "### Stop the RLlib Trainer\n",
        "\n",
        "In order to release all resources from a Trainer, you can use a Trainer's `stop()` method."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "737dca4f-942f-4fda-abcc-0052263a103b",
      "metadata": {
        "id": "737dca4f-942f-4fda-abcc-0052263a103b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "da8a0b45-6af8-432e-a0ff-3300c256fc80"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-04-19 16:39:00,893\tINFO services.py:1456 -- View the Ray dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8266\u001b[39m\u001b[22m\n"
          ]
        }
      ],
      "source": [
        "# In order to release resources that a Trainer uses, you can call its `stop()` method:\n",
        "slateq_trainer.stop()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ecce74b8-20ed-43c5-ad88-54a2dec32f71",
      "metadata": {
        "tags": [],
        "id": "ecce74b8-20ed-43c5-ad88-54a2dec32f71"
      },
      "source": [
        "### Recap: Advantages and Disadvantages of SlateQ:\n",
        "#### Advantages:\n",
        "* Decomposes MultiDiscrete action space (better understanding of items inside a k-slate)\n",
        "* Handles long-horizon credit assignment better than bandits (Q-learning)\n",
        "* Handles > 1 user problems\n",
        "* Sample efficient (due to replay buffer + off-policy DQN-style learning)\n",
        "\n",
        "#### Disadvantages\n",
        "* Uses larger (deep) model(s): One Q-value NN head per candidate\n",
        "* Slower and heavier feel to it\n",
        "* Requires careful hyperparameter-tuning, e.g. exploration timesteps."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e00f8e5a-d8a8-451d-bb97-b2000dbb2f9d",
      "metadata": {
        "id": "e00f8e5a-d8a8-451d-bb97-b2000dbb2f9d"
      },
      "source": [
        "## Time for Q&A\n",
        "\n",
        "...\n",
        "\n",
        "## Thank you for listening and participating!\n",
        "\n",
        "### Here are a couple of links that you may find useful.\n",
        "\n",
        "- The <a href=\"https://github.com/sven1977/rllib_tutorials/tree/main/production_rl_2022\">github repo of this tutorial</a>.\n",
        "- <a href=\"https://docs.ray.io/en/latest/rllib/index.html\">RLlib's documentation main page</a>.\n",
        "- <a href=\"http://discuss.ray.io\">Our discourse forum</a> to ask questions on Ray and its libraries.\n",
        "- Our <a href=\"https://forms.gle/9TSdDYUgxYs8SA9e8\">Slack channel</a> for interacting with other Ray RLlib users.\n",
        "- The <a href=\"https://github.com/ray-project/ray/blob/master/rllib/examples/\">RLlib examples scripts folder</a> with tons of examples on how to do different stuff with RLlib.\n",
        "- A <a href=\"https://medium.com/distributed-computing-with-ray/reinforcement-learning-with-rllib-in-the-unity-game-engine-1a98080a7c0d\">blog post on training with RLlib inside a Unity3D environment</a>.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Shutdown Ray if you are finished"
      ],
      "metadata": {
        "id": "u6ej2NZcIvrv"
      },
      "id": "u6ej2NZcIvrv"
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "dfac2bd7-8a01-4a89-a899-c5a33b7ada4c",
      "metadata": {
        "id": "dfac2bd7-8a01-4a89-a899-c5a33b7ada4c"
      },
      "outputs": [],
      "source": [
        "# # if you are done with Ray\n",
        "# if ray.is_initialized():\n",
        "#     ray.shutdown()\n",
        "\n",
        "# # # note this is how to start ray manually on your laptop\n",
        "# # ray.init()\n",
        "\n",
        "# # # note this is how to start ray on a cloud\n",
        "# # ray.init(anyscale_cluster_name, [cluster_env | runtime_env] )"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "colab": {
      "name": "Copy of tutorial_notebook.ipynb",
      "provenance": [],
      "toc_visible": true,
      "collapsed_sections": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}