{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6aa06051",
   "metadata": {},
   "source": [
    "# Reinforcement Learning for Recommender Systems\n",
    "## From Contextual Bandits to Slate-Q\n",
    "\n",
    "<table>\n",
    "<tr>\n",
    "    <td> <img src=\"images/youtube.png\" style=\"width: 230px;\"/> </td>\n",
    "    <td> <img src=\"images/dota2.jpg\" style=\"width: 213px;\"/> </td>\n",
    "    <td> <img src=\"images/forklifts.jpg\" style=\"width: 169px;\"/> </td>\n",
    "    <td> <img src=\"images/spotify.jpg\" style=\"width: 254px;\"/> </td>\n",
    "    <td> <img src=\"images/robots.jpg\" style=\"width: 252px;\"/> </td>\n",
    "</tr>\n",
    "</table>\n",
    "\n",
    "\n",
    "### Overview\n",
    "“Reinforcement Learning for Recommender Systems, From Contextual Bandits to Slate-Q” is a tutorial for industry researchers, domain-experts, and ML-engineers, showcasing ...\n",
    "\n",
    "1) .. how you can use RLlib to build a recommender system **simulator** for your industry applications and run Bandit algorithms and the Slate-Q algorithm against this simulator.\n",
    "\n",
    "2) .. how RLlib's offline algorithms pose solutions in case you **don't have a simulator** of your problem environment at hand.\n",
    "\n",
    "We will further explore how to deploy trained models to production using Ray Serve.\n",
    "\n",
    "During the live-coding phases, we will using a recommender system simulating environment by google's RecSim and configure and run 2 RLlib algorithms against it. We'll also demonstrate how you may use offline RL as a solution for recommender systems and how to deploy a learned policy into production.\n",
    "\n",
    "RLlib offers industry-grade scalability, a large list of algos to choose from (offline, model-based, model-free, etc..), support for TensorFlow and PyTorch, and a unified API for a variety of applications. This tutorial includes a brief introduction to provide an overview of concepts (e.g. why RL?) before proceeding to RLlib (recommender system) environments, neural network models, offline RL, student exercises, Q/A, and more. All code will be provided as .py files in a GitHub repo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "green-insertion",
   "metadata": {},
   "source": [
    "### Intended Audience\n",
    "* Python programmers who are interested in using RL to solve their specific industry decision making problems and who want to get started with RLlib.\n",
    "\n",
    "### Prerequisites\n",
    "* Some Python programming experience.\n",
    "* Some familiarity with machine learning.\n",
    "* *Helpful, but not required:* Experience in reinforcement learning and Ray.\n",
    "* *Helpful, but not required:* Experience with TensorFlow or PyTorch.\n",
    "\n",
    "### Requirements/Dependencies\n",
    "\n",
    "To get this very notebook up and running on your local machine, you can follow these steps here:\n",
    "\n",
    "Install conda (https://www.anaconda.com/products/individual)\n",
    "\n",
    "Then ...\n",
    "\n",
    "#### Quick `conda` setup instructions (Linux):\n",
    "```\n",
    "$ conda create -n rllib_tutorial python=3.9\n",
    "$ conda activate rllib_tutorial\n",
    "$ pip install \"ray[rllib,serve]\" recsim jupyterlab tensorflow torch\n",
    "```\n",
    "\n",
    "#### Quick `conda` setup instructions (Mac):\n",
    "```\n",
    "$ conda create -n rllib_tutorial python=3.9\n",
    "$ conda activate rllib_tutorial\n",
    "$ pip install cmake \"ray[rllib,serve]\" recsim jupyterlab tensorflow torch\n",
    "$ pip install grpcio # <- extra install only on apple M1 mac\n",
    "$ # **Note:** In case you are getting a \"requires TensorFlow version >= 2.8\" error at some point in the notebook, try the following:\n",
    "$ pip uninstall -y tensorflow\n",
    "$ python -m pip install tensorflow-macos --no-cache-dir\n",
    "```\n",
    "\n",
    "#### Quick `conda` setup instructions (Win10):\n",
    "```\n",
    "$ conda create -n rllib_tutorial python=3.9\n",
    "$ conda activate rllib_tutorial\n",
    "$ pip install \"ray[rllib,serve]\" recsim jupyterlab tensorflow torch\n",
    "$ pip install pywin32 # <- extra install only on Win10.\n",
    "```\n",
    "\n",
    "### Opening these tutorial files:\n",
    "```\n",
    "$ git clone https://github.com/sven1977/rllib_tutorials\n",
    "$ cd rllib_tutorials/production_rl_2022\n",
    "$ jupyter-lab\n",
    "```\n",
    "\n",
    "\n",
    "### Key Takeaways\n",
    "* What is reinforcement learning and RLlib?\n",
    "* How do recommender systems work? How do we build our own?\n",
    "* How do we train RLlib's different algorithms on a recommender system problem?\n",
    "* What's offline RL and how can I use it with RLlib?\n",
    "* How do I deploy an already trained policy into production using Ray Serve.\n",
    "\n",
    "\n",
    "### Tutorial Outline\n",
    "\n",
    "1. Reinforcement learning (RL) in a nutshell.\n",
    "1. How to formulate any problem as an RL-solvable one?\n",
    "1. Recommender systems - How they work.\n",
    "1. Why you should use RLlib.\n",
    "\n",
    "(15min break)\n",
    "\n",
    "1. [Google RecSim - Build your own recom sys simulator.](#recsim)\n",
    "1. [Dissecting the \"long term satisfaction\" (LTE) environment.](#dissecting_lte)\n",
    "1. [Using a contextual Bandit algorithm with RLlib and starting our first training run on the LTE env.](#rllib)\n",
    "1. [What did the Bandit learn?](#bandit_results)\n",
    "1. [Intro to Slate-Q.](#slateq)\n",
    "1. [Starting a Slate-Q training run.](#slateq_experiment)\n",
    "\n",
    "(15min break)\n",
    "\n",
    "1. [Analyzing the results of the SlateQ run.](#slateq_results)\n",
    "1. [Intro to Offline RL - What if we don't have an environment?](#offline_rl)\n",
    "1. [BC (behavior cloning) and MARWIL: Quick how-to and setup instructions.](#bc_and_marwil)\n",
    "1. [Off policy evaluation (OPE) as a means to estimate how well an offline-RL trained policy will perform in production.](#ope)\n",
    "1. [Ray Serve example: How can we deploy a trained policy into our production environment?](#ray_serve)\n",
    "\n",
    "\n",
    "### Other Recommended Readings\n",
    "* [Intro to RLlib: Example Environments](https://medium.com/distributed-computing-with-ray/intro-to-rllib-example-environments-3a113f532c70)* [Reinforcement Learning with RLlib in the Unity Game Engine](https://medium.com/distributed-computing-with-ray/reinforcement-learning-with-rllib-in-the-unity-game-engine-1a98080a7c0d)\n",
    "\n",
    "<img src=\"images/unity3d_blog_post.png\" width=400>\n",
    "\n",
    "* [Attention Nets and More with RLlib's Trajectory View API](https://medium.com/distributed-computing-with-ray/attention-nets-and-more-with-rllibs-trajectory-view-api-d326339a6e65)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9f7e21f-f3de-4bad-a3a7-4bbd0b015559",
   "metadata": {},
   "source": [
    "# Let's start!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "930deb27-e739-4507-bc24-e39ded9caeb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.9.12\n",
      "ray: 1.12.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/christy/mambaforge/envs/rllib/lib/python3.9/site-packages/jax/_src/lib/__init__.py:33: UserWarning: JAX on Mac ARM machines is experimental and minimally tested. Please see https://github.com/google/jax/issues/5501 in the event of problems.\n",
      "  warnings.warn(\"JAX on Mac ARM machines is experimental and minimally tested. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf: 2.7.0\n",
      "ok\n"
     ]
    }
   ],
   "source": [
    "# Let's get started with some basic imports.\n",
    "\n",
    "import ray  # .. of course\n",
    "from ray import serve\n",
    "from ray import tune\n",
    "\n",
    "from collections import OrderedDict\n",
    "import gym  # RL environments and action/observation spaces\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas\n",
    "from pprint import pprint\n",
    "import re\n",
    "import recsim  # google's RecSim package.\n",
    "import requests\n",
    "from scipy.interpolate import make_interp_spline, BSpline\n",
    "from scipy.stats import linregress, sem\n",
    "from starlette.requests import Request\n",
    "import tree  # dm_tree\n",
    "\n",
    "!python --version\n",
    "print(f\"ray: {ray.__version__}\")\n",
    "import tensorflow as tf\n",
    "print(f\"tf: {tf.__version__}\")\n",
    "print(\"ok\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90f010a6-7ee3-49b3-814a-2b9455c66c8f",
   "metadata": {},
   "source": [
    "<a id='recsim'></a>\n",
    "## Introducing google RecSim\n",
    "\n",
    "<img src=\"images/recsim_documentation.png\" width=600 style=\"float:right;\">\n",
    "\n",
    "<a href=\"https://github.com/google-research/recsim\">Google's RecSim package</a> offers a flexible way for you to <a href=\"https://github.com/google-research/recsim/blob/master/recsim/colab/RecSim_Developing_an_Environment.ipynb\">define the different building blocks of a recommender system</a>:\n",
    "\n",
    "\n",
    "- User model (how do users change their preferences when having been faced with, selected, and consumed certain items?).\n",
    "- Document model: Features of documents and how do documents get pre-selected/sampled.\n",
    "- Reward functions.\n",
    "\n",
    "RLlib comes with 3 off-the-shelf RecSim environments that are ready for training (with RLlib):\n",
    "* Long Term Satisfaction (<- the \"env\" we will use in this tutorial)\n",
    "* Interest Evolution\n",
    "* Interest Exploration\n",
    "\n",
    "<a id='dissecting_lte'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c3363126-0f38-4f92-a031-1ea791b9a747",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action space = Discrete(10)\n"
     ]
    }
   ],
   "source": [
    "# Import the built-in RecSim exapmle environment: \"Long Term Satisfaction\", ready to be trained by RLlib.\n",
    "from ray.rllib.examples.env.recommender_system_envs_with_recsim import LongTermSatisfactionRecSimEnv\n",
    "\n",
    "# Create a RecSim instance using the following config parameters (very similar to what we used above in our own recommender system env):\n",
    "lts_10_1_env = LongTermSatisfactionRecSimEnv({\n",
    "    \"num_candidates\": 10,  # action_sapce = Discrete(10) -> int 0-9\n",
    "    \"slate_size\": 1,\n",
    "    # Set to False for re-using the same candidate doecuments each timestep.\n",
    "    \"resample_documents\": False,\n",
    "    # Convert MultiDiscrete actions to Discrete (flatten action space).\n",
    "    # e.g. slate_size=2 and num_candidates=10 -> MultiDiscrete([10, 10]) -> Discrete(100)  # 10x10\n",
    "    \"convert_to_discrete_action_space\": True,\n",
    "})\n",
    "\n",
    "# What are our spaces?\n",
    "#print(f\"observation space = {lts_10_1_env.observation_space}\")\n",
    "print(f\"action space = {lts_10_1_env.action_space}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3eac27a-bc9a-47dd-b3f9-cffd3e590e3b",
   "metadata": {},
   "source": [
    "Let's make use of our knowledge on the gym.Env API and call our new environment's `reset()` and `step()` methods.\n",
    "First: `reset()` to receive the initial observation in a new episode/trajectory/session:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "913d34a6-fac6-4436-b1d5-9292ebf88006",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('user', array([], dtype=float32)),\n",
      "             ('doc',\n",
      "              {'0': array([0.5488135], dtype=float32),\n",
      "               '1': array([0.71518934], dtype=float32),\n",
      "               '2': array([0.60276335], dtype=float32),\n",
      "               '3': array([0.5448832], dtype=float32),\n",
      "               '4': array([0.4236548], dtype=float32),\n",
      "               '5': array([0.6458941], dtype=float32),\n",
      "               '6': array([0.4375872], dtype=float32),\n",
      "               '7': array([0.891773], dtype=float32),\n",
      "               '8': array([0.96366274], dtype=float32),\n",
      "               '9': array([0.3834415], dtype=float32)}),\n",
      "             ('response',\n",
      "              (OrderedDict([('click', 0),\n",
      "                            ('engagement',\n",
      "                             array(50.288185, dtype=float32))]),))])\n"
     ]
    }
   ],
   "source": [
    "# Start a new episode and look at initial observation.\n",
    "obs = lts_10_1_env.reset()\n",
    "pprint(obs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9857197-f79f-4f9d-8e3f-68eee20daf94",
   "metadata": {},
   "source": [
    "Now let's play RL agent ourselves and recommend some items (pick some actions) via the environment's `step()` method:\n",
    "\n",
    "**Task:** Execute the following cell a couple of times chosing different actions (from 0 - 9) to be sent into the environment's `step()` method. Each time, look at the returned next observation, reward, and `done` flag and write down what you find interesting about the dynamics and observations of this environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1f10b83e-993f-4c59-8e13-4e1074bfd7af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('user', array([], dtype=float32)),\n",
      "             ('doc',\n",
      "              {'0': array([0.5488135], dtype=float32),\n",
      "               '1': array([0.71518934], dtype=float32),\n",
      "               '2': array([0.60276335], dtype=float32),\n",
      "               '3': array([0.5448832], dtype=float32),\n",
      "               '4': array([0.4236548], dtype=float32),\n",
      "               '5': array([0.6458941], dtype=float32),\n",
      "               '6': array([0.4375872], dtype=float32),\n",
      "               '7': array([0.891773], dtype=float32),\n",
      "               '8': array([0.96366274], dtype=float32),\n",
      "               '9': array([0.3834415], dtype=float32)}),\n",
      "             ('response',\n",
      "              ({'click': 1, 'engagement': array(3.6387048, dtype=float32)},))])\n",
      "reward = 3.64; done = False\n"
     ]
    }
   ],
   "source": [
    "# Let's send our first action (1-slate back into the env) using the env's `step()` method.\n",
    "action = 3  # Discrete(10): 0-9 are all valid actions\n",
    "\n",
    "# This method returns 4 items:\n",
    "# - next observation (after having applied the action)\n",
    "# - reward (after having applied the action)\n",
    "# - `done` flag; if True, the episode is terminated and the environment needs to be `reset()` again.\n",
    "# - info dict (we'll ignore this)\n",
    "next_obs, reward, done, _ = lts_10_1_env.step(action)\n",
    "\n",
    "# Print out the next observation.\n",
    "# We expect the \"doc\" and \"user\" items to be the same as in the previous observation\n",
    "# b/c we set \"resample_documents\" to False.\n",
    "pprint(next_obs)\n",
    "# Print out rewards and the vlaue of the `done` flag.\n",
    "print(f\"reward = {reward:.2f}; done = {done}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "444c7982-b372-4fe9-806a-18a29101c094",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "<br />.<br />.<br />.<br />.<br />.\n",
    "<br />.<br />.<br />.<br />.<br />.\n",
    "<br />.<br />.<br />.<br />.<br />.\n",
    "<br />.<br />.<br />.<br />.<br />.\n",
    "<br />..<br />.<br />.<br />.<br />.\n",
    "\n",
    "\n",
    "### What have we learnt from experimenting with the environment?\n",
    "\n",
    "* User's state (if any) is hidden to agent (not part of observation).\n",
    "* Episodes seem to last at least n timesteps -> user seems to have some time budget to spend.\n",
    "* User always seems to click, no matter what we recommend.\n",
    "* Reward seems to be always identical to the \"engagement\" value (of the clicked item). These values range somewhere between 0.0 and 20.0+.\n",
    "* Weak suspicion: If we always recommend the item with the highest feature value, rewards seem to taper off over time - in most of the episodes.\n",
    "* Weak suspicion: If we always recommend the item with the lowest feature value, rewards seem to increase over time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a05210c-69ea-4c09-acf8-831fffca5f8c",
   "metadata": {},
   "source": [
    "### What the environment actually does under the hood\n",
    "\n",
    "Let's take a quick look at a pre-configured RecSim environment: \"Long Term Satisfaction\".\n",
    "\n",
    "<img src=\"images/long_term_satisfaction_env.png\" width=1200>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5958ff84-f7d0-45c9-aa43-b807243b8452",
   "metadata": {},
   "source": [
    "Now that we know, that there is a double objective built into the env (a. sweetness -> engagement; b. sweetness -> unhappyness; unhappyness -> low engagement), let's make this effect a tiny bit stronger by slightly modifying the environment. As said above, the effect is very weak and almost not measurable, which is a problem on the env's side. We can use this following `gym.ObservationWrapper` class in the cell below to \"fix\" that problem:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "375a469e-aa46-4038-9df7-02cabccdad50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ok; registered the string 'modified_lts' to be used in RLlib configs (see below)\n"
     ]
    }
   ],
   "source": [
    "# Modifying wrapper around the LTS (Long Term Satisfaction) env:\n",
    "# - allows us to tweak the user model (and thus: reward behavior)\n",
    "# - adds user's current satisfaction value to observation\n",
    "\n",
    "class LTSWithStrongerDissatisfactionEffect(gym.ObservationWrapper):\n",
    "\n",
    "    def __init__(self, env):\n",
    "        # Tweak incoming environment.\n",
    "        env.environment._user_model._user_sampler._state_parameters.update({\n",
    "            \"sensitivity\": 0.058,\n",
    "            \"time_budget\": 120,\n",
    "            \"choc_stddev\": 0.1,\n",
    "            \"kale_stddev\": 0.1,\n",
    "            #\"innovation_stddev\": 0.01,\n",
    "            #\"choc_mean\": 1.25,\n",
    "            #\"kale_mean\": 1.0,\n",
    "            #\"memory_discount\": 0.9,\n",
    "        })\n",
    "\n",
    "        super().__init__(env)\n",
    "\n",
    "        # Adjust observation space.\n",
    "        if \"response\" in self.observation_space.spaces:\n",
    "            self.observation_space.spaces[\"user\"] = gym.spaces.Box(0.0, 1.0, (1, ), dtype=np.float32)\n",
    "            for r in self.observation_space[\"response\"]:\n",
    "                if \"engagement\" in r.spaces:\n",
    "                    r.spaces[\"watch_time\"] = r.spaces[\"engagement\"]\n",
    "                    del r.spaces[\"engagement\"]\n",
    "                    break\n",
    "\n",
    "    def observation(self, observation):\n",
    "        if \"response\" in self.observation_space.spaces:\n",
    "            observation[\"user\"] = np.array([self.env.environment._user_model._user_state.satisfaction])\n",
    "            for r in observation[\"response\"]:\n",
    "                if \"engagement\" in r:\n",
    "                    r[\"watch_time\"] = r[\"engagement\"]\n",
    "                    del r[\"engagement\"]\n",
    "        return observation\n",
    "\n",
    "\n",
    "# Add the wrapping around \n",
    "tune.register_env(\"modified_lts\", lambda env_config: LTSWithStrongerDissatisfactionEffect(LongTermSatisfactionRecSimEnv(env_config)))\n",
    "\n",
    "print(\"ok; registered the string 'modified_lts' to be used in RLlib configs (see below)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbce752b-68a3-4a84-aada-97810039e4e8",
   "metadata": {},
   "source": [
    "Now that we have a stronger effect of the user's satisfaction value on the long-term rewards, we may be able to measure this effect reliably\n",
    "using the following utility code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3b1f1ab8-6b08-47c7-9dfa-3fe9bd672c14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0007349420303810714\n"
     ]
    }
   ],
   "source": [
    "# This cell should help you with your own analysis of the two above \"suspicions\":\n",
    "# Always chosing the highest/lowest-valued action will lead to a decrease/increase in rewards over the course of an episode.\n",
    "modified_lts_10_1_env = LTSWithStrongerDissatisfactionEffect(lts_10_1_env)\n",
    "\n",
    "# Capture slopes of all trendlines over all episodes.\n",
    "slopes = []\n",
    "# Run 1000 episodes.\n",
    "for _ in range(1000):\n",
    "    obs = modified_lts_10_1_env.reset()  # Reset environment to get initial observation:\n",
    "\n",
    "    # Compute actions that pick doc with highest/lowest feature value.\n",
    "    action_sweetest = np.argmax([value for _, value in obs[\"doc\"].items()])\n",
    "    action_kaleiest = np.argmin([value for _, value in obs[\"doc\"].items()])\n",
    "\n",
    "    # Play one episode.\n",
    "    done = False\n",
    "    rewards = []\n",
    "    while not done:\n",
    "        #action = action_sweetest\n",
    "        action = action_kaleiest\n",
    "        #action = np.random.choice([action_kaleiest, action_sweetest])\n",
    "\n",
    "        obs, reward, done, _ = modified_lts_10_1_env.step(action)\n",
    "        rewards.append(reward)\n",
    "\n",
    "    # Create linear model of rewards over time.\n",
    "    reward_linreg = linregress(np.array((range(len(rewards)))), np.array(rewards))\n",
    "    slopes.append(reward_linreg.slope)\n",
    "\n",
    "print(np.mean(slopes))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be848212-87b8-4eb1-9353-74e09ae72310",
   "metadata": {},
   "source": [
    "## Measuring random baseline of our environment\n",
    "\n",
    "In the cells above, we created a new environment instance (`lts_10_1_env`). As we have seen above, in order to start \"walking\" through a recommender system episode, we need to perform `reset()` and then several `step()` calls (with different actions) until the returned `done` flag is True.\n",
    "\n",
    "Let's find out how well a randomly acting agent performs in this environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "spatial-geography",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that measures and outputs the random baseline reward.\n",
    "# This is the expected accumulated reward per episode, if we act randomly (recommend random items) at each time step.\n",
    "def measure_random_performance_for_env(env, episodes=1000, verbose=False):\n",
    "\n",
    "    # Reset the env.\n",
    "    env.reset()\n",
    "\n",
    "    # Number of episodes already done.\n",
    "    num_episodes = 0\n",
    "    # Current episode's accumulated reward.\n",
    "    episode_reward = 0.0\n",
    "    # Collect all episode rewards here to be able to calculate a random baseline reward.\n",
    "    episode_rewards = []\n",
    "\n",
    "    # Enter while loop (to step through the episode).\n",
    "    while num_episodes < episodes:\n",
    "        # Produce a random action.\n",
    "        action = env.action_space.sample()\n",
    "\n",
    "        # Send the action to the env's `step()` method to receive: obs, reward, done, and info.\n",
    "        obs, reward, done, _ = env.step(action)\n",
    "        episode_reward += reward\n",
    "\n",
    "        # Check, whether the episde is done, if yes, reset and increase episode counter.\n",
    "        if done:\n",
    "            if verbose:\n",
    "                print(f\"Episode done - accumulated reward={episode_reward}\")\n",
    "            elif num_episodes % 100 == 0:\n",
    "                print(f\" {num_episodes} \", end=\"\")\n",
    "            elif num_episodes % 10 == 0:\n",
    "                print(\".\", end=\"\")\n",
    "            num_episodes += 1\n",
    "            env.reset()\n",
    "            episode_rewards.append(episode_reward)\n",
    "            episode_reward = 0.0\n",
    "\n",
    "    # Print out and return mean episode reward (and standard error of the mean).\n",
    "    env_mean_random_reward = np.mean(episode_rewards)\n",
    "\n",
    "    print(f\"\\n\\nMean episode reward when acting randomly: {env_mean_random_reward:.2f}+/-{sem(episode_rewards):.2f}\")\n",
    "\n",
    "    return env_mean_random_reward, sem(episode_rewards)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0e6e63e6-d030-4a45-af5b-ab88eaef3969",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1e+03 ns, sys: 0 ns, total: 1e+03 ns\n",
      "Wall time: 3.1 µs\n",
      " 0 ......... 100 ......... 200 ......... 300 ......... 400 ......... 500 ......... 600 ......... 700 ......... 800 ......... 900 .........\n",
      "\n",
      "Mean episode reward when acting randomly: 1158.06+/-0.36\n"
     ]
    }
   ],
   "source": [
    "%time\n",
    "# Let's create a somewhat tougher version of this with 20 candidates (instead of 10) and a slate-size of 2.\n",
    "# We'll also keep using our wrapper from above to strengthen the dissatisfaction effect on the engagement:\n",
    "lts_20_2_env = LTSWithStrongerDissatisfactionEffect(LongTermSatisfactionRecSimEnv(config={\n",
    "    \"num_candidates\": 20,\n",
    "    \"slate_size\": 2,  # MultiDiscrete([20, 20]) -> Discrete(400)\n",
    "    \"resample_documents\": True,\n",
    "    # Convert to Discrete action space.\n",
    "    \"convert_to_discrete_action_space\": True,\n",
    "    # Wrap observations for RLlib bandit: Only changes dict keys (\"item\" instead of \"doc\").\n",
    "    \"wrap_for_bandits\": True,\n",
    "}))\n",
    "\n",
    "lts_20_2_env_mean_random_reward, _ = measure_random_performance_for_env(lts_20_2_env, episodes=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b20ac75-f3e6-4975-a209-2bf110b4ee13",
   "metadata": {},
   "source": [
    "# Plugging in RLlib\n",
    "<a id='rllib'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a76f02f-ef66-484d-8a1a-074a6e25c84a",
   "metadata": {},
   "source": [
    "## Picking an RLlib algorithm (\"Trainer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65aa24b2-ac17-44a3-b7b1-274ce2f50a87",
   "metadata": {},
   "source": [
    "https://docs.ray.io/en/master/rllib-algorithms.html#available-algorithms-overview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0194b33a-e031-49ce-9ff2-b32e328f9955",
   "metadata": {},
   "source": [
    "<img src=\"images/rllib_algorithms_bandits.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62b1b0b3-ec96-41c0-9d5b-93db1c5ce021",
   "metadata": {},
   "source": [
    "### Trying a \"Contextual n-armed Bandit\" on our environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4a26e094-9887-4fc6-88b6-d1448e931526",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In order to use one of the above algorithms, you may instantiate its associated Trainer class.\n",
    "# For example, to import a Bandit Trainer w/ Upper Confidence Bound (UCB) exploration, do:\n",
    "\n",
    "from ray.rllib.agents.bandit import BanditLinUCBTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0911f212-523e-4a75-846d-342dd2a681a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bandit's default config is:\n",
      "{'_disable_action_flattening': False,\n",
      " '_disable_execution_plan_api': False,\n",
      " '_disable_preprocessor_api': False,\n",
      " '_fake_gpus': False,\n",
      " '_tf_policy_handles_more_than_one_loss': False,\n",
      " 'action_space': None,\n",
      " 'actions_in_input_normalized': False,\n",
      " 'always_attach_evaluation_results': False,\n",
      " 'batch_mode': 'truncate_episodes',\n",
      " 'callbacks': <class 'ray.rllib.agents.callbacks.DefaultCallbacks'>,\n",
      " 'clip_actions': False,\n",
      " 'clip_rewards': None,\n",
      " 'collect_metrics_timeout': -1,\n",
      " 'compress_observations': False,\n",
      " 'create_env_on_driver': False,\n",
      " 'custom_eval_function': None,\n",
      " 'custom_resources_per_worker': {},\n",
      " 'disable_env_checking': False,\n",
      " 'eager_max_retraces': 20,\n",
      " 'eager_tracing': False,\n",
      " 'env': None,\n",
      " 'env_config': {},\n",
      " 'env_task_fn': None,\n",
      " 'evaluation_config': {},\n",
      " 'evaluation_duration': 10,\n",
      " 'evaluation_duration_unit': 'episodes',\n",
      " 'evaluation_interval': None,\n",
      " 'evaluation_num_episodes': -1,\n",
      " 'evaluation_num_workers': 0,\n",
      " 'evaluation_parallel_to_training': False,\n",
      " 'exploration_config': {'type': 'StochasticSampling'},\n",
      " 'explore': True,\n",
      " 'extra_python_environs_for_driver': {},\n",
      " 'extra_python_environs_for_worker': {},\n",
      " 'fake_sampler': False,\n",
      " 'framework': 'torch',\n",
      " 'gamma': 0.99,\n",
      " 'horizon': None,\n",
      " 'ignore_worker_failures': False,\n",
      " 'in_evaluation': False,\n",
      " 'input': 'sampler',\n",
      " 'input_config': {},\n",
      " 'input_evaluation': ['is', 'wis'],\n",
      " 'keep_per_episode_custom_metrics': False,\n",
      " 'local_tf_session_args': {'inter_op_parallelism_threads': 8,\n",
      "                           'intra_op_parallelism_threads': 8},\n",
      " 'log_level': 'WARN',\n",
      " 'log_sys_usage': True,\n",
      " 'logger_config': None,\n",
      " 'lr': 0.0001,\n",
      " 'metrics_episode_collection_timeout_s': 180,\n",
      " 'metrics_num_episodes_for_smoothing': 100,\n",
      " 'metrics_smoothing_episodes': -1,\n",
      " 'min_iter_time_s': -1,\n",
      " 'min_sample_timesteps_per_reporting': None,\n",
      " 'min_time_s_per_reporting': None,\n",
      " 'min_train_timesteps_per_reporting': None,\n",
      " 'model': {'_disable_action_flattening': False,\n",
      "           '_disable_preprocessor_api': False,\n",
      "           '_time_major': False,\n",
      "           '_use_default_native_models': False,\n",
      "           'attention_dim': 64,\n",
      "           'attention_head_dim': 32,\n",
      "           'attention_init_gru_gate_bias': 2.0,\n",
      "           'attention_memory_inference': 50,\n",
      "           'attention_memory_training': 50,\n",
      "           'attention_num_heads': 1,\n",
      "           'attention_num_transformer_units': 1,\n",
      "           'attention_position_wise_mlp_dim': 32,\n",
      "           'attention_use_n_prev_actions': 0,\n",
      "           'attention_use_n_prev_rewards': 0,\n",
      "           'conv_activation': 'relu',\n",
      "           'conv_filters': None,\n",
      "           'custom_action_dist': None,\n",
      "           'custom_model': None,\n",
      "           'custom_model_config': {},\n",
      "           'custom_preprocessor': None,\n",
      "           'dim': 84,\n",
      "           'fcnet_activation': 'tanh',\n",
      "           'fcnet_hiddens': [256, 256],\n",
      "           'framestack': True,\n",
      "           'free_log_std': False,\n",
      "           'grayscale': False,\n",
      "           'lstm_cell_size': 256,\n",
      "           'lstm_use_prev_action': False,\n",
      "           'lstm_use_prev_action_reward': -1,\n",
      "           'lstm_use_prev_reward': False,\n",
      "           'max_seq_len': 20,\n",
      "           'no_final_linear': False,\n",
      "           'post_fcnet_activation': 'relu',\n",
      "           'post_fcnet_hiddens': [],\n",
      "           'use_attention': False,\n",
      "           'use_lstm': False,\n",
      "           'vf_share_layers': True,\n",
      "           'zero_mean': True},\n",
      " 'monitor': -1,\n",
      " 'multiagent': {'count_steps_by': 'env_steps',\n",
      "                'observation_fn': None,\n",
      "                'policies': {},\n",
      "                'policies_to_train': None,\n",
      "                'policy_map_cache': None,\n",
      "                'policy_map_capacity': 100,\n",
      "                'policy_mapping_fn': None,\n",
      "                'replay_mode': 'independent'},\n",
      " 'no_done_at_end': False,\n",
      " 'normalize_actions': True,\n",
      " 'num_cpus_for_driver': 1,\n",
      " 'num_cpus_per_worker': 1,\n",
      " 'num_envs_per_worker': 1,\n",
      " 'num_gpus': 0,\n",
      " 'num_gpus_per_worker': 0,\n",
      " 'num_workers': 0,\n",
      " 'observation_filter': 'NoFilter',\n",
      " 'observation_space': None,\n",
      " 'optimizer': {},\n",
      " 'output': None,\n",
      " 'output_compress_columns': ['obs', 'new_obs'],\n",
      " 'output_config': {},\n",
      " 'output_max_file_size': 67108864,\n",
      " 'placement_strategy': 'PACK',\n",
      " 'postprocess_inputs': False,\n",
      " 'preprocessor_pref': 'deepmind',\n",
      " 'record_env': False,\n",
      " 'remote_env_batch_wait_ms': 0,\n",
      " 'remote_worker_envs': False,\n",
      " 'render_env': False,\n",
      " 'rollout_fragment_length': 1,\n",
      " 'sample_async': False,\n",
      " 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>,\n",
      " 'seed': None,\n",
      " 'shuffle_buffer_size': 0,\n",
      " 'simple_optimizer': -1,\n",
      " 'soft_horizon': False,\n",
      " 'synchronize_filters': True,\n",
      " 'tf_session_args': {'allow_soft_placement': True,\n",
      "                     'device_count': {'CPU': 1},\n",
      "                     'gpu_options': {'allow_growth': True},\n",
      "                     'inter_op_parallelism_threads': 2,\n",
      "                     'intra_op_parallelism_threads': 2,\n",
      "                     'log_device_placement': False},\n",
      " 'timesteps_per_iteration': 100,\n",
      " 'train_batch_size': 1}\n"
     ]
    }
   ],
   "source": [
    "# Configuration dicts for RLlib Trainers.\n",
    "# Where are the default configuration dicts stored?\n",
    "\n",
    "# E.g. Bandit algorithms:\n",
    "from ray.rllib.agents.bandit.bandit import DEFAULT_CONFIG as BANDIT_DEFAULT_CONFIG\n",
    "print(f\"Bandit's default config is:\")\n",
    "pprint(BANDIT_DEFAULT_CONFIG)\n",
    "\n",
    "# DQN algorithm:\n",
    "#from ray.rllib.agents.dqn import DEFAULT_CONFIG as DQN_DEFAULT_CONFIG\n",
    "#print(f\"DQN's default config is:\")\n",
    "#pprint(DQN_DEFAULT_CONFIG)\n",
    "\n",
    "# Common (all algorithms).\n",
    "#from ray.rllib.agents.trainer import COMMON_CONFIG\n",
    "#print(f\"RLlib Trainer's default config is:\")\n",
    "#pprint(COMMON_CONFIG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c9bd9775-f2bb-41d9-8ff6-20be9abd68db",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-17 17:24:39,724\tWARNING rollout_worker.py:498 -- We've added a module for checking environments that are used in experiments. It will cause your environment to fail if your environment is not set upcorrectly. You can disable check env by setting `disable_env_checking` to True in your experiment config dictionary. You can run the environment checking module standalone by calling ray.rllib.utils.check_env(env).\n",
      "2022-04-17 17:24:39,725\tWARNING env.py:120 -- Your env doesn't have a .spec.max_episode_steps attribute. This is fine if you have set 'horizon' in your config dictionary, or `soft_horizon`. However, if you haven't, 'horizon' will default to infinity, and your environment will not be reset.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BanditLinUCBTrainer"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bandit_config = {\n",
    "    \"env\": \"modified_lts\",\n",
    "    \"env_config\": {\n",
    "        \"num_candidates\": 20,  # 20x19 = ~400 unique slates (arms)\n",
    "        \"slate_size\": 2,\n",
    "        \"resample_documents\": True,\n",
    "\n",
    "        # Bandit-specific flags:\n",
    "        \"convert_to_discrete_action_space\": True,\n",
    "        # Convert \"doc\" key into \"item\" key.\n",
    "        \"wrap_for_bandits\": True,\n",
    "        # Use consistent seeds for the environment ...\n",
    "        \"seed\": 0,\n",
    "    },\n",
    "    # ... and the Trainer itself.\n",
    "    \"seed\": 0,\n",
    "\n",
    "    # The following settings are affecting the reporting only:\n",
    "    # ---\n",
    "    # Generate a result dict every single time step.\n",
    "    \"timesteps_per_iteration\": 1,\n",
    "    # Report rewards as smoothed mean over this many episodes.\n",
    "    \"metrics_num_episodes_for_smoothing\": 200,\n",
    "}\n",
    "\n",
    "# Create the RLlib Trainer using above config.\n",
    "bandit_trainer = BanditLinUCBTrainer(config=bandit_config)\n",
    "bandit_trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46a22cc0-0efb-40be-85fe-720e62a7a419",
   "metadata": {},
   "source": [
    "#### Running a single training iteration, by calling the `.train()` method:\n",
    "\n",
    "One iteration for most algos involves:\n",
    "\n",
    "1. Sampling from the environment(s)\n",
    "1. Using the sampled data (observations, actions taken, rewards) to update the policy model (e.g. a neural network), such that it would pick better actions in the future, leading to higher rewards.\n",
    "\n",
    "Let's try it out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ddd18251-2a1a-4822-8744-ca6df4a14787",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-17 17:24:39,755\tWARNING bandit_torch_policy.py:48 -- The env did not report `regret` values in its `info` return, ignoring.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'agent_timesteps_total': 1,\n",
      " 'custom_metrics': {},\n",
      " 'date': '2022-04-17_17-24-39',\n",
      " 'done': False,\n",
      " 'episode_len_mean': nan,\n",
      " 'episode_media': {},\n",
      " 'episode_reward_max': nan,\n",
      " 'episode_reward_mean': nan,\n",
      " 'episode_reward_min': nan,\n",
      " 'episodes_this_iter': 0,\n",
      " 'episodes_total': 0,\n",
      " 'experiment_id': 'b81a3a2a0a284a54b6c9c19fe366ef44',\n",
      " 'hist_stats': {'episode_lengths': [], 'episode_reward': []},\n",
      " 'hostname': 'Christys-MacBook-Pro.local',\n",
      " 'info': {'learner': {'default_policy': {'learner_stats': {'update_latency': 0.0008311271667480469}}},\n",
      "          'num_agent_steps_sampled': 1,\n",
      "          'num_agent_steps_trained': 1,\n",
      "          'num_steps_sampled': 1,\n",
      "          'num_steps_trained': 1,\n",
      "          'num_steps_trained_this_iter': 1},\n",
      " 'iterations_since_restore': 1,\n",
      " 'node_ip': '127.0.0.1',\n",
      " 'num_healthy_workers': 0,\n",
      " 'off_policy_estimator': {},\n",
      " 'perf': {'cpu_util_percent': 23.6, 'ram_util_percent': 68.0},\n",
      " 'pid': 98498,\n",
      " 'policy_reward_max': {},\n",
      " 'policy_reward_mean': {},\n",
      " 'policy_reward_min': {},\n",
      " 'sampler_perf': {},\n",
      " 'time_since_restore': 0.009498119354248047,\n",
      " 'time_this_iter_s': 0.009498119354248047,\n",
      " 'time_total_s': 0.009498119354248047,\n",
      " 'timers': {'learn_throughput': 898.523,\n",
      "            'learn_time_ms': 1.113,\n",
      "            'load_throughput': 9258.949,\n",
      "            'load_time_ms': 0.108,\n",
      "            'sample_throughput': 47.149,\n",
      "            'sample_time_ms': 21.209},\n",
      " 'timestamp': 1650241479,\n",
      " 'timesteps_since_restore': 1,\n",
      " 'timesteps_this_iter': 1,\n",
      " 'timesteps_total': 1,\n",
      " 'training_iteration': 1,\n",
      " 'trial_id': 'default',\n",
      " 'warmup_time': 0.03723883628845215}\n"
     ]
    }
   ],
   "source": [
    "# Perform single `.train()` call.\n",
    "result = bandit_trainer.train()\n",
    "# Erase config dict from result (for better overview).\n",
    "del result[\"config\"]\n",
    "# Print out training iteration results.\n",
    "pprint(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6d6f089b-e0cc-47de-af9b-dc05a71e102f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 0 .... 500 .... 1000 .... 1500 .... 2000 .... 2500 ...."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/0g/jfs_l_113_356_c0rfp4jd8c0000gn/T/ipykernel_98498/4138643203.py:20: RuntimeWarning: Mean of empty slice\n",
      "  y = [np.nanmean(rewards[max(i - smoothing_win, 0):i + 1]) for i in range(start_at, len(rewards))]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x504 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Train for n more iterations (timesteps) and collect n-arm rewards.\n",
    "rewards = []\n",
    "for i in range(3000):\n",
    "    # Run a single timestep in the environment and update\n",
    "    # the model immediately on the received reward.\n",
    "    result = bandit_trainer.train()\n",
    "    # Extract reward from results.\n",
    "    #rewards.extend(result[\"hist_stats\"][\"episode_reward\"]\n",
    "    rewards.append(result[\"episode_reward_mean\"])\n",
    "    if i % 500 == 0:\n",
    "        print(f\" {i} \", end=\"\")\n",
    "    elif i % 100 == 0:\n",
    "        print(\".\", end=\"\")\n",
    "\n",
    "# Plot per-timestep (episode) rewards.\n",
    "plt.figure(figsize=(10,7))\n",
    "start_at = 0\n",
    "smoothing_win = 200\n",
    "x = list(range(start_at, len(rewards)))\n",
    "y = [np.nanmean(rewards[max(i - smoothing_win, 0):i + 1]) for i in range(start_at, len(rewards))]\n",
    "plt.plot(x, y)\n",
    "plt.title(\"Mean reward\")\n",
    "plt.xlabel(\"Time/Training steps\")\n",
    "\n",
    "# Add mean random baseline reward (red line).\n",
    "plt.axhline(y=lts_20_2_env_mean_random_reward, color=\"r\", linestyle=\"-\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b77c3cf-226c-4cab-82c6-7d8a54bfe9ea",
   "metadata": {},
   "source": [
    "<a id='bandit_results'></a>\n",
    "### What does our trained Bandit actually recommend?\n",
    "\n",
    "The first method of the RLlib Trainer API we used above was `train()`.\n",
    "We'll now use another method of the Trainer, `compute_single_action(input_dict={})`.\n",
    "It takes a input_dict keyword arg, into which you may pass a single (unbatched!) observation to receive an action for:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6eb62acb-0a16-4412-949a-4851201620bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action's feature value=0.8916457295417786; max-choc-feature=0.8916457295417786; \n",
      "action's feature value=0.9777311682701111; max-choc-feature=0.9777311682701111; \n",
      "action's feature value=0.9989755749702454; max-choc-feature=0.9989755749702454; \n",
      "action's feature value=0.8999373912811279; max-choc-feature=0.8999373912811279; \n",
      "action's feature value=0.9658776521682739; max-choc-feature=0.9658776521682739; \n",
      "action's feature value=0.9644351601600647; max-choc-feature=0.9644351601600647; \n",
      "action's feature value=0.9434471726417542; max-choc-feature=0.9434471726417542; \n",
      "action's feature value=0.974498987197876; max-choc-feature=0.974498987197876; \n",
      "action's feature value=0.9987043142318726; max-choc-feature=0.9987043142318726; \n",
      "action's feature value=0.9603764414787292; max-choc-feature=0.9603764414787292; \n",
      "action's feature value=0.9986302256584167; max-choc-feature=0.9986302256584167; \n",
      "action's feature value=0.9272821545600891; max-choc-feature=0.9272821545600891; \n",
      "action's feature value=0.9550436735153198; max-choc-feature=0.9550436735153198; \n",
      "action's feature value=0.9447901844978333; max-choc-feature=0.9447901844978333; \n",
      "action's feature value=0.9695131778717041; max-choc-feature=0.9695131778717041; \n",
      "action's feature value=0.965463399887085; max-choc-feature=0.965463399887085; \n",
      "action's feature value=0.8990983963012695; max-choc-feature=0.8990983963012695; \n",
      "action's feature value=0.9040490984916687; max-choc-feature=0.9040490984916687; \n",
      "action's feature value=0.9543625116348267; max-choc-feature=0.9543625116348267; \n",
      "action's feature value=0.9245824813842773; max-choc-feature=0.9245824813842773; \n",
      "action's feature value=0.9099351763725281; max-choc-feature=0.9099351763725281; \n",
      "action's feature value=0.9679779410362244; max-choc-feature=0.9679779410362244; \n",
      "action's feature value=0.9706671833992004; max-choc-feature=0.9706671833992004; \n",
      "action's feature value=0.995324432849884; max-choc-feature=0.995324432849884; \n",
      "action's feature value=0.9396806955337524; max-choc-feature=0.9396806955337524; \n",
      "action's feature value=0.8787536025047302; max-choc-feature=0.8787536025047302; \n",
      "action's feature value=0.9149923920631409; max-choc-feature=0.9149923920631409; \n",
      "action's feature value=0.9406102895736694; max-choc-feature=0.9406102895736694; \n",
      "action's feature value=0.9478465914726257; max-choc-feature=0.9478465914726257; \n",
      "action's feature value=0.9911457300186157; max-choc-feature=0.9911457300186157; \n",
      "action's feature value=0.8890330791473389; max-choc-feature=0.8890330791473389; \n",
      "action's feature value=0.9480608701705933; max-choc-feature=0.9480608701705933; \n",
      "action's feature value=0.9916664958000183; max-choc-feature=0.9916664958000183; \n",
      "action's feature value=0.9847033619880676; max-choc-feature=0.9847033619880676; \n",
      "action's feature value=0.9915906190872192; max-choc-feature=0.9915906190872192; \n",
      "action's feature value=0.9487980008125305; max-choc-feature=0.9487980008125305; \n",
      "action's feature value=0.9980818629264832; max-choc-feature=0.9980818629264832; \n",
      "action's feature value=0.9360242486000061; max-choc-feature=0.9360242486000061; \n",
      "action's feature value=0.9260164499282837; max-choc-feature=0.9260164499282837; \n",
      "action's feature value=0.9987164735794067; max-choc-feature=0.9987164735794067; \n",
      "action's feature value=0.8080435991287231; max-choc-feature=0.8080435991287231; \n",
      "action's feature value=0.9191829562187195; max-choc-feature=0.9191829562187195; \n",
      "action's feature value=0.9863934516906738; max-choc-feature=0.9863934516906738; \n",
      "action's feature value=0.9714748859405518; max-choc-feature=0.9714748859405518; \n",
      "action's feature value=0.9963867664337158; max-choc-feature=0.9963867664337158; \n",
      "action's feature value=0.9447032809257507; max-choc-feature=0.9447032809257507; \n",
      "action's feature value=0.9920045733451843; max-choc-feature=0.9920045733451843; \n",
      "action's feature value=0.9945886731147766; max-choc-feature=0.9945886731147766; \n",
      "action's feature value=0.9901645183563232; max-choc-feature=0.9901645183563232; \n",
      "action's feature value=0.9611482620239258; max-choc-feature=0.9611482620239258; \n",
      "action's feature value=0.8877958059310913; max-choc-feature=0.8877958059310913; \n",
      "action's feature value=0.980872392654419; max-choc-feature=0.980872392654419; \n",
      "action's feature value=0.9676306247711182; max-choc-feature=0.9676306247711182; \n",
      "action's feature value=0.7901422381401062; max-choc-feature=0.7901422381401062; \n",
      "action's feature value=0.9796108603477478; max-choc-feature=0.9796108603477478; \n",
      "action's feature value=0.9665442705154419; max-choc-feature=0.9665442705154419; \n",
      "action's feature value=0.9356783032417297; max-choc-feature=0.9356783032417297; \n",
      "action's feature value=0.971942126750946; max-choc-feature=0.971942126750946; \n",
      "action's feature value=0.9433143734931946; max-choc-feature=0.9433143734931946; \n",
      "action's feature value=0.9321243166923523; max-choc-feature=0.9321243166923523; \n",
      "action's feature value=0.8538448810577393; max-choc-feature=0.8538448810577393; \n",
      "action's feature value=0.9785804152488708; max-choc-feature=0.9785804152488708; \n",
      "action's feature value=0.9912826418876648; max-choc-feature=0.9912826418876648; \n",
      "action's feature value=0.8888525366783142; max-choc-feature=0.8888525366783142; \n",
      "action's feature value=0.9974634051322937; max-choc-feature=0.9974634051322937; \n",
      "action's feature value=0.9976392984390259; max-choc-feature=0.9976392984390259; \n",
      "action's feature value=0.9779126048088074; max-choc-feature=0.9779126048088074; \n",
      "action's feature value=0.9817178249359131; max-choc-feature=0.9817178249359131; \n",
      "action's feature value=0.9769214987754822; max-choc-feature=0.9769214987754822; \n",
      "action's feature value=0.9773058891296387; max-choc-feature=0.9773058891296387; \n",
      "action's feature value=0.9939188361167908; max-choc-feature=0.9939188361167908; \n",
      "action's feature value=0.9815660715103149; max-choc-feature=0.9815660715103149; \n",
      "action's feature value=0.9989805817604065; max-choc-feature=0.9989805817604065; \n",
      "action's feature value=0.9938614368438721; max-choc-feature=0.9938614368438721; \n",
      "action's feature value=0.9548345804214478; max-choc-feature=0.9548345804214478; \n",
      "action's feature value=0.9095875024795532; max-choc-feature=0.9095875024795532; \n",
      "action's feature value=0.9673813581466675; max-choc-feature=0.9673813581466675; \n",
      "action's feature value=0.931671142578125; max-choc-feature=0.931671142578125; \n",
      "action's feature value=0.9167065024375916; max-choc-feature=0.9167065024375916; \n",
      "action's feature value=0.9021305441856384; max-choc-feature=0.9021305441856384; \n",
      "action's feature value=0.999610185623169; max-choc-feature=0.999610185623169; \n",
      "action's feature value=0.8620670437812805; max-choc-feature=0.8620670437812805; \n",
      "action's feature value=0.9598987698554993; max-choc-feature=0.9598987698554993; \n",
      "action's feature value=0.9733729362487793; max-choc-feature=0.9733729362487793; \n",
      "action's feature value=0.9917956590652466; max-choc-feature=0.9917956590652466; \n",
      "action's feature value=0.919234573841095; max-choc-feature=0.919234573841095; \n",
      "action's feature value=0.9918598532676697; max-choc-feature=0.9918598532676697; \n",
      "action's feature value=0.9960801005363464; max-choc-feature=0.9960801005363464; \n",
      "action's feature value=0.9754700064659119; max-choc-feature=0.9754700064659119; \n",
      "action's feature value=0.9660402536392212; max-choc-feature=0.9660402536392212; \n",
      "action's feature value=0.996029257774353; max-choc-feature=0.996029257774353; \n",
      "action's feature value=0.9930408000946045; max-choc-feature=0.9930408000946045; \n",
      "action's feature value=0.9743787050247192; max-choc-feature=0.9743787050247192; \n",
      "action's feature value=0.9861087203025818; max-choc-feature=0.9861087203025818; \n",
      "action's feature value=0.9269380569458008; max-choc-feature=0.9269380569458008; \n",
      "action's feature value=0.9651040434837341; max-choc-feature=0.9651040434837341; \n",
      "action's feature value=0.9919588565826416; max-choc-feature=0.9919588565826416; \n",
      "action's feature value=0.946600079536438; max-choc-feature=0.946600079536438; \n",
      "action's feature value=0.9954290986061096; max-choc-feature=0.9954290986061096; \n",
      "action's feature value=0.916007399559021; max-choc-feature=0.916007399559021; \n",
      "action's feature value=0.9317758679389954; max-choc-feature=0.9317758679389954; \n",
      "action's feature value=0.9713598489761353; max-choc-feature=0.9713598489761353; \n",
      "action's feature value=0.9755606651306152; max-choc-feature=0.9755606651306152; \n",
      "action's feature value=0.986943781375885; max-choc-feature=0.986943781375885; \n",
      "action's feature value=0.9602279663085938; max-choc-feature=0.9602279663085938; \n",
      "action's feature value=0.8469273447990417; max-choc-feature=0.8469273447990417; \n",
      "action's feature value=0.9099560379981995; max-choc-feature=0.9099560379981995; \n",
      "action's feature value=0.9875852465629578; max-choc-feature=0.9875852465629578; \n",
      "action's feature value=0.9507820010185242; max-choc-feature=0.9507820010185242; \n",
      "action's feature value=0.9826569557189941; max-choc-feature=0.9826569557189941; \n",
      "action's feature value=0.9213010668754578; max-choc-feature=0.9213010668754578; \n",
      "action's feature value=0.9418420791625977; max-choc-feature=0.9418420791625977; \n",
      "action's feature value=0.9259132742881775; max-choc-feature=0.9259132742881775; \n",
      "action's feature value=0.9294229745864868; max-choc-feature=0.9294229745864868; \n",
      "action's feature value=0.9663825035095215; max-choc-feature=0.9663825035095215; \n",
      "action's feature value=0.9695129990577698; max-choc-feature=0.9695129990577698; \n",
      "action's feature value=0.9583300948143005; max-choc-feature=0.9583300948143005; \n",
      "action's feature value=0.9953047037124634; max-choc-feature=0.9953047037124634; \n",
      "action's feature value=0.9923339486122131; max-choc-feature=0.9923339486122131; \n",
      "action's feature value=0.9950851202011108; max-choc-feature=0.9950851202011108; \n"
     ]
    }
   ],
   "source": [
    "# Let's see what items our bandit recommends now that it has been trained and achieves good (>> random) rewards.\n",
    "obs = lts_20_2_env.reset()\n",
    "\n",
    "# Run a single episode.\n",
    "done = False\n",
    "while not done:\n",
    "    # Pass the single (unbatched) observation into the `compute_single_action` method of our Trainer.\n",
    "    # This is one way to perform inference on a learned policy.\n",
    "    action = bandit_trainer.compute_single_action(input_dict={\"obs\": obs})\n",
    "    feat_value_of_action = obs[\"item\"][action][0]\n",
    "    max_choc_feat = obs['item'][np.argmax(obs[\"item\"])][0]\n",
    "\n",
    "    # Print out the picked document's feature value and compare that to the highest possible feature value.\n",
    "    print(f\"action's feature value={feat_value_of_action}; max-choc-feature={max_choc_feat}; \")\n",
    "\n",
    "    # Apply the computed action in the environment and continue.\n",
    "    obs, r, done, _ = lts_20_2_env.step(action)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9355c1b-f0f7-4690-a7fe-332b01a651c4",
   "metadata": {},
   "source": [
    "### Ok, Bandits want Chocolate! :)\n",
    "#### Why is that?\n",
    "\n",
    "<img src=\"images/contextual_bandit.png\" width=1000>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84291b69-050f-489a-b822-239294bb3a2e",
   "metadata": {},
   "source": [
    "### Recap: Advantages and Disatvantages of Bandits:\n",
    "#### Advantages:\n",
    "* Very fast\n",
    "* Very sample-efficient\n",
    "* Easy to understand learning process\n",
    "\n",
    "#### Disadvantages\n",
    "* Need immediate reward (not capable of solving long-horizon credit assignment problem)\n",
    "* Models user -> If > 1 user, must train separate bandit per user\n",
    "* Not able to handle components of MultiDiscrete action space separately (works only on flattened Discrete action space)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "108a830b-cc5f-454c-b986-75c59d40df89",
   "metadata": {},
   "source": [
    "<a id='slateq'></a>\n",
    "### Switching to Slate-Q\n",
    "\n",
    "<img src=\"images/rllib_algorithms_slateq.png\" width=800>\n",
    "\n",
    "RLlib offers another algorithm - Slate-Q - designed for k-slate, long time horizon, and dynamic user recommendation problems. Let's take a quick look:\n",
    "\n",
    "<img src=\"images/slateq.png\" width=1000>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "32828a91-2da7-4f5f-9374-d12f32ec0b87",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-17 17:24:52,958\tWARNING rollout_worker.py:498 -- We've added a module for checking environments that are used in experiments. It will cause your environment to fail if your environment is not set upcorrectly. You can disable check env by setting `disable_env_checking` to True in your experiment config dictionary. You can run the environment checking module standalone by calling ray.rllib.utils.check_env(env).\n",
      "2022-04-17 17:24:52,958\tWARNING env.py:120 -- Your env doesn't have a .spec.max_episode_steps attribute. This is fine if you have set 'horizon' in your config dictionary, or `soft_horizon`. However, if you haven't, 'horizon' will default to infinity, and your environment will not be reset.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/christy/mambaforge/envs/rllib/lib/python3.9/site-packages/tensorflow/python/util/deprecation.py:620: calling map_fn_v2 (from tensorflow.python.ops.map_fn) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use fn_output_signature instead\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/christy/mambaforge/envs/rllib/lib/python3.9/site-packages/tensorflow/python/util/deprecation.py:620: calling map_fn_v2 (from tensorflow.python.ops.map_fn) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use fn_output_signature instead\n",
      "2022-04-17 17:24:53.003551: W tensorflow/core/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n",
      "/Users/christy/mambaforge/envs/rllib/lib/python3.9/site-packages/tensorflow/python/framework/indexed_slices.py:448: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"default_policy/gradients/default_policy/GatherV2_5_grad/Reshape_1:0\", shape=(None,), dtype=int64), values=Tensor(\"default_policy/gradients/default_policy/GatherV2_5_grad/Reshape:0\", shape=(None,), dtype=float32), dense_shape=Tensor(\"default_policy/gradients/default_policy/GatherV2_5_grad/Cast:0\", shape=(1,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/christy/mambaforge/envs/rllib/lib/python3.9/site-packages/tensorflow/python/training/rmsprop.py:192: calling Ones.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/christy/mambaforge/envs/rllib/lib/python3.9/site-packages/tensorflow/python/training/rmsprop.py:192: calling Ones.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SlateQTrainer"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ray.rllib.agents.slateq import SlateQTrainer, DEFAULT_CONFIG\n",
    "\n",
    "slateq_config = {\n",
    "    \"env\": \"modified_lts\",\n",
    "    \"env_config\": {\n",
    "        \"num_candidates\": 20,  # MultiDiscrete([20, 20]) -> no flattening necessary (see `convert_to_discrete_action_space=False` below)\n",
    "        \"slate_size\": 2,\n",
    "        \"resample_documents\": True,\n",
    "        \"wrap_for_bandits\": False,  # SlateQ != Bandit (will keep \"doc\" key, instead of \"items\")\n",
    "        \"convert_to_discrete_action_space\": False,  # SlateQ handles MultiDiscrete action spaces (slate recommendations).\n",
    "    },\n",
    "    # Setup exploratory behavior: Implemented as \"epsilon greedy\" strategy:\n",
    "    # Act randomly `e` percent of the time; `e` gets reduced from 1.0 to almost 0.0 over\n",
    "    # the course of `epsilon_timesteps`.\n",
    "    \"exploration_config\": {\n",
    "        #\"warmup_timesteps\": 20000,  # default\n",
    "        \n",
    "        # Use Ray Tune to run 3 parallel tuning trials\n",
    "        # \"epsilon_timesteps\": tune.grid_search([40000, 2000, 3000]),  # default: 250000\n",
    "        \n",
    "        # Do not use Ray Tune\n",
    "        \"epsilon_timesteps\": 25000\n",
    "    },\n",
    "    #\"learning_starts\": 20000,  # default\n",
    "    \"target_network_update_freq\": 3200,\n",
    "\n",
    "    # Report rewards as smoothed mean over this many episodes.\n",
    "    \"metrics_num_episodes_for_smoothing\": 200,\n",
    "}\n",
    "\n",
    "# Instantiate the Trainer object using the exact same config as in our Bandit experiment above.\n",
    "slateq_trainer = SlateQTrainer(config=slateq_config)\n",
    "slateq_trainer\n",
    "\n",
    "# # You can change timesteps_total here to see more tuning\n",
    "# tune.run(\"SlateQ\", config=slateq_config, stop={\"timesteps_total\":1000, \n",
    "#                                                \"training_iteration\":5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0845c458-d656-417e-b3d8-60596650c2bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the default configs of slateq\n",
    "# DEFAULT_CONFIG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95395f1a-31c6-4933-b09a-d06959ad5714",
   "metadata": {},
   "source": [
    "<a id='slateq_experiment'></a>\n",
    "Now that we have confirmed we have setup the Trainer correctly, let's call `train()` on it several times:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dc47c75f-4f6f-4806-995e-80ec974cfd86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1 µs, sys: 1e+03 ns, total: 2 µs\n",
      "Wall time: 3.1 µs\n",
      "Iteration=1; ts=20000: R(\"return\")=1157.368746254907\n",
      "Iteration=2; ts=21000: R(\"return\")=1157.068833599915\n",
      "Iteration=3; ts=22000: R(\"return\")=1156.9151409800165\n",
      "Iteration=4; ts=23000: R(\"return\")=1157.265269660385\n",
      "Iteration=5; ts=24000: R(\"return\")=1157.4801583872588\n",
      "Iteration=6; ts=25000: R(\"return\")=1157.7468297937626\n",
      "Iteration=7; ts=26000: R(\"return\")=1158.173960238746\n",
      "Iteration=8; ts=27000: R(\"return\")=1158.7865023220877\n",
      "Iteration=9; ts=28000: R(\"return\")=1159.092165033014\n",
      "Iteration=10; ts=29000: R(\"return\")=1159.1763366744285\n",
      "Iteration=11; ts=30000: R(\"return\")=1159.7515586640466\n",
      "Iteration=12; ts=31000: R(\"return\")=1160.5510641772535\n",
      "Iteration=13; ts=32000: R(\"return\")=1161.0059399579054\n",
      "Iteration=14; ts=33000: R(\"return\")=1161.4603824883889\n",
      "Iteration=15; ts=34000: R(\"return\")=1161.8721689874653\n",
      "Iteration=16; ts=35000: R(\"return\")=1162.849063316966\n",
      "Iteration=17; ts=36000: R(\"return\")=1163.6274527139542\n",
      "Iteration=18; ts=37000: R(\"return\")=1164.0968644903571\n",
      "Iteration=19; ts=38000: R(\"return\")=1164.5878891504012\n",
      "Iteration=20; ts=39000: R(\"return\")=1165.0953513453655\n",
      "Iteration=21; ts=40000: R(\"return\")=1166.1808798861298\n",
      "Iteration=22; ts=41000: R(\"return\")=1166.3279779834954\n",
      "Iteration=23; ts=42000: R(\"return\")=1166.9486508639359\n",
      "Iteration=24; ts=43000: R(\"return\")=1167.3388026604268\n",
      "Iteration=25; ts=44000: R(\"return\")=1167.6614514731436\n",
      "Iteration=26; ts=45000: R(\"return\")=1168.408450802785\n",
      "Iteration=27; ts=46000: R(\"return\")=1168.9366241178252\n",
      "Iteration=28; ts=47000: R(\"return\")=1169.0747592263897\n",
      "Iteration=29; ts=48000: R(\"return\")=1169.3249466146444\n",
      "Iteration=30; ts=49000: R(\"return\")=1169.499186824654\n",
      "Iteration=31; ts=50000: R(\"return\")=1169.3988966203362\n",
      "Iteration=32; ts=51000: R(\"return\")=1169.1400745767712\n",
      "Iteration=33; ts=52000: R(\"return\")=1168.8512320194416\n",
      "Iteration=34; ts=53000: R(\"return\")=1168.8148902709502\n",
      "Iteration=35; ts=54000: R(\"return\")=1169.0251778105987\n",
      "Iteration=36; ts=55000: R(\"return\")=1168.924473926535\n",
      "Iteration=37; ts=56000: R(\"return\")=1168.5699314169026\n",
      "Iteration=38; ts=57000: R(\"return\")=1168.5091620797134\n",
      "Iteration=39; ts=58000: R(\"return\")=1168.3388273635592\n",
      "Iteration=40; ts=59000: R(\"return\")=1167.8028863072407\n",
      "Iteration=41; ts=60000: R(\"return\")=1167.1791916790053\n",
      "Iteration=42; ts=61000: R(\"return\")=1167.008841843145\n",
      "Iteration=43; ts=62000: R(\"return\")=1166.9570452554829\n",
      "Iteration=44; ts=63000: R(\"return\")=1166.7546649587168\n",
      "Iteration=45; ts=64000: R(\"return\")=1166.4880396187511\n",
      "Iteration=46; ts=65000: R(\"return\")=1166.5000070441797\n",
      "Iteration=47; ts=66000: R(\"return\")=1166.1084145194368\n",
      "Iteration=48; ts=67000: R(\"return\")=1166.0069680533256\n",
      "Iteration=49; ts=68000: R(\"return\")=1165.9443647630712\n",
      "Iteration=50; ts=69000: R(\"return\")=1165.754751797165\n",
      "Iteration=51; ts=70000: R(\"return\")=1165.5410170127725\n",
      "Iteration=52; ts=71000: R(\"return\")=1165.6153361159659\n",
      "Iteration=53; ts=72000: R(\"return\")=1165.4869622261583\n",
      "Iteration=54; ts=73000: R(\"return\")=1165.6622835329717\n",
      "Iteration=55; ts=74000: R(\"return\")=1165.9878873335983\n",
      "Iteration=56; ts=75000: R(\"return\")=1166.2422068292933\n",
      "Iteration=57; ts=76000: R(\"return\")=1166.1396132194327\n",
      "Iteration=58; ts=77000: R(\"return\")=1166.2644365706367\n",
      "Iteration=59; ts=78000: R(\"return\")=1165.9212169650687\n",
      "Iteration=60; ts=79000: R(\"return\")=1165.6375736231682\n"
     ]
    }
   ],
   "source": [
    "%time\n",
    "# See reward progress with time\n",
    "# Run `train()` n times. Repeatedly call `train()` now to see rewards increase.\n",
    "for _ in range(60):\n",
    "    results = slateq_trainer.train()\n",
    "    print(f\"Iteration={slateq_trainer.iteration}; ts={results['timesteps_total']}: R(\\\"return\\\")={results['episode_reward_mean']}\")\n",
    "    \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b86aecb-90ce-4be1-91a2-5c5391ab6adf",
   "metadata": {},
   "source": [
    "------------------\n",
    "## 15 min break :)\n",
    "\n",
    "while Slate-Q is (hopefully) leaning\n",
    "\n",
    "------------------\n",
    "\n",
    "<a id='slateq_results'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "34bc1113-bd5e-45f5-bd8e-93931b8cee0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action's feature value=0.9446688890457153 max-choc-feature=0.978618323802948\n",
      "action's feature value=0.6531082987785339 max-choc-feature=0.9883738160133362\n",
      "action's feature value=0.9767611026763916 max-choc-feature=0.9767611026763916\n",
      "action's feature value=0.9292961955070496 max-choc-feature=0.9292961955070496\n",
      "action's feature value=0.9527490139007568 max-choc-feature=0.9621885418891907\n",
      "action's feature value=0.9560836553573608 max-choc-feature=0.9560836553573608\n",
      "action's feature value=0.9988470077514648 max-choc-feature=0.9988470077514648\n",
      "action's feature value=0.9280812740325928 max-choc-feature=0.9755215048789978\n",
      "action's feature value=0.9342139959335327 max-choc-feature=0.9443724155426025\n",
      "action's feature value=0.8966712951660156 max-choc-feature=0.990338921546936\n",
      "action's feature value=0.9527916312217712 max-choc-feature=0.9527916312217712\n",
      "action's feature value=0.9404319524765015 max-choc-feature=0.961936354637146\n",
      "action's feature value=0.9413776993751526 max-choc-feature=0.9774951338768005\n",
      "action's feature value=0.9608346819877625 max-choc-feature=0.9818294048309326\n",
      "action's feature value=0.774047315120697 max-choc-feature=0.9065554738044739\n",
      "action's feature value=0.7885454893112183 max-choc-feature=0.7885454893112183\n",
      "action's feature value=0.9564056992530823 max-choc-feature=0.9594333171844482\n",
      "action's feature value=0.9591665863990784 max-choc-feature=0.9591665863990784\n",
      "action's feature value=0.9088436961174011 max-choc-feature=0.9589827060699463\n",
      "action's feature value=0.9453015327453613 max-choc-feature=0.9453015327453613\n",
      "action's feature value=0.9903450012207031 max-choc-feature=0.9903450012207031\n",
      "action's feature value=0.9854914546012878 max-choc-feature=0.9920112490653992\n",
      "action's feature value=0.9679655432701111 max-choc-feature=0.9944007992744446\n",
      "action's feature value=0.96779465675354 max-choc-feature=0.96779465675354\n",
      "action's feature value=0.879234790802002 max-choc-feature=0.9241587519645691\n",
      "action's feature value=0.9488610029220581 max-choc-feature=0.9488610029220581\n",
      "action's feature value=0.8136786222457886 max-choc-feature=0.9627703428268433\n",
      "action's feature value=0.968286395072937 max-choc-feature=0.9805801510810852\n",
      "action's feature value=0.979526937007904 max-choc-feature=0.997962236404419\n",
      "action's feature value=0.9992780089378357 max-choc-feature=0.9992780089378357\n",
      "action's feature value=0.8897936344146729 max-choc-feature=0.9762256741523743\n",
      "action's feature value=0.9358320832252502 max-choc-feature=0.9834262132644653\n",
      "action's feature value=0.8224067091941833 max-choc-feature=0.857124924659729\n",
      "action's feature value=0.8427768349647522 max-choc-feature=0.95914226770401\n",
      "action's feature value=0.7814795970916748 max-choc-feature=0.8562761545181274\n",
      "action's feature value=0.987348735332489 max-choc-feature=0.987348735332489\n",
      "action's feature value=0.9663899540901184 max-choc-feature=0.9707314372062683\n",
      "action's feature value=0.9450267553329468 max-choc-feature=0.9918903112411499\n",
      "action's feature value=0.9758838415145874 max-choc-feature=0.9758838415145874\n",
      "action's feature value=0.7699671983718872 max-choc-feature=0.9543338418006897\n",
      "action's feature value=0.914863109588623 max-choc-feature=0.9323939681053162\n",
      "action's feature value=0.9195073843002319 max-choc-feature=0.9195073843002319\n",
      "action's feature value=0.923305332660675 max-choc-feature=0.9631972908973694\n",
      "action's feature value=0.8659454584121704 max-choc-feature=0.8667885661125183\n",
      "action's feature value=0.9185464382171631 max-choc-feature=0.9804856777191162\n",
      "action's feature value=0.9011621475219727 max-choc-feature=0.9031496644020081\n",
      "action's feature value=0.9738187193870544 max-choc-feature=0.9738187193870544\n",
      "action's feature value=0.9998085498809814 max-choc-feature=0.9998085498809814\n",
      "action's feature value=0.805263876914978 max-choc-feature=0.9384120106697083\n",
      "action's feature value=0.9536756873130798 max-choc-feature=0.9536756873130798\n",
      "action's feature value=0.9958152770996094 max-choc-feature=0.9958152770996094\n",
      "action's feature value=0.9692058563232422 max-choc-feature=0.9692058563232422\n",
      "action's feature value=0.9750946760177612 max-choc-feature=0.9750946760177612\n",
      "action's feature value=0.9670549035072327 max-choc-feature=0.9673377871513367\n",
      "action's feature value=0.9851086735725403 max-choc-feature=0.9851086735725403\n",
      "action's feature value=0.9979940056800842 max-choc-feature=0.9979940056800842\n",
      "action's feature value=0.8268052935600281 max-choc-feature=0.9555683732032776\n",
      "action's feature value=0.983853816986084 max-choc-feature=0.983853816986084\n",
      "action's feature value=0.8668609261512756 max-choc-feature=0.9279761910438538\n",
      "action's feature value=0.8152250647544861 max-choc-feature=0.9421847462654114\n",
      "action's feature value=0.9822477698326111 max-choc-feature=0.992667019367218\n",
      "action's feature value=0.9163403511047363 max-choc-feature=0.9717630743980408\n",
      "action's feature value=0.3006511628627777 max-choc-feature=0.9792863130569458\n",
      "action's feature value=0.9468071460723877 max-choc-feature=0.9960712790489197\n",
      "action's feature value=0.9665750861167908 max-choc-feature=0.9942330718040466\n",
      "action's feature value=0.9132835865020752 max-choc-feature=0.9824448823928833\n",
      "action's feature value=0.7811928391456604 max-choc-feature=0.9492799043655396\n",
      "action's feature value=0.767210066318512 max-choc-feature=0.9568705558776855\n",
      "action's feature value=0.9853785634040833 max-choc-feature=0.9853785634040833\n",
      "action's feature value=0.9121509790420532 max-choc-feature=0.9903685450553894\n",
      "action's feature value=0.944202721118927 max-choc-feature=0.9980227947235107\n",
      "action's feature value=0.7924988865852356 max-choc-feature=0.9376630783081055\n",
      "action's feature value=0.902131199836731 max-choc-feature=0.9834339618682861\n",
      "action's feature value=0.9832748770713806 max-choc-feature=0.9832748770713806\n",
      "action's feature value=0.9804664850234985 max-choc-feature=0.9804664850234985\n",
      "action's feature value=0.9231590032577515 max-choc-feature=0.9825738668441772\n",
      "action's feature value=0.9983548521995544 max-choc-feature=0.9983548521995544\n",
      "action's feature value=0.6886483430862427 max-choc-feature=0.7697890400886536\n",
      "action's feature value=0.8565674424171448 max-choc-feature=0.9985265731811523\n",
      "action's feature value=0.9290661811828613 max-choc-feature=0.9906516671180725\n",
      "action's feature value=0.9890884160995483 max-choc-feature=0.9890884160995483\n",
      "action's feature value=0.9596956372261047 max-choc-feature=0.9605224132537842\n",
      "action's feature value=0.9260265231132507 max-choc-feature=0.9260265231132507\n",
      "action's feature value=0.8029753565788269 max-choc-feature=0.9829264879226685\n",
      "action's feature value=0.9454309940338135 max-choc-feature=0.9454309940338135\n",
      "action's feature value=0.9270205497741699 max-choc-feature=0.9270205497741699\n",
      "action's feature value=0.9585323333740234 max-choc-feature=0.9585323333740234\n",
      "action's feature value=0.9738933444023132 max-choc-feature=0.9738933444023132\n",
      "action's feature value=0.9793245196342468 max-choc-feature=0.9793245196342468\n",
      "action's feature value=0.9890880584716797 max-choc-feature=0.9890880584716797\n",
      "action's feature value=0.9670467972755432 max-choc-feature=0.9670467972755432\n",
      "action's feature value=0.9301263689994812 max-choc-feature=0.9703752994537354\n",
      "action's feature value=0.8652114868164062 max-choc-feature=0.9550641179084778\n",
      "action's feature value=0.8626660704612732 max-choc-feature=0.9574885368347168\n",
      "action's feature value=0.7468338012695312 max-choc-feature=0.9229142665863037\n",
      "action's feature value=0.9832027554512024 max-choc-feature=0.9832027554512024\n",
      "action's feature value=0.9427794218063354 max-choc-feature=0.998198926448822\n",
      "action's feature value=0.9634699821472168 max-choc-feature=0.9642097353935242\n",
      "action's feature value=0.9004101753234863 max-choc-feature=0.9417421221733093\n",
      "action's feature value=0.9040508270263672 max-choc-feature=0.9040508270263672\n",
      "action's feature value=0.9788569808006287 max-choc-feature=0.9835554361343384\n",
      "action's feature value=0.9438275694847107 max-choc-feature=0.9747744202613831\n",
      "action's feature value=0.7133703827857971 max-choc-feature=0.8408302664756775\n",
      "action's feature value=0.834176242351532 max-choc-feature=0.9682117700576782\n",
      "action's feature value=0.9669559597969055 max-choc-feature=0.974723219871521\n",
      "action's feature value=0.9895250797271729 max-choc-feature=0.9895250797271729\n",
      "action's feature value=0.9818642139434814 max-choc-feature=0.9818642139434814\n",
      "action's feature value=0.9262658953666687 max-choc-feature=0.987434983253479\n",
      "action's feature value=0.8903785347938538 max-choc-feature=0.9674006104469299\n",
      "action's feature value=0.940950334072113 max-choc-feature=0.9522157907485962\n",
      "action's feature value=0.9103958606719971 max-choc-feature=0.9649279713630676\n",
      "action's feature value=0.9670055508613586 max-choc-feature=0.9949018955230713\n",
      "action's feature value=0.9238783121109009 max-choc-feature=0.9630938768386841\n",
      "action's feature value=0.9101313948631287 max-choc-feature=0.9801590442657471\n",
      "action's feature value=0.9809793829917908 max-choc-feature=0.9809793829917908\n",
      "action's feature value=0.9905391931533813 max-choc-feature=0.9905391931533813\n",
      "action's feature value=0.9747872352600098 max-choc-feature=0.9747872352600098\n",
      "action's feature value=0.9444741606712341 max-choc-feature=0.9961004257202148\n",
      "action's feature value=0.815617561340332 max-choc-feature=0.9473085999488831\n",
      "action's feature value=0.8309087753295898 max-choc-feature=0.9615751504898071\n"
     ]
    }
   ],
   "source": [
    "# Let's see what items our SlateQ recommends now that it has been lightly trained\n",
    "lts_20_2_env = LTSWithStrongerDissatisfactionEffect(LongTermSatisfactionRecSimEnv(config=slateq_config[\"env_config\"]))\n",
    "\n",
    "obs = lts_20_2_env.reset()\n",
    "done = False\n",
    "while not done:\n",
    "    action = slateq_trainer.compute_single_action(input_dict={\"obs\": obs})\n",
    "    feat_value_of_action = obs[\"doc\"][str(action[0])][0]\n",
    "    max_feat_action = np.argmax([value for _, value in obs[\"doc\"].items()])\n",
    "    max_choc_feat = obs['doc'][str(max_feat_action)][0]\n",
    "    # Print out the picked document's feature value and compare that to the highest possible feature value.\n",
    "    print(f\"action's feature value={feat_value_of_action} max-choc-feature={max_choc_feat}\")\n",
    "    \n",
    "    obs, r, done, _ = lts_20_2_env.step(action)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f409efcd-9c5c-4d91-a1ae-121b1b2fa698",
   "metadata": {},
   "source": [
    "#### !OPTIONAL HACK!\n",
    "\n",
    "Feel free to play around with the following code in order to learn how RLlib - under the hood - calculates actions from the environment's observations using the SlateQ Policy and its NN models inside our Trainer object):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b2b62d74-6392-453a-b25f-f8cbc90009d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our Policy right now is: SlateQTFPolicy\n",
      "Our Policy's observation space is: Dict(user:Box([0.], [1.], (1,), float32), doc:Dict(0:Box([0.], [1.], (1,), float32), 1:Box([0.], [1.], (1,), float32), 2:Box([0.], [1.], (1,), float32), 3:Box([0.], [1.], (1,), float32), 4:Box([0.], [1.], (1,), float32), 5:Box([0.], [1.], (1,), float32), 6:Box([0.], [1.], (1,), float32), 7:Box([0.], [1.], (1,), float32), 8:Box([0.], [1.], (1,), float32), 9:Box([0.], [1.], (1,), float32), 10:Box([0.], [1.], (1,), float32), 11:Box([0.], [1.], (1,), float32), 12:Box([0.], [1.], (1,), float32), 13:Box([0.], [1.], (1,), float32), 14:Box([0.], [1.], (1,), float32), 15:Box([0.], [1.], (1,), float32), 16:Box([0.], [1.], (1,), float32), 17:Box([0.], [1.], (1,), float32), 18:Box([0.], [1.], (1,), float32), 19:Box([0.], [1.], (1,), float32)), response:Tuple(Dict(click:Discrete(2), watch_time:Box(0.0, 100.0, (), float32)), Dict(click:Discrete(2), watch_time:Box(0.0, 100.0, (), float32))))\n",
      "\n",
      "Our Policy's action space is: MultiDiscrete([20 20])\n",
      "\n",
      "q_values_per_candidate=[[27.70777  27.146362 26.724005 27.040277 26.776361 27.136713 26.61579\n",
      "  27.170992 27.086895 27.451275 26.351063 26.90992  26.28308  26.491766\n",
      "  26.71741  26.658493 27.446154 26.980213 26.424349 26.727854]]\n"
     ]
    }
   ],
   "source": [
    "# To get the policy inside the Trainer, use `Trainer.get_policy([policy ID]=\"default_policy\")`:\n",
    "policy = slateq_trainer.get_policy()\n",
    "print(f\"Our Policy right now is: {policy}\")\n",
    "\n",
    "# To get to the model inside any policy, do:\n",
    "model = policy.model\n",
    "#print(f\"Our Policy's model is: {model}\")\n",
    "\n",
    "# Print out the policy's action and observation spaces.\n",
    "print(f\"Our Policy's observation space is: {policy.observation_space}\\n\")\n",
    "print(f\"Our Policy's action space is: {policy.action_space}\\n\")\n",
    "\n",
    "# Produce a random obervation (B=1; batch of size 1).\n",
    "obs = lts_20_2_env.observation_space.sample()\n",
    "\n",
    "# tf-specific code: Use tf1.Session().\n",
    "sess = policy.get_session()\n",
    "\n",
    "# Get the action logits (as torch tensor).\n",
    "with sess.graph.as_default():\n",
    "    q_values_per_candidate = model.q_value_head([\n",
    "        np.expand_dims(obs[\"user\"], 0),\n",
    "        np.expand_dims(np.concatenate([value for value in obs[\"doc\"].values()]), 0),\n",
    "    ])\n",
    "print(f\"q_values_per_candidate={sess.run(q_values_per_candidate)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ddf1390-bdea-412a-be31-ebbbc6f4ac72",
   "metadata": {},
   "source": [
    "#### !END: OPTIONAL HACK!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de603d14-f0cb-4363-a72b-8f147c094071",
   "metadata": {},
   "source": [
    "In order to release all resources from a Trainer, you can use a Trainer's `stop()` method.\n",
    "You should definitley run this cell as it frees resources that we'll need later in this tutorial, when we'll do parallel hyperparameter sweeps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "737dca4f-942f-4fda-abcc-0052263a103b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-18 08:31:58,526\tINFO services.py:1456 -- View the Ray dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8265\u001b[39m\u001b[22m\n"
     ]
    }
   ],
   "source": [
    "# In order to release resources that a Trainer uses, you can call its `stop()` method:\n",
    "slateq_trainer.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecce74b8-20ed-43c5-ad88-54a2dec32f71",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Recap: Advantages and Disadvantages of SlateQ:\n",
    "#### Advantages:\n",
    "* Decomposes MultiDiscrete action space (better understanding of items inside a k-slate)\n",
    "* Handles long-horizon credit assignment better than bandits (Q-learning)\n",
    "* Handles > 1 user problems\n",
    "* Sample efficient (due to replay buffer + off-policy DQN-style learning)\n",
    "\n",
    "#### Disadvantages\n",
    "* Uses larger (deep) model(s): One Q-value NN head per candidate\n",
    "* Slower and heavier feel to it\n",
    "* Requires careful hyperparameter-tuning, e.g. exploration timesteps."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcc82057-6b4c-4075-bd32-93c3426a1700",
   "metadata": {
    "tags": []
   },
   "source": [
    "<a id='offline_rl'></a>\n",
    "## Introduction to Offline RL\n",
    "\n",
    "<img src=\"images/offline_rl.png\" width=800>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f5114691-b74f-4b4e-8bdb-5df704bee067",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "The logdir contains the following files:\n",
      "['README.py', 'output-2022-03-28_16-43-43_worker-2_0.json']\n",
      "\n",
      "\n",
      "The JSON file with all sampled trajectories is:\n",
      "offline_rl/output-2022-03-28_16-43-43_worker-2_0.json\n"
     ]
    }
   ],
   "source": [
    "# The previous tune.run (the one we did before the break) produced \"historic data\" output.\n",
    "# We will use this output in the following as input to a newly initialized, untrained offline RL algorithm.\n",
    "\n",
    "# Let's take a look at the generated file(s) first:\n",
    "output_dir = \"offline_rl/\"\n",
    "\n",
    "# Here is what the best log directory contains:\n",
    "print(\"\\n\\nThe logdir contains the following files:\")\n",
    "all_output_files = os.listdir(os.path.dirname(output_dir + \"/\"))\n",
    "pprint(all_output_files)\n",
    "\n",
    "json_output_file = os.path.join(output_dir, [f for f in all_output_files if re.match(\"^.*worker.*\\.json$\", f)][0])\n",
    "print(\"\\n\\nThe JSON file with all sampled trajectories is:\")\n",
    "print(json_output_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75f4230f",
   "metadata": {},
   "source": [
    "### Using an (offline) input file with an offline RL algorithm.\n",
    "\n",
    "We will now pretend that we don't have a simulator for our problem (same recommender system problem as above) available, however, let's assume we possess a lot of pre-recorded, historic data from some legacy (non-RL) system.\n",
    "\n",
    "Assuming that this legacy system wrote some data into a JSON file (we'll simply use the same JSON file that our SlateQ algo produced above), how can we use this historic data to do RL either way?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "34a0aaea-b811-41e1-9e6c-532d9ce1b060",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>obs</th>\n",
       "      <th>actions</th>\n",
       "      <th>prev_actions</th>\n",
       "      <th>rewards</th>\n",
       "      <th>prev_rewards</th>\n",
       "      <th>dones</th>\n",
       "      <th>eps_id</th>\n",
       "      <th>unroll_id</th>\n",
       "      <th>agent_index</th>\n",
       "      <th>t</th>\n",
       "      <th>action_dist_inputs</th>\n",
       "      <th>action_logp</th>\n",
       "      <th>action_prob</th>\n",
       "      <th>advantages</th>\n",
       "      <th>value_targets</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SampleBatch</td>\n",
       "      <td>BCJNGGhAwFEAAAAAAACsI0kAAGGABZW1UQABAPMZjBJudW...</td>\n",
       "      <td>[[18, 2], [2, 15], [11, 14], [8, 16], [4, 18],...</td>\n",
       "      <td>[[18, 2], [18, 2], [2, 15], [11, 14], [8, 16],...</td>\n",
       "      <td>[23.29007911682129, 7.152186393737793, 2.76373...</td>\n",
       "      <td>[23.29007911682129, 23.29007911682129, 7.15218...</td>\n",
       "      <td>[False, False, False, False, False, False, Fal...</td>\n",
       "      <td>[542138261, 542138261, 542138261, 542138261, 5...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13,...</td>\n",
       "      <td>[[-0.006720410659909, 0.014437448233366, -0.00...</td>\n",
       "      <td>[-5.9771623611450195, -6.004640102386475, -5.9...</td>\n",
       "      <td>[0.0025360123254350004, 0.0024672772269690004,...</td>\n",
       "      <td>[932.8204345703125, 918.7171630859375, 920.772...</td>\n",
       "      <td>[932.8135986328125, 918.7106323242188, 920.766...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SampleBatch</td>\n",
       "      <td>BCJNGGhAwFEAAAAAAACs/0cAAGGABZW1UQABAPMZjBJudW...</td>\n",
       "      <td>[[13, 3], [16, 17], [7, 17], [8, 7], [4, 5], [...</td>\n",
       "      <td>[[14, 1], [13, 3], [16, 17], [7, 17], [8, 7], ...</td>\n",
       "      <td>[5.713678359985352, 18.23175048828125, 6.07325...</td>\n",
       "      <td>[21.042491912841797, 5.713678359985352, 18.231...</td>\n",
       "      <td>[False, False, False, False, False, False, Fal...</td>\n",
       "      <td>[1118934910, 1118934910, 1118934910, 111893491...</td>\n",
       "      <td>[4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 3...</td>\n",
       "      <td>[[-0.003344316966831, -0.002333797048777, 0.00...</td>\n",
       "      <td>[-6.002190589904785, -5.994460582733154, -5.98...</td>\n",
       "      <td>[0.002473328262567, 0.0024925211910150004, 0.0...</td>\n",
       "      <td>[515.4043579101562, 514.835205078125, 501.6228...</td>\n",
       "      <td>[515.3938598632812, 514.8284301757812, 501.612...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SampleBatch</td>\n",
       "      <td>BCJNGGhAwFEAAAAAAACsX0kAAGGABZW1UQABAPMZjBJudW...</td>\n",
       "      <td>[[3, 17], [6, 3], [15, 17], [11, 2], [0, 12], ...</td>\n",
       "      <td>[[10, 1], [3, 17], [6, 3], [15, 17], [11, 2], ...</td>\n",
       "      <td>[14.05547046661377, 20.706499099731445, 34.158...</td>\n",
       "      <td>[4.046912670135498, 14.05547046661377, 20.7064...</td>\n",
       "      <td>[False, False, False, False, False, False, Fal...</td>\n",
       "      <td>[1466202559, 1466202559, 1466202559, 146620255...</td>\n",
       "      <td>[8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 5...</td>\n",
       "      <td>[[-0.003262223675847, 0.008641279302537, -0.00...</td>\n",
       "      <td>[-5.993566513061523, -5.996161460876465, -5.98...</td>\n",
       "      <td>[0.002494750544428, 0.0024882853031150003, 0.0...</td>\n",
       "      <td>[221.78857421875, 209.83851623535156, 191.0375...</td>\n",
       "      <td>[221.7843475341797, 209.8271484375, 191.030960...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>SampleBatch</td>\n",
       "      <td>BCJNGGhAwFEAAAAAAACsCkkAAGGABZW1UQABAPMZjBJudW...</td>\n",
       "      <td>[[10, 19], [8, 3], [18, 8], [8, 7], [16, 2], [...</td>\n",
       "      <td>[[10, 19], [10, 19], [8, 3], [18, 8], [8, 7], ...</td>\n",
       "      <td>[34.26883316040039, 21.044775009155273, 8.3825...</td>\n",
       "      <td>[34.26883316040039, 34.26883316040039, 21.0447...</td>\n",
       "      <td>[False, False, False, False, False, False, Fal...</td>\n",
       "      <td>[1418911249, 1418911249, 1418911249, 141891124...</td>\n",
       "      <td>[12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 1...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13,...</td>\n",
       "      <td>[[-0.008724463172256001, 0.0040771835483610006...</td>\n",
       "      <td>[-5.995693683624268, -5.9909610748291025, -5.9...</td>\n",
       "      <td>[0.002489449456334, 0.002501259092241, 0.00250...</td>\n",
       "      <td>[658.6182861328125, 630.65478515625, 615.76763...</td>\n",
       "      <td>[658.6080322265625, 630.6456298828125, 615.758...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>SampleBatch</td>\n",
       "      <td>BCJNGGhAwFEAAAAAAACsjkkAAGGABZW1UQABAPMZjBJudW...</td>\n",
       "      <td>[[1, 8], [2, 19], [9, 1], [1, 16], [5, 2], [15...</td>\n",
       "      <td>[[6, 5], [1, 8], [2, 19], [9, 1], [1, 16], [5,...</td>\n",
       "      <td>[3.041858196258545, 8.18496322631836, 62.71932...</td>\n",
       "      <td>[3.607390880584717, 3.041858196258545, 8.18496...</td>\n",
       "      <td>[False, False, False, False, False, False, Fal...</td>\n",
       "      <td>[450621694, 450621694, 450621694, 450621694, 4...</td>\n",
       "      <td>[16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 1...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 3...</td>\n",
       "      <td>[[-0.002998510375618, -0.00399568863213, -0.00...</td>\n",
       "      <td>[-5.999508380889893, -5.996327877044678, -5.98...</td>\n",
       "      <td>[0.0024799711536610002, 0.0024878710974000004,...</td>\n",
       "      <td>[455.0074157714844, 456.52606201171875, 452.87...</td>\n",
       "      <td>[454.9999084472656, 456.5232849121094, 452.866...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          type                                                obs  \\\n",
       "0  SampleBatch  BCJNGGhAwFEAAAAAAACsI0kAAGGABZW1UQABAPMZjBJudW...   \n",
       "1  SampleBatch  BCJNGGhAwFEAAAAAAACs/0cAAGGABZW1UQABAPMZjBJudW...   \n",
       "2  SampleBatch  BCJNGGhAwFEAAAAAAACsX0kAAGGABZW1UQABAPMZjBJudW...   \n",
       "3  SampleBatch  BCJNGGhAwFEAAAAAAACsCkkAAGGABZW1UQABAPMZjBJudW...   \n",
       "4  SampleBatch  BCJNGGhAwFEAAAAAAACsjkkAAGGABZW1UQABAPMZjBJudW...   \n",
       "\n",
       "                                             actions  \\\n",
       "0  [[18, 2], [2, 15], [11, 14], [8, 16], [4, 18],...   \n",
       "1  [[13, 3], [16, 17], [7, 17], [8, 7], [4, 5], [...   \n",
       "2  [[3, 17], [6, 3], [15, 17], [11, 2], [0, 12], ...   \n",
       "3  [[10, 19], [8, 3], [18, 8], [8, 7], [16, 2], [...   \n",
       "4  [[1, 8], [2, 19], [9, 1], [1, 16], [5, 2], [15...   \n",
       "\n",
       "                                        prev_actions  \\\n",
       "0  [[18, 2], [18, 2], [2, 15], [11, 14], [8, 16],...   \n",
       "1  [[14, 1], [13, 3], [16, 17], [7, 17], [8, 7], ...   \n",
       "2  [[10, 1], [3, 17], [6, 3], [15, 17], [11, 2], ...   \n",
       "3  [[10, 19], [10, 19], [8, 3], [18, 8], [8, 7], ...   \n",
       "4  [[6, 5], [1, 8], [2, 19], [9, 1], [1, 16], [5,...   \n",
       "\n",
       "                                             rewards  \\\n",
       "0  [23.29007911682129, 7.152186393737793, 2.76373...   \n",
       "1  [5.713678359985352, 18.23175048828125, 6.07325...   \n",
       "2  [14.05547046661377, 20.706499099731445, 34.158...   \n",
       "3  [34.26883316040039, 21.044775009155273, 8.3825...   \n",
       "4  [3.041858196258545, 8.18496322631836, 62.71932...   \n",
       "\n",
       "                                        prev_rewards  \\\n",
       "0  [23.29007911682129, 23.29007911682129, 7.15218...   \n",
       "1  [21.042491912841797, 5.713678359985352, 18.231...   \n",
       "2  [4.046912670135498, 14.05547046661377, 20.7064...   \n",
       "3  [34.26883316040039, 34.26883316040039, 21.0447...   \n",
       "4  [3.607390880584717, 3.041858196258545, 8.18496...   \n",
       "\n",
       "                                               dones  \\\n",
       "0  [False, False, False, False, False, False, Fal...   \n",
       "1  [False, False, False, False, False, False, Fal...   \n",
       "2  [False, False, False, False, False, False, Fal...   \n",
       "3  [False, False, False, False, False, False, Fal...   \n",
       "4  [False, False, False, False, False, False, Fal...   \n",
       "\n",
       "                                              eps_id  \\\n",
       "0  [542138261, 542138261, 542138261, 542138261, 5...   \n",
       "1  [1118934910, 1118934910, 1118934910, 111893491...   \n",
       "2  [1466202559, 1466202559, 1466202559, 146620255...   \n",
       "3  [1418911249, 1418911249, 1418911249, 141891124...   \n",
       "4  [450621694, 450621694, 450621694, 450621694, 4...   \n",
       "\n",
       "                                           unroll_id  \\\n",
       "0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "1  [4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, ...   \n",
       "2  [8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, ...   \n",
       "3  [12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 1...   \n",
       "4  [16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 1...   \n",
       "\n",
       "                                         agent_index  \\\n",
       "0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "2  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "3  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "4  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "\n",
       "                                                   t  \\\n",
       "0  [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13,...   \n",
       "1  [20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 3...   \n",
       "2  [40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 5...   \n",
       "3  [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13,...   \n",
       "4  [20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 3...   \n",
       "\n",
       "                                  action_dist_inputs  \\\n",
       "0  [[-0.006720410659909, 0.014437448233366, -0.00...   \n",
       "1  [[-0.003344316966831, -0.002333797048777, 0.00...   \n",
       "2  [[-0.003262223675847, 0.008641279302537, -0.00...   \n",
       "3  [[-0.008724463172256001, 0.0040771835483610006...   \n",
       "4  [[-0.002998510375618, -0.00399568863213, -0.00...   \n",
       "\n",
       "                                         action_logp  \\\n",
       "0  [-5.9771623611450195, -6.004640102386475, -5.9...   \n",
       "1  [-6.002190589904785, -5.994460582733154, -5.98...   \n",
       "2  [-5.993566513061523, -5.996161460876465, -5.98...   \n",
       "3  [-5.995693683624268, -5.9909610748291025, -5.9...   \n",
       "4  [-5.999508380889893, -5.996327877044678, -5.98...   \n",
       "\n",
       "                                         action_prob  \\\n",
       "0  [0.0025360123254350004, 0.0024672772269690004,...   \n",
       "1  [0.002473328262567, 0.0024925211910150004, 0.0...   \n",
       "2  [0.002494750544428, 0.0024882853031150003, 0.0...   \n",
       "3  [0.002489449456334, 0.002501259092241, 0.00250...   \n",
       "4  [0.0024799711536610002, 0.0024878710974000004,...   \n",
       "\n",
       "                                          advantages  \\\n",
       "0  [932.8204345703125, 918.7171630859375, 920.772...   \n",
       "1  [515.4043579101562, 514.835205078125, 501.6228...   \n",
       "2  [221.78857421875, 209.83851623535156, 191.0375...   \n",
       "3  [658.6182861328125, 630.65478515625, 615.76763...   \n",
       "4  [455.0074157714844, 456.52606201171875, 452.87...   \n",
       "\n",
       "                                       value_targets  \n",
       "0  [932.8135986328125, 918.7106323242188, 920.766...  \n",
       "1  [515.3938598632812, 514.8284301757812, 501.612...  \n",
       "2  [221.7843475341797, 209.8271484375, 191.030960...  \n",
       "3  [658.6080322265625, 630.6456298828125, 615.758...  \n",
       "4  [454.9999084472656, 456.5232849121094, 452.866...  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's take a look at the output file first:\n",
    "dataframe = pandas.read_json(json_output_file, lines=True)  # don't forget lines=True -> Each line in the json is one \"rollout\" of 4 timesteps.\n",
    "dataframe.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48768f8f-25ad-4fee-92d2-5f99f34295f9",
   "metadata": {},
   "source": [
    "<a id='bc_and_marwil'></a>\n",
    "### Picking an offline RL algorithm\n",
    "\n",
    "RLlib offers different offline specialized algorithms, such as Behavior Cloning (imitation learning), MARWIL, or CQL.\n",
    "\n",
    "<img src=\"images/rllib_algorithms_offline_rl.png\" width=800>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "01e7b767-d11e-4b2d-bf86-bf88f06f8146",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BCTrainer"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's configure a new RLlib Trainer, one that's capable of reading the JSON input described\n",
    "# above and able to learn from this input.\n",
    "\n",
    "# For simplicity, we'll start with a behavioral cloning (BC) trainer:\n",
    "from ray.rllib.agents.marwil import BCTrainer\n",
    "\n",
    "offline_rl_env = LongTermSatisfactionRecSimEnv({\n",
    "    \"num_candidates\": 20,\n",
    "    \"slate_size\": 2,\n",
    "    \"wrap_for_bandits\": False,  # SlateQ != Bandit\n",
    "    \"convert_to_discrete_action_space\": False,\n",
    "})\n",
    "\n",
    "\n",
    "# Configuring the BCTrainer:\n",
    "offline_rl_config = {\n",
    "    # Specify your offline RL algo's historic (JSON) inputs:\n",
    "    \"input\": [json_output_file],\n",
    "    \"actions_in_input_normalized\": True,\n",
    "    # Note: For non-offline RL algos, this is set to \"sampler\" by default.\n",
    "    #\"input\": \"sampler\",\n",
    "\n",
    "    # Since we don't have an environment and the obs/action-spaces are not defined in the JSON file,\n",
    "    # we need to provide these here manually.\n",
    "    \"env\": None,  # default\n",
    "    \"observation_space\": offline_rl_env.observation_space,\n",
    "    \"action_space\": offline_rl_env.action_space,\n",
    "\n",
    "    # Perform \"off-policy estimation\" (OPE) on train batches and report results.\n",
    "    \"input_evaluation\": [\"is\", \"wis\"],\n",
    "}\n",
    "\n",
    "# Create a behavior cloning (BC) Trainer from our config.\n",
    "bc_trainer = BCTrainer(config=offline_rl_config)\n",
    "bc_trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d181dd19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000 steps trained; reward = nan\n",
      "4000 steps trained; reward = nan\n",
      "6000 steps trained; reward = nan\n",
      "8000 steps trained; reward = nan\n",
      "10000 steps trained; reward = nan\n",
      "12000 steps trained; reward = nan\n",
      "14000 steps trained; reward = nan\n",
      "16000 steps trained; reward = nan\n",
      "18000 steps trained; reward = nan\n",
      "20000 steps trained; reward = nan\n"
     ]
    }
   ],
   "source": [
    "# Let's train our new behavioral cloning Trainer for some iterations:\n",
    "for _ in range(10):\n",
    "    results = bc_trainer.train()\n",
    "    print(f\"{results['info']['num_agent_steps_trained']} steps trained; reward = {results['episode_reward_mean']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3e48b13b-5b2d-4e75-9b50-c1a6b45c253b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22000 steps trained; loss = 5.98137903213501\n",
      "24000 steps trained; loss = 5.9829840660095215\n",
      "26000 steps trained; loss = 5.986771583557129\n",
      "28000 steps trained; loss = 5.985103607177734\n",
      "30000 steps trained; loss = 5.987226486206055\n",
      "32000 steps trained; loss = 5.990427017211914\n",
      "34000 steps trained; loss = 5.987091541290283\n",
      "36000 steps trained; loss = 5.982422828674316\n",
      "38000 steps trained; loss = 5.9872660636901855\n",
      "40000 steps trained; loss = 5.989049911499023\n"
     ]
    }
   ],
   "source": [
    "# Oh no! What happened?\n",
    "# We don't have an environment! No way to measure rewards per episode.\n",
    "\n",
    "# For behavior cloning, simply looking at the loss as a measurement of progress would be a good idea:\n",
    "for _ in range(10):\n",
    "    results = bc_trainer.train()\n",
    "    print(f\"{results['info']['num_agent_steps_trained']} steps trained; loss = {results['info']['learner']['default_policy']['learner_stats']['total_loss']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb18e474-c5d0-45be-97e3-932482bc1cbb",
   "metadata": {},
   "source": [
    "<a id='ope'></a>\n",
    "### Off Policy Estimators\n",
    "Also: `results` still holds the last output of our non-evaluation BCTrainer.\n",
    "\n",
    "Extract off-policy estimator (OPE) results for the different methods:\n",
    "* 'is'=importance sampling\n",
    "<img src=\"images/ope.png\" width=400>\n",
    "\n",
    "* 'wis'=weighted importance sampling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "db41f0d5-7388-42b3-bd49-e82789011d90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IS off-policy estimation: {'V_prev': 644.5048140222481, 'V_step_IS': 574.6360991256745, 'V_gain_est': 0.9200230457233484}\n",
      "WIS off-policy estimation: {'V_prev': 642.4363916563145, 'V_step_WIS': 590.9794543437492, 'V_gain_est': 0.9472168038255506}\n"
     ]
    }
   ],
   "source": [
    "ope_results = results['off_policy_estimator']\n",
    "print(f\"IS off-policy estimation: {ope_results['is']}\")\n",
    "print(f\"WIS off-policy estimation: {ope_results['wis']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47dd66c3-f07a-4795-84ea-6b232ba6a047",
   "metadata": {},
   "source": [
    "### Saving and restoring a trained Trainer.\n",
    "Currently, `bc_trainer` is in an already trained state.\n",
    "It holds optimized weights in its Q-value/Policy's models that allow it to act\n",
    "already somewhat smart in our environment when given an observation.\n",
    "\n",
    "However, if we closed this notebook right now, all the effort would have been for nothing.\n",
    "Let's therefore save the state of our trainer to disk for later!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "57eae1e4-3cc4-4282-9a83-bc374bdad978",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainer (at iteration 20 was saved in '/Users/christy/ray_results/BCTrainer_None_2022-04-18_08-32-00l6uftbfw/checkpoint_000020/checkpoint-20'!\n",
      "The checkpoint directory contains the following files:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['checkpoint-20', 'checkpoint-20.tune_metadata', '.is_checkpoint']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We use the `Trainer.save()` method to create a checkpoint.\n",
    "checkpoint_file = bc_trainer.save()\n",
    "print(f\"Trainer (at iteration {bc_trainer.iteration} was saved in '{checkpoint_file}'!\")\n",
    "\n",
    "# Here is what a checkpoint directory contains:\n",
    "print(\"The checkpoint directory contains the following files:\")\n",
    "os.listdir(os.path.dirname(checkpoint_file))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fc1e0ab-2c10-469a-97b1-4aadf1a1ec97",
   "metadata": {},
   "source": [
    "### Restoring and evaluating a Trainer\n",
    "In the following cell, we'll learn how to restore a saved Trainer from a checkpoint file.\n",
    "\n",
    "We'll also evaluate a completely new Trainer (should act more or less randomly) vs an already trained one (the one we just restored from the created checkpoint file)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "74ceedb9-c225-46f2-ad1d-f902c81d3256",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-18 08:32:02,561\tINFO trainable.py:534 -- Restored on 127.0.0.1 from checkpoint: /Users/christy/ray_results/BCTrainer_None_2022-04-18_08-32-00l6uftbfw/checkpoint_000020/checkpoint-20\n",
      "2022-04-18 08:32:02,561\tINFO trainable.py:543 -- Current state after restoring: {'_iteration': 20, '_timesteps_total': 40000, '_time_total': 1.4988558292388916, '_episodes_total': 0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before restoring: Trainer is at iteration=0\n",
      "After restoring: Trainer is at iteration=20\n"
     ]
    }
   ],
   "source": [
    "# Pretend, we wanted to pick up training from a previous run:\n",
    "new_bc_trainer = BCTrainer(config=offline_rl_config)\n",
    "\n",
    "# Restoring the trained state into the `new_trainer` object.\n",
    "print(f\"Before restoring: Trainer is at iteration={new_bc_trainer.iteration}\")\n",
    "new_bc_trainer.restore(checkpoint_file)\n",
    "print(f\"After restoring: Trainer is at iteration={new_bc_trainer.iteration}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4147db25-4202-433e-aadb-c4fadbdfcbea",
   "metadata": {},
   "source": [
    "<a id='ray_serve'></a>\n",
    "### Deploying a trained policy via Ray Serve\n",
    "In the following cell, we'll learn how to use Ray Serve to deploy a trained policy, e.g. in your production for inference (computing actions; no further learning):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "adadb612-9578-46de-a9e8-67724045c801",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(ServeController pid=16925)\u001b[0m 2022-04-18 08:32:03,129\tINFO checkpoint_path.py:15 -- Using RayInternalKVStore for controller checkpoint and recovery.\n",
      "\u001b[2m\u001b[36m(ServeController pid=16925)\u001b[0m 2022-04-18 08:32:03,231\tINFO http_state.py:106 -- Starting HTTP proxy with name 'SERVE_CONTROLLER_ACTOR:yfrUBV:SERVE_PROXY_ACTOR-node:127.0.0.1-0' on node 'node:127.0.0.1-0' listening on '127.0.0.1:8000'\n",
      "2022-04-18 08:32:03,774\tINFO api.py:794 -- Started Serve instance in namespace '3dabef01-98f9-4181-b574-4065c8108013'.\n",
      "\u001b[2m\u001b[36m(HTTPProxyActor pid=16928)\u001b[0m INFO:     Started server process [16928]\n",
      "2022-04-18 08:32:03,787\tINFO api.py:615 -- Updating deployment 'ServeModel'. component=serve deployment=ServeModel\n",
      "\u001b[2m\u001b[36m(ServeController pid=16925)\u001b[0m 2022-04-18 08:32:03,860\tINFO deployment_state.py:1210 -- Adding 1 replicas to deployment 'ServeModel'. component=serve deployment=ServeModel\n",
      "\u001b[2m\u001b[36m(ServeModel pid=16932)\u001b[0m /Users/christy/mambaforge/envs/rllib/lib/python3.9/site-packages/jax/_src/lib/__init__.py:33: UserWarning: JAX on Mac ARM machines is experimental and minimally tested. Please see https://github.com/google/jax/issues/5501 in the event of problems.\n",
      "\u001b[2m\u001b[36m(ServeModel pid=16932)\u001b[0m   warnings.warn(\"JAX on Mac ARM machines is experimental and minimally tested. \"\n",
      "\u001b[2m\u001b[36m(ServeModel pid=16932)\u001b[0m 2022-04-18 08:32:06,865\tINFO trainer.py:2295 -- Your framework setting is 'tf', meaning you are using static-graph mode. Set framework='tf2' to enable eager execution with tf2.x. You may also then want to set eager_tracing=True in order to reach similar execution speed as with static-graph mode.\n",
      "\u001b[2m\u001b[36m(ServeModel pid=16932)\u001b[0m 2022-04-18 08:32:06,866\tINFO trainer.py:864 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(ServeModel pid=16932)\u001b[0m 2022-04-18 08:32:06.955263: W tensorflow/core/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n",
      "\u001b[2m\u001b[36m(ServeModel pid=16932)\u001b[0m 2022-04-18 08:32:07,152\tINFO trainable.py:534 -- Restored on 127.0.0.1 from checkpoint: /Users/christy/ray_results/BCTrainer_None_2022-04-18_08-32-00l6uftbfw/checkpoint_000020/checkpoint-20\n",
      "\u001b[2m\u001b[36m(ServeModel pid=16932)\u001b[0m 2022-04-18 08:32:07,152\tINFO trainable.py:543 -- Current state after restoring: {'_iteration': 20, '_timesteps_total': 40000, '_time_total': 1.4988558292388916, '_episodes_total': 0}\n",
      "2022-04-18 08:32:07,812\tINFO api.py:630 -- Deployment 'ServeModel' is ready at `http://127.0.0.1:8000/long-term-satisfaction`. component=serve deployment=ServeModel\n"
     ]
    }
   ],
   "source": [
    "serve.start()\n",
    "\n",
    "@serve.deployment(route_prefix=\"/long-term-satisfaction\")\n",
    "class ServeModel:\n",
    "    def __init__(self, checkpoint_path) -> None:\n",
    "        self.trainer = BCTrainer(\n",
    "            config=offline_rl_config,\n",
    "        )\n",
    "        self.trainer.restore(checkpoint_path)\n",
    "\n",
    "    async def __call__(self, request: Request):\n",
    "        json_input = await request.json()\n",
    "        obs = json_input[\"observation\"]\n",
    "        # Translate obs back to np.arrays.\n",
    "        np_obs = OrderedDict(tree.map_structure(lambda s: np.array(s) if isinstance(s, list) else s, obs))\n",
    "        action = self.trainer.compute_single_action(np_obs, explore=False)\n",
    "        return {\"action\": action}\n",
    "\n",
    "\n",
    "ServeModel.deploy(checkpoint_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1082340d-51f4-42a2-853a-508413a3d20d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> Sending observation OrderedDict([('user', []), ('doc', {'0': [0.978618323802948], '1': [0.7991585731506348], '2': [0.4614793658256531], '3': [0.7805292010307312], '4': [0.11827442795038223], '5': [0.6399210095405579], '6': [0.14335328340530396], '7': [0.9446688890457153], '8': [0.5218483209609985], '9': [0.4146619439125061], '10': [0.26455560326576233], '11': [0.7742336988449097], '12': [0.4561503231525421], '13': [0.568433940410614], '14': [0.018789799883961678], '15': [0.6176354885101318], '16': [0.6120957136154175], '17': [0.6169340014457703], '18': [0.9437480568885803], '19': [0.681820273399353]}), ('response', (OrderedDict([('click', 0), ('engagement', 11.579423904418945)]), OrderedDict([('click', 0), ('engagement', 12.06338882446289)])))])\n",
      "<- got {'action': [8, 17]}\n",
      "-> Sending observation OrderedDict([('user', []), ('doc', {'0': [0.35950788855552673], '1': [0.43703195452690125], '2': [0.6976311802864075], '3': [0.0602254718542099], '4': [0.6667667031288147], '5': [0.670637845993042], '6': [0.21038256585597992], '7': [0.12892629206180573], '8': [0.31542834639549255], '9': [0.36371076107025146], '10': [0.5701967477798462], '11': [0.4386015236377716], '12': [0.9883738160133362], '13': [0.10204481333494186], '14': [0.20887675881385803], '15': [0.16130951046943665], '16': [0.6531082987785339], '17': [0.25329160690307617], '18': [0.4663107693195343], '19': [0.24442559480667114]}), ('response', ({'click': 1, 'engagement': 4.234246730804443}, {'click': 0, 'engagement': 0.0}))])\n",
      "<- got {'action': [8, 12]}\n",
      "-> Sending observation OrderedDict([('user', []), ('doc', {'0': [0.15896958112716675], '1': [0.11037514358758926], '2': [0.6563295722007751], '3': [0.13818295300006866], '4': [0.1965823620557785], '5': [0.3687251806259155], '6': [0.8209932446479797], '7': [0.09710127860307693], '8': [0.8379449248313904], '9': [0.0960984081029892], '10': [0.9764594435691833], '11': [0.4686512053012848], '12': [0.9767611026763916], '13': [0.6048455238342285], '14': [0.7392635941505432], '15': [0.03918779268860817], '16': [0.28280696272850037], '17': [0.12019655853509903], '18': [0.296140193939209], '19': [0.11872772127389908]}), ('response', ({'click': 1, 'engagement': 3.9065513610839844}, {'click': 0, 'engagement': 0.0}))])\n",
      "<- got {'action': [8, 12]}\n",
      "-> Sending observation OrderedDict([('user', []), ('doc', {'0': [0.3179831802845001], '1': [0.414262980222702], '2': [0.06414749473333359], '3': [0.6924721002578735], '4': [0.5666014552116394], '5': [0.26538950204849243], '6': [0.5232480764389038], '7': [0.09394051134586334], '8': [0.5759465098381042], '9': [0.9292961955070496], '10': [0.3185689449310303], '11': [0.6674103736877441], '12': [0.13179786503314972], '13': [0.7163271903991699], '14': [0.28940609097480774], '15': [0.18319135904312134], '16': [0.5865129232406616], '17': [0.02010754682123661], '18': [0.8289400339126587], '19': [0.004695476032793522]}), ('response', ({'click': 1, 'engagement': 7.404976844787598}, {'click': 0, 'engagement': 0.0}))])\n",
      "<- got {'action': [8, 12]}\n",
      "-> Sending observation OrderedDict([('user', []), ('doc', {'0': [0.6778165102005005], '1': [0.2700079679489136], '2': [0.7351940274238586], '3': [0.9621885418891907], '4': [0.2487531453371048], '5': [0.5761573314666748], '6': [0.5920419096946716], '7': [0.5722519159317017], '8': [0.22308163344860077], '9': [0.9527490139007568], '10': [0.4471253752708435], '11': [0.8464086651802063], '12': [0.6994792819023132], '13': [0.2974369525909424], '14': [0.8137978315353394], '15': [0.396505743265152], '16': [0.8811032176017761], '17': [0.5812729001045227], '18': [0.8817353844642639], '19': [0.6925315856933594]}), ('response', ({'click': 1, 'engagement': 7.42411994934082}, {'click': 0, 'engagement': 0.0}))])\n",
      "<- got {'action': [8, 12]}\n"
     ]
    }
   ],
   "source": [
    "# Request 5 actions of an episode from served policy.\n",
    "obs = offline_rl_env.reset()\n",
    "\n",
    "for _ in range(5):\n",
    "    # Convert numpy arrays to lists (needed for transfer).\n",
    "    obs = tree.map_structure(lambda s: s.tolist() if isinstance(s, np.ndarray) else s, obs)\n",
    "\n",
    "    print(f\"-> Sending observation {obs}\")\n",
    "    resp = requests.get(\n",
    "        \"http://localhost:8000/long-term-satisfaction\", json={\"observation\": obs}\n",
    "    )\n",
    "    response_json = resp.json()\n",
    "    print(f\"<- got {response_json}\")\n",
    "    obs, _, _, _ = offline_rl_env.step(np.array(response_json[\"action\"]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e00f8e5a-d8a8-451d-bb97-b2000dbb2f9d",
   "metadata": {},
   "source": [
    "## Time for Q&A\n",
    "\n",
    "...\n",
    "\n",
    "## Thank you for listening and participating!\n",
    "\n",
    "### Here are a couple of links that you may find useful.\n",
    "\n",
    "- The <a href=\"https://github.com/sven1977/rllib_tutorials/tree/main/production_rl_2022\">github repo of this tutorial</a>.\n",
    "- <a href=\"https://docs.ray.io/en/latest/rllib/index.html\">RLlib's documentation main page</a>.\n",
    "- <a href=\"http://discuss.ray.io\">Our discourse forum</a> to ask questions on Ray and its libraries.\n",
    "- Our <a href=\"https://forms.gle/9TSdDYUgxYs8SA9e8\">Slack channel</a> for interacting with other Ray RLlib users.\n",
    "- The <a href=\"https://github.com/ray-project/ray/blob/master/rllib/examples/\">RLlib examples scripts folder</a> with tons of examples on how to do different stuff with RLlib.\n",
    "- A <a href=\"https://medium.com/distributed-computing-with-ray/reinforcement-learning-with-rllib-in-the-unity-game-engine-1a98080a7c0d\">blog post on training with RLlib inside a Unity3D environment</a>.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "dfac2bd7-8a01-4a89-a899-c5a33b7ada4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # if you are done with Ray\n",
    "# ray.shutdown()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
