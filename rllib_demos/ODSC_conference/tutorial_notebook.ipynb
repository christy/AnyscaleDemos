{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3o3rZytugfo6",
   "metadata": {
    "id": "3o3rZytugfo6"
   },
   "source": [
    "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/christy/AnyscaleDemos/blob/main/rllib_demos/ODSC_conference/tutorial_notebook.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://github.com/christy/AnyscaleDemos/blob/main/rllib_demos/ODSC_conference/tutorial_notebook.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />View source on GitHub</a>\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f53bf9b-4c9c-46df-8a3b-87dd54c4d4ec",
   "metadata": {},
   "source": [
    "#### Google Colab First Step: Look at top of notebook and click \"Copy to Drive\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "_BD9NRNPltO8",
   "metadata": {
    "id": "_BD9NRNPltO8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ray[rllib,serve,tune]==1.12 in /Users/christy/mambaforge/envs/rllib/lib/python3.9/site-packages (1.12.0)\n",
      "Requirement already satisfied: sklearn in /Users/christy/mambaforge/envs/rllib/lib/python3.9/site-packages (0.0)\n",
      "Requirement already satisfied: tensorflow in /Users/christy/mambaforge/envs/rllib/lib/python3.9/site-packages (2.7.0)\n",
      "Requirement already satisfied: gputil in /Users/christy/mambaforge/envs/rllib/lib/python3.9/site-packages (1.4.0)\n",
      "Requirement already satisfied: recsim in /Users/christy/mambaforge/envs/rllib/lib/python3.9/site-packages (0.2.4)\n",
      "Requirement already satisfied: gym==0.21 in /Users/christy/mambaforge/envs/rllib/lib/python3.9/site-packages (0.21.0)\n",
      "Requirement already satisfied: grpcio<=1.43.0,>=1.28.1 in /Users/christy/mambaforge/envs/rllib/lib/python3.9/site-packages (from ray[rllib,serve,tune]==1.12) (1.43.0)\n",
      "Requirement already satisfied: pyyaml in /Users/christy/mambaforge/envs/rllib/lib/python3.9/site-packages (from ray[rllib,serve,tune]==1.12) (6.0)\n",
      "Requirement already satisfied: attrs in /Users/christy/mambaforge/envs/rllib/lib/python3.9/site-packages (from ray[rllib,serve,tune]==1.12) (21.4.0)\n",
      "Requirement already satisfied: filelock in /Users/christy/mambaforge/envs/rllib/lib/python3.9/site-packages (from ray[rllib,serve,tune]==1.12) (3.6.0)\n",
      "Requirement already satisfied: virtualenv in /Users/christy/mambaforge/envs/rllib/lib/python3.9/site-packages (from ray[rllib,serve,tune]==1.12) (20.14.1)\n",
      "Requirement already satisfied: jsonschema in /Users/christy/mambaforge/envs/rllib/lib/python3.9/site-packages (from ray[rllib,serve,tune]==1.12) (4.4.0)\n",
      "Requirement already satisfied: protobuf>=3.15.3 in /Users/christy/mambaforge/envs/rllib/lib/python3.9/site-packages (from ray[rllib,serve,tune]==1.12) (3.20.0)\n",
      "Requirement already satisfied: msgpack<2.0.0,>=1.0.0 in /Users/christy/mambaforge/envs/rllib/lib/python3.9/site-packages (from ray[rllib,serve,tune]==1.12) (1.0.3)\n",
      "Requirement already satisfied: requests in /Users/christy/mambaforge/envs/rllib/lib/python3.9/site-packages (from ray[rllib,serve,tune]==1.12) (2.27.1)\n",
      "Requirement already satisfied: numpy>=1.19.3 in /Users/christy/mambaforge/envs/rllib/lib/python3.9/site-packages (from ray[rllib,serve,tune]==1.12) (1.22.3)\n",
      "Requirement already satisfied: frozenlist in /Users/christy/mambaforge/envs/rllib/lib/python3.9/site-packages (from ray[rllib,serve,tune]==1.12) (1.3.0)\n",
      "Requirement already satisfied: aiosignal in /Users/christy/mambaforge/envs/rllib/lib/python3.9/site-packages (from ray[rllib,serve,tune]==1.12) (1.2.0)\n",
      "Requirement already satisfied: click>=7.0 in /Users/christy/mambaforge/envs/rllib/lib/python3.9/site-packages (from ray[rllib,serve,tune]==1.12) (8.1.2)\n",
      "Requirement already satisfied: aiorwlock in /Users/christy/mambaforge/envs/rllib/lib/python3.9/site-packages (from ray[rllib,serve,tune]==1.12) (1.3.0)\n",
      "Requirement already satisfied: prometheus-client<0.14.0,>=0.7.1 in /Users/christy/mambaforge/envs/rllib/lib/python3.9/site-packages (from ray[rllib,serve,tune]==1.12) (0.13.1)\n",
      "Requirement already satisfied: gpustat>=1.0.0b1 in /Users/christy/mambaforge/envs/rllib/lib/python3.9/site-packages (from ray[rllib,serve,tune]==1.12) (1.0.0b1)\n",
      "Requirement already satisfied: uvicorn==0.16.0 in /Users/christy/mambaforge/envs/rllib/lib/python3.9/site-packages (from ray[rllib,serve,tune]==1.12) (0.16.0)\n",
      "Requirement already satisfied: opencensus in /Users/christy/mambaforge/envs/rllib/lib/python3.9/site-packages (from ray[rllib,serve,tune]==1.12) (0.8.0)\n",
      "Requirement already satisfied: colorful in /Users/christy/mambaforge/envs/rllib/lib/python3.9/site-packages (from ray[rllib,serve,tune]==1.12) (0.5.4)\n",
      "Requirement already satisfied: starlette in /Users/christy/mambaforge/envs/rllib/lib/python3.9/site-packages (from ray[rllib,serve,tune]==1.12) (0.17.1)\n",
      "Requirement already satisfied: smart-open in /Users/christy/mambaforge/envs/rllib/lib/python3.9/site-packages (from ray[rllib,serve,tune]==1.12) (5.2.1)\n",
      "Requirement already satisfied: py-spy>=0.2.0 in /Users/christy/mambaforge/envs/rllib/lib/python3.9/site-packages (from ray[rllib,serve,tune]==1.12) (0.3.11)\n",
      "Requirement already satisfied: aiohttp-cors in /Users/christy/mambaforge/envs/rllib/lib/python3.9/site-packages (from ray[rllib,serve,tune]==1.12) (0.7.0)\n",
      "Requirement already satisfied: fastapi in /Users/christy/mambaforge/envs/rllib/lib/python3.9/site-packages (from ray[rllib,serve,tune]==1.12) (0.75.2)\n",
      "Requirement already satisfied: aiohttp>=3.7 in /Users/christy/mambaforge/envs/rllib/lib/python3.9/site-packages (from ray[rllib,serve,tune]==1.12) (3.8.1)\n",
      "Requirement already satisfied: tabulate in /Users/christy/mambaforge/envs/rllib/lib/python3.9/site-packages (from ray[rllib,serve,tune]==1.12) (0.8.9)\n",
      "Requirement already satisfied: tensorboardX>=1.9 in /Users/christy/mambaforge/envs/rllib/lib/python3.9/site-packages (from ray[rllib,serve,tune]==1.12) (2.5)\n",
      "Requirement already satisfied: pandas in /Users/christy/mambaforge/envs/rllib/lib/python3.9/site-packages (from ray[rllib,serve,tune]==1.12) (1.4.2)\n",
      "Requirement already satisfied: lz4 in /Users/christy/mambaforge/envs/rllib/lib/python3.9/site-packages (from ray[rllib,serve,tune]==1.12) (4.0.0)\n",
      "Requirement already satisfied: scikit-image in /Users/christy/mambaforge/envs/rllib/lib/python3.9/site-packages (from ray[rllib,serve,tune]==1.12) (0.19.2)\n",
      "Requirement already satisfied: scipy in /Users/christy/mambaforge/envs/rllib/lib/python3.9/site-packages (from ray[rllib,serve,tune]==1.12) (1.8.0)\n",
      "Requirement already satisfied: dm-tree in /Users/christy/mambaforge/envs/rllib/lib/python3.9/site-packages (from ray[rllib,serve,tune]==1.12) (0.1.7)\n",
      "Requirement already satisfied: matplotlib!=3.4.3 in /Users/christy/mambaforge/envs/rllib/lib/python3.9/site-packages (from ray[rllib,serve,tune]==1.12) (3.5.1)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in /Users/christy/mambaforge/envs/rllib/lib/python3.9/site-packages (from gym==0.21) (2.0.0)\n",
      "Requirement already satisfied: h11>=0.8 in /Users/christy/mambaforge/envs/rllib/lib/python3.9/site-packages (from uvicorn==0.16.0->ray[rllib,serve,tune]==1.12) (0.13.0)\n",
      "Requirement already satisfied: asgiref>=3.4.0 in /Users/christy/mambaforge/envs/rllib/lib/python3.9/site-packages (from uvicorn==0.16.0->ray[rllib,serve,tune]==1.12) (3.5.0)\n",
      "Requirement already satisfied: scikit-learn in /Users/christy/mambaforge/envs/rllib/lib/python3.9/site-packages (from sklearn) (1.0.2)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /Users/christy/mambaforge/envs/rllib/lib/python3.9/site-packages (from tensorflow) (1.1.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /Users/christy/mambaforge/envs/rllib/lib/python3.9/site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: gast<0.5.0,>=0.2.1 in /Users/christy/mambaforge/envs/rllib/lib/python3.9/site-packages (from tensorflow) (0.4.0)\n",
      "Requirement already satisfied: absl-py>=0.4.0 in /Users/christy/mambaforge/envs/rllib/lib/python3.9/site-packages (from tensorflow) (1.0.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /Users/christy/mambaforge/envs/rllib/lib/python3.9/site-packages (from tensorflow) (4.2.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /Users/christy/mambaforge/envs/rllib/lib/python3.9/site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /Users/christy/mambaforge/envs/rllib/lib/python3.9/site-packages (from tensorflow) (1.14.0)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /Users/christy/mambaforge/envs/rllib/lib/python3.9/site-packages (from tensorflow) (3.3.0)\n",
      "Requirement already satisfied: flatbuffers<3.0,>=1.12 in /Users/christy/mambaforge/envs/rllib/lib/python3.9/site-packages (from tensorflow) (2.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /Users/christy/mambaforge/envs/rllib/lib/python3.9/site-packages (from tensorflow) (1.16.0)\n",
      "Requirement already satisfied: tensorflow-estimator<2.8,~=2.7.0rc0 in /Users/christy/mambaforge/envs/rllib/lib/python3.9/site-packages (from tensorflow) (2.7.0)\n",
      "Requirement already satisfied: tensorboard~=2.6 in /Users/christy/mambaforge/envs/rllib/lib/python3.9/site-packages (from tensorflow) (2.6.0)\n",
      "Requirement already satisfied: keras-preprocessing>=1.1.1 in /Users/christy/mambaforge/envs/rllib/lib/python3.9/site-packages (from tensorflow) (1.1.2)\n",
      "Requirement already satisfied: keras<2.8,>=2.7.0rc0 in /Users/christy/mambaforge/envs/rllib/lib/python3.9/site-packages (from tensorflow) (2.7.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in /Users/christy/mambaforge/envs/rllib/lib/python3.9/site-packages (from tensorflow) (3.6.0)\n",
      "Requirement already satisfied: dopamine-rl>=2.0.5 in /Users/christy/mambaforge/envs/rllib/lib/python3.9/site-packages (from recsim) (4.0.2)\n",
      "Requirement already satisfied: gin-config in /Users/christy/mambaforge/envs/rllib/lib/python3.9/site-packages (from recsim) (0.5.0)\n",
      "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /Users/christy/mambaforge/envs/rllib/lib/python3.9/site-packages (from aiohttp>=3.7->ray[rllib,serve,tune]==1.12) (2.0.12)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /Users/christy/mambaforge/envs/rllib/lib/python3.9/site-packages (from aiohttp>=3.7->ray[rllib,serve,tune]==1.12) (1.7.2)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /Users/christy/mambaforge/envs/rllib/lib/python3.9/site-packages (from aiohttp>=3.7->ray[rllib,serve,tune]==1.12) (4.0.2)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/christy/mambaforge/envs/rllib/lib/python3.9/site-packages (from aiohttp>=3.7->ray[rllib,serve,tune]==1.12) (6.0.2)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /Users/christy/mambaforge/envs/rllib/lib/python3.9/site-packages (from astunparse>=1.6.0->tensorflow) (0.37.1)\n",
      "Requirement already satisfied: pygame>=1.9.2 in /Users/christy/mambaforge/envs/rllib/lib/python3.9/site-packages (from dopamine-rl>=2.0.5->recsim) (2.1.2)\n",
      "Requirement already satisfied: flax>=0.2.0 in /Users/christy/mambaforge/envs/rllib/lib/python3.9/site-packages (from dopamine-rl>=2.0.5->recsim) (0.4.1)\n",
      "Requirement already satisfied: opencv-python>=3.4.8.29 in /Users/christy/mambaforge/envs/rllib/lib/python3.9/site-packages (from dopamine-rl>=2.0.5->recsim) (4.5.5.64)\n",
      "Requirement already satisfied: jax>=0.1.72 in /Users/christy/mambaforge/envs/rllib/lib/python3.9/site-packages (from dopamine-rl>=2.0.5->recsim) (0.3.7)\n",
      "Requirement already satisfied: tf-slim>=1.0 in /Users/christy/mambaforge/envs/rllib/lib/python3.9/site-packages (from dopamine-rl>=2.0.5->recsim) (1.1.0)\n",
      "Requirement already satisfied: Pillow>=7.0.0 in /Users/christy/mambaforge/envs/rllib/lib/python3.9/site-packages (from dopamine-rl>=2.0.5->recsim) (9.1.0)\n",
      "Requirement already satisfied: jaxlib>=0.1.51 in /Users/christy/mambaforge/envs/rllib/lib/python3.9/site-packages (from dopamine-rl>=2.0.5->recsim) (0.3.7)\n",
      "Requirement already satisfied: tensorflow-probability>=0.13.0 in /Users/christy/mambaforge/envs/rllib/lib/python3.9/site-packages (from dopamine-rl>=2.0.5->recsim) (0.16.0)\n",
      "Requirement already satisfied: nvidia-ml-py3>=7.352.0 in /Users/christy/mambaforge/envs/rllib/lib/python3.9/site-packages (from gpustat>=1.0.0b1->ray[rllib,serve,tune]==1.12) (7.352.0)\n",
      "Requirement already satisfied: psutil in /Users/christy/mambaforge/envs/rllib/lib/python3.9/site-packages (from gpustat>=1.0.0b1->ray[rllib,serve,tune]==1.12) (5.9.0)\n",
      "Requirement already satisfied: blessed>=1.17.1 in /Users/christy/mambaforge/envs/rllib/lib/python3.9/site-packages (from gpustat>=1.0.0b1->ray[rllib,serve,tune]==1.12) (1.19.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Users/christy/mambaforge/envs/rllib/lib/python3.9/site-packages (from matplotlib!=3.4.3->ray[rllib,serve,tune]==1.12) (4.32.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /Users/christy/mambaforge/envs/rllib/lib/python3.9/site-packages (from matplotlib!=3.4.3->ray[rllib,serve,tune]==1.12) (2.8.2)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /Users/christy/mambaforge/envs/rllib/lib/python3.9/site-packages (from matplotlib!=3.4.3->ray[rllib,serve,tune]==1.12) (1.4.2)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /Users/christy/mambaforge/envs/rllib/lib/python3.9/site-packages (from matplotlib!=3.4.3->ray[rllib,serve,tune]==1.12) (3.0.8)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/christy/mambaforge/envs/rllib/lib/python3.9/site-packages (from matplotlib!=3.4.3->ray[rllib,serve,tune]==1.12) (0.11.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/christy/mambaforge/envs/rllib/lib/python3.9/site-packages (from matplotlib!=3.4.3->ray[rllib,serve,tune]==1.12) (21.3)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/christy/mambaforge/envs/rllib/lib/python3.9/site-packages (from pandas->ray[rllib,serve,tune]==1.12) (2022.1)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /Users/christy/mambaforge/envs/rllib/lib/python3.9/site-packages (from tensorboard~=2.6->tensorflow) (62.1.0)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /Users/christy/mambaforge/envs/rllib/lib/python3.9/site-packages (from tensorboard~=2.6->tensorflow) (1.8.1)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /Users/christy/mambaforge/envs/rllib/lib/python3.9/site-packages (from tensorboard~=2.6->tensorflow) (2.1.1)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /Users/christy/mambaforge/envs/rllib/lib/python3.9/site-packages (from tensorboard~=2.6->tensorflow) (0.4.6)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /Users/christy/mambaforge/envs/rllib/lib/python3.9/site-packages (from tensorboard~=2.6->tensorflow) (3.3.6)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /Users/christy/mambaforge/envs/rllib/lib/python3.9/site-packages (from tensorboard~=2.6->tensorflow) (0.6.0)\n",
      "Requirement already satisfied: google-auth<2,>=1.6.3 in /Users/christy/mambaforge/envs/rllib/lib/python3.9/site-packages (from tensorboard~=2.6->tensorflow) (1.35.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/christy/mambaforge/envs/rllib/lib/python3.9/site-packages (from requests->ray[rllib,serve,tune]==1.12) (1.26.9)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/christy/mambaforge/envs/rllib/lib/python3.9/site-packages (from requests->ray[rllib,serve,tune]==1.12) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/christy/mambaforge/envs/rllib/lib/python3.9/site-packages (from requests->ray[rllib,serve,tune]==1.12) (2021.10.8)\n",
      "Requirement already satisfied: pydantic!=1.7,!=1.7.1,!=1.7.2,!=1.7.3,!=1.8,!=1.8.1,<2.0.0,>=1.6.2 in /Users/christy/mambaforge/envs/rllib/lib/python3.9/site-packages (from fastapi->ray[rllib,serve,tune]==1.12) (1.9.0)\n",
      "Requirement already satisfied: anyio<4,>=3.0.0 in /Users/christy/mambaforge/envs/rllib/lib/python3.9/site-packages (from starlette->ray[rllib,serve,tune]==1.12) (3.5.0)\n",
      "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /Users/christy/mambaforge/envs/rllib/lib/python3.9/site-packages (from jsonschema->ray[rllib,serve,tune]==1.12) (0.18.1)\n",
      "Requirement already satisfied: opencensus-context==0.1.2 in /Users/christy/mambaforge/envs/rllib/lib/python3.9/site-packages (from opencensus->ray[rllib,serve,tune]==1.12) (0.1.2)\n",
      "Requirement already satisfied: google-api-core<3.0.0,>=1.0.0 in /Users/christy/mambaforge/envs/rllib/lib/python3.9/site-packages (from opencensus->ray[rllib,serve,tune]==1.12) (2.7.2)\n",
      "Requirement already satisfied: imageio>=2.4.1 in /Users/christy/mambaforge/envs/rllib/lib/python3.9/site-packages (from scikit-image->ray[rllib,serve,tune]==1.12) (2.16.2)\n",
      "Requirement already satisfied: PyWavelets>=1.1.1 in /Users/christy/mambaforge/envs/rllib/lib/python3.9/site-packages (from scikit-image->ray[rllib,serve,tune]==1.12) (1.3.0)\n",
      "Requirement already satisfied: tifffile>=2019.7.26 in /Users/christy/mambaforge/envs/rllib/lib/python3.9/site-packages (from scikit-image->ray[rllib,serve,tune]==1.12) (2022.4.8)\n",
      "Requirement already satisfied: networkx>=2.2 in /Users/christy/mambaforge/envs/rllib/lib/python3.9/site-packages (from scikit-image->ray[rllib,serve,tune]==1.12) (2.8)\n",
      "Requirement already satisfied: joblib>=0.11 in /Users/christy/mambaforge/envs/rllib/lib/python3.9/site-packages (from scikit-learn->sklearn) (1.1.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Users/christy/mambaforge/envs/rllib/lib/python3.9/site-packages (from scikit-learn->sklearn) (3.1.0)\n",
      "Requirement already satisfied: distlib<1,>=0.3.1 in /Users/christy/mambaforge/envs/rllib/lib/python3.9/site-packages (from virtualenv->ray[rllib,serve,tune]==1.12) (0.3.4)\n",
      "Requirement already satisfied: platformdirs<3,>=2 in /Users/christy/mambaforge/envs/rllib/lib/python3.9/site-packages (from virtualenv->ray[rllib,serve,tune]==1.12) (2.5.1)\n",
      "Requirement already satisfied: sniffio>=1.1 in /Users/christy/mambaforge/envs/rllib/lib/python3.9/site-packages (from anyio<4,>=3.0.0->starlette->ray[rllib,serve,tune]==1.12) (1.2.0)\n",
      "Requirement already satisfied: wcwidth>=0.1.4 in /Users/christy/mambaforge/envs/rllib/lib/python3.9/site-packages (from blessed>=1.17.1->gpustat>=1.0.0b1->ray[rllib,serve,tune]==1.12) (0.2.5)\n",
      "Requirement already satisfied: optax in /Users/christy/mambaforge/envs/rllib/lib/python3.9/site-packages (from flax>=0.2.0->dopamine-rl>=2.0.5->recsim) (0.1.2)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.52.0 in /Users/christy/mambaforge/envs/rllib/lib/python3.9/site-packages (from google-api-core<3.0.0,>=1.0.0->opencensus->ray[rllib,serve,tune]==1.12) (1.56.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /Users/christy/mambaforge/envs/rllib/lib/python3.9/site-packages (from google-auth<2,>=1.6.3->tensorboard~=2.6->tensorflow) (0.2.8)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /Users/christy/mambaforge/envs/rllib/lib/python3.9/site-packages (from google-auth<2,>=1.6.3->tensorboard~=2.6->tensorflow) (4.2.4)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /Users/christy/mambaforge/envs/rllib/lib/python3.9/site-packages (from google-auth<2,>=1.6.3->tensorboard~=2.6->tensorflow) (4.8)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /Users/christy/mambaforge/envs/rllib/lib/python3.9/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow) (1.3.1)\n",
      "Requirement already satisfied: ale-py~=0.7.1 in /Users/christy/mambaforge/envs/rllib/lib/python3.9/site-packages (from gym==0.21) (0.7.4)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in /Users/christy/mambaforge/envs/rllib/lib/python3.9/site-packages (from markdown>=2.6.8->tensorboard~=2.6->tensorflow) (4.11.3)\n",
      "Requirement already satisfied: decorator in /Users/christy/mambaforge/envs/rllib/lib/python3.9/site-packages (from tensorflow-probability>=0.13.0->dopamine-rl>=2.0.5->recsim) (5.1.1)\n",
      "Requirement already satisfied: importlib-resources in /Users/christy/mambaforge/envs/rllib/lib/python3.9/site-packages (from ale-py~=0.7.1->gym==0.21) (5.7.1)\n",
      "Requirement already satisfied: zipp>=0.5 in /Users/christy/mambaforge/envs/rllib/lib/python3.9/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard~=2.6->tensorflow) (3.8.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /Users/christy/mambaforge/envs/rllib/lib/python3.9/site-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard~=2.6->tensorflow) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /Users/christy/mambaforge/envs/rllib/lib/python3.9/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow) (3.2.0)\n",
      "Requirement already satisfied: chex>=0.0.4 in /Users/christy/mambaforge/envs/rllib/lib/python3.9/site-packages (from optax->flax>=0.2.0->dopamine-rl>=2.0.5->recsim) (0.1.2)\n",
      "Requirement already satisfied: toolz>=0.9.0 in /Users/christy/mambaforge/envs/rllib/lib/python3.9/site-packages (from chex>=0.0.4->optax->flax>=0.2.0->dopamine-rl>=2.0.5->recsim) (0.11.2)\n"
     ]
    }
   ],
   "source": [
    "# Run this cell only for Google Colab\n",
    "!pip install \"ray[rllib,serve,tune]==1.12\" sklearn tensorflow gputil recsim gym==0.21\n",
    "\n",
    "# Regular Jupyter notebook, you can skip all above steps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aa06051",
   "metadata": {
    "id": "6aa06051"
   },
   "source": [
    "# Reinforcement Learning for Recommender Systems\n",
    "## From Contextual Bandits to Slate-Q\n",
    "\n",
    "\n",
    "\n",
    "<table>\n",
    "<tr>\n",
    "    <td> <img src=https://drive.google.com/uc?id=1jAhSZfGDIcdlBXd6EtfxPhBvV54F16RX style=\"width: 230px;\"/> </td>\n",
    "    <td> <img src=https://drive.google.com/uc?id=1rum1twl4g0nsPJzxDplFjknZHrYbQ3XW style=\"width: 213px;\"/> </td>\n",
    "    <td> <img src=https://drive.google.com/uc?id=1t9w6Z87vd7cgtjxAa3chlGwX_fde03Oy style=\"width: 169px;\"/> </td>\n",
    "    <td> <img src=https://drive.google.com/uc?id=1tVPSOMDzIqrSK7NHEzrYc_xSdEpe6Ibg style=\"width: 254px;\"/> </td>\n",
    "    <td> <img src=https://drive.google.com/uc?id=10kw_frXjhtaLrxYNIHlSAPgAGZ_tNbv- style=\"width: 252px;\"/> </td>\n",
    "</tr>\n",
    "</table>\n",
    "\n",
    "\n",
    "### Overview\n",
    "“Reinforcement Learning for Recommender Systems, From Contextual Bandits to Slate-Q” is a tutorial for industry researchers, domain-experts, and ML-engineers, showcasing ...\n",
    "\n",
    "1) .. how you can use RLlib to build a recommender system **simulator** for your industry applications and run Bandit algorithms and the Slate-Q algorithm against this simulator.\n",
    "\n",
    "2) .. how RLlib's offline algorithms pose solutions in case you **don't have a simulator** of your problem environment at hand.\n",
    "\n",
    "We will further explore how to deploy trained models to production using Ray Serve.\n",
    "\n",
    "During the live-coding phases, we will using a recommender system simulating environment by google's RecSim and configure and run 2 RLlib algorithms against it. We'll also demonstrate how you may use offline RL as a solution for recommender systems and how to deploy a learned policy into production.\n",
    "\n",
    "RLlib offers industry-grade scalability, a large list of algos to choose from (offline, model-based, model-free, etc..), support for TensorFlow and PyTorch, and a unified API for a variety of applications. This tutorial includes a brief introduction to provide an overview of concepts (e.g. why RL?) before proceeding to RLlib (recommender system) environments, neural network models, offline RL, student exercises, Q/A, and more. All code will be provided as .py files in a GitHub repo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "green-insertion",
   "metadata": {
    "id": "green-insertion"
   },
   "source": [
    "### Intended Audience\n",
    "* Python programmers who are interested in using RL to solve their specific industry decision making problems and who want to get started with RLlib.\n",
    "\n",
    "### Prerequisites\n",
    "* Some Python programming experience.\n",
    "* Some familiarity with machine learning.\n",
    "* *Helpful, but not required:* Experience in reinforcement learning and Ray.\n",
    "* *Helpful, but not required:* Experience with TensorFlow or PyTorch.\n",
    "\n",
    "\n",
    "### Key Takeaways\n",
    "* What is reinforcement learning and RLlib?\n",
    "* How do recommender systems work? How do we build our own?\n",
    "* How do we train RLlib's different algorithms on a recommender system problem?\n",
    "* How do I deploy an already trained policy into production using Ray Serve.\n",
    "\n",
    "\n",
    "### Tutorial Outline\n",
    "\n",
    "1. Reinforcement learning (RL) in a nutshell.\n",
    "1. How to formulate any problem as an RL-solvable one?\n",
    "1. Recommender systems - How they work.\n",
    "1. Why you should use RLlib.\n",
    "\n",
    "(10min break)\n",
    "\n",
    "1. [Google RecSim - Build your own recom sys simulator.](#recsim)\n",
    "1. [Dissecting the \"long term satisfaction\" (LTE) environment.](#dissecting_lte)\n",
    "1. [Using a contextual Bandit algorithm with RLlib and starting our first training run on the LTE env.](#rllib)\n",
    "1. [What did the Bandit learn?](#bandit_results)\n",
    "1. [Intro to Slate-Q.](#slateq)\n",
    "1. [Starting a Slate-Q training run.](#slateq_experiment)\n",
    "\n",
    "(10min break)\n",
    "\n",
    "1. [Analyzing the results of the SlateQ run.](#slateq_results)\n",
    "1. [Ray Serve example: How can we deploy a trained policy into our production environment?](#ray_serve)\n",
    "\n",
    "\n",
    "### Other Recommended Readings\n",
    "* [Intro to RLlib: Example Environments](https://medium.com/distributed-computing-with-ray/intro-to-rllib-example-environments-3a113f532c70)\n",
    "* [Reinforcement Learning with RLlib in the Unity Game Engine](https://medium.com/distributed-computing-with-ray/reinforcement-learning-with-rllib-in-the-unity-game-engine-1a98080a7c0d)\n",
    "\n",
    "<td> <img src=https://drive.google.com/uc?id=1skpzZvJPWLSbGg0xj1obuQegKc0YfwZO width=400> </td>\n",
    "\n",
    "* [Attention Nets and More with RLlib's Trajectory View API](https://medium.com/distributed-computing-with-ray/attention-nets-and-more-with-rllibs-trajectory-view-api-d326339a6e65)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9f7e21f-f3de-4bad-a3a7-4bbd0b015559",
   "metadata": {
    "id": "e9f7e21f-f3de-4bad-a3a7-4bbd0b015559"
   },
   "source": [
    "# Let's start!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "930deb27-e739-4507-bc24-e39ded9caeb4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "930deb27-e739-4507-bc24-e39ded9caeb4",
    "outputId": "c5fae381-70b2-4f56-e1cf-46c6e4b65755"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.9.12\n",
      "ray: 1.12.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/christy/mambaforge/envs/rllib/lib/python3.9/site-packages/jax/_src/lib/__init__.py:33: UserWarning: JAX on Mac ARM machines is experimental and minimally tested. Please see https://github.com/google/jax/issues/5501 in the event of problems.\n",
      "  warnings.warn(\"JAX on Mac ARM machines is experimental and minimally tested. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf: 2.7.0\n"
     ]
    }
   ],
   "source": [
    "# Let's get started with some basic imports.\n",
    "\n",
    "import ray  # .. of course\n",
    "from ray import serve\n",
    "from ray import tune\n",
    "\n",
    "from collections import OrderedDict\n",
    "import gym  # RL environments and action/observation spaces\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas\n",
    "from pprint import pprint\n",
    "import re\n",
    "import recsim  # google's RecSim package.\n",
    "import requests\n",
    "from scipy.interpolate import make_interp_spline, BSpline\n",
    "from scipy.stats import linregress, sem\n",
    "from starlette.requests import Request\n",
    "import tree  # dm_tree\n",
    "\n",
    "!python --version\n",
    "print(f\"ray: {ray.__version__}\")\n",
    "import tensorflow as tf\n",
    "print(f\"tf: {tf.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90f010a6-7ee3-49b3-814a-2b9455c66c8f",
   "metadata": {
    "id": "90f010a6-7ee3-49b3-814a-2b9455c66c8f"
   },
   "source": [
    "<a id='recsim'></a>\n",
    "## Introducing google RecSim\n",
    "\n",
    "<td> <img src=https://drive.google.com/uc?id=1B9zX184GMYZ6_oLR8p16tSJnzIHofk1g width=600/> </td>\n",
    "\n",
    "<a href=\"https://github.com/google-research/recsim\">Google's RecSim package</a> offers a flexible way for you to <a href=\"https://github.com/google-research/recsim/blob/master/recsim/colab/RecSim_Developing_an_Environment.ipynb\">define the different building blocks of a recommender system</a>:\n",
    "\n",
    "\n",
    "- User model (how do users change their preferences when having been faced with, selected, and consumed certain items?).\n",
    "- Document model: Features of documents and how do documents get pre-selected/sampled.\n",
    "- Reward functions.\n",
    "\n",
    "RLlib comes with 3 off-the-shelf RecSim environments that are ready for training (with RLlib):\n",
    "* Long Term Satisfaction (<- the \"env\" we will use in this tutorial)\n",
    "* Interest Evolution\n",
    "* Interest Exploration\n",
    "\n",
    "<a id='dissecting_lte'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c3363126-0f38-4f92-a031-1ea791b9a747",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c3363126-0f38-4f92-a031-1ea791b9a747",
    "outputId": "75ecf413-fc95-4596-de9b-2add9fe3ea5b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('observation space = Dict(user:Box([], [], (0,), float32), '\n",
      " 'doc:Dict(0:Box([0.], [1.], (1,), float32), 1:Box([0.], [1.], (1,), float32), '\n",
      " '2:Box([0.], [1.], (1,), float32), 3:Box([0.], [1.], (1,), float32), '\n",
      " '4:Box([0.], [1.], (1,), float32), 5:Box([0.], [1.], (1,), float32), '\n",
      " '6:Box([0.], [1.], (1,), float32), 7:Box([0.], [1.], (1,), float32), '\n",
      " '8:Box([0.], [1.], (1,), float32), 9:Box([0.], [1.], (1,), float32)), '\n",
      " 'response:Tuple(Dict(click:Discrete(2), engagement:Box(0.0, 100.0, (), '\n",
      " 'float32))))')\n",
      "action space = Discrete(10)\n"
     ]
    }
   ],
   "source": [
    "# Import the built-in RecSim exapmle environment: \"Long Term Satisfaction\", ready to be trained by RLlib.\n",
    "from ray.rllib.examples.env.recommender_system_envs_with_recsim import LongTermSatisfactionRecSimEnv\n",
    "\n",
    "# Create a RecSim instance using the following config parameters (very similar to what we used above in our own recommender system env):\n",
    "lts_10_1_env = LongTermSatisfactionRecSimEnv({\n",
    "    \"num_candidates\": 10,  # The number of possible documents/videos/candidates that we can recommend\n",
    "    \"slate_size\": 1, # The number of recommendations that we will be making\n",
    "    # Set to False for re-using the same candidate doecuments each timestep.\n",
    "    \"resample_documents\": False,\n",
    "    # Convert MultiDiscrete actions to Discrete (flatten action space).\n",
    "    # e.g. slate_size=2 and num_candidates=10 -> MultiDiscrete([10, 10]) -> Discrete(100)  # 10x10\n",
    "    \"convert_to_discrete_action_space\": True,\n",
    "})\n",
    "\n",
    "# What are our spaces?\n",
    "pprint(f\"observation space = {lts_10_1_env.observation_space}\")\n",
    "print(f\"action space = {lts_10_1_env.action_space}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3eac27a-bc9a-47dd-b3f9-cffd3e590e3b",
   "metadata": {
    "id": "a3eac27a-bc9a-47dd-b3f9-cffd3e590e3b"
   },
   "source": [
    "Let's make use of our knowledge on the gym.Env API and call our new environment's `reset()` and `step()` methods.\n",
    "First: `reset()` to receive the initial observation in a new episode/trajectory/session:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "913d34a6-fac6-4436-b1d5-9292ebf88006",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "913d34a6-fac6-4436-b1d5-9292ebf88006",
    "outputId": "3b4635e1-599b-4a44-be17-2b9f46eb279e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('user', array([], dtype=float32)),\n",
      "             ('doc',\n",
      "              {'0': array([0.5488135], dtype=float32),\n",
      "               '1': array([0.71518934], dtype=float32),\n",
      "               '2': array([0.60276335], dtype=float32),\n",
      "               '3': array([0.5448832], dtype=float32),\n",
      "               '4': array([0.4236548], dtype=float32),\n",
      "               '5': array([0.6458941], dtype=float32),\n",
      "               '6': array([0.4375872], dtype=float32),\n",
      "               '7': array([0.891773], dtype=float32),\n",
      "               '8': array([0.96366274], dtype=float32),\n",
      "               '9': array([0.3834415], dtype=float32)}),\n",
      "             ('response',\n",
      "              (OrderedDict([('click', 0),\n",
      "                            ('engagement',\n",
      "                             array(17.464443, dtype=float32))]),))])\n"
     ]
    }
   ],
   "source": [
    "# Start a new episode and look at initial observation.\n",
    "obs = lts_10_1_env.reset()\n",
    "pprint(obs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9857197-f79f-4f9d-8e3f-68eee20daf94",
   "metadata": {
    "id": "e9857197-f79f-4f9d-8e3f-68eee20daf94"
   },
   "source": [
    "Now let's play RL agent ourselves and recommend some items (pick some actions) via the environment's `step()` method:\n",
    "\n",
    "**Task:** Execute the following cell a couple of times chosing different actions (from 0 - 9) to be sent into the environment's `step()` method. Each time, look at the returned next observation, reward, and `done` flag and write down what you find interesting about the dynamics and observations of this environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1f10b83e-993f-4c59-8e13-4e1074bfd7af",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1f10b83e-993f-4c59-8e13-4e1074bfd7af",
    "outputId": "e672004b-73d9-42ce-e22a-8e5a07cfe8cb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('user', array([], dtype=float32)),\n",
      "             ('doc',\n",
      "              {'0': array([0.5488135], dtype=float32),\n",
      "               '1': array([0.71518934], dtype=float32),\n",
      "               '2': array([0.60276335], dtype=float32),\n",
      "               '3': array([0.5448832], dtype=float32),\n",
      "               '4': array([0.4236548], dtype=float32),\n",
      "               '5': array([0.6458941], dtype=float32),\n",
      "               '6': array([0.4375872], dtype=float32),\n",
      "               '7': array([0.891773], dtype=float32),\n",
      "               '8': array([0.96366274], dtype=float32),\n",
      "               '9': array([0.3834415], dtype=float32)}),\n",
      "             ('response',\n",
      "              ({'click': 1, 'engagement': array(10.584764, dtype=float32)},))])\n",
      "reward = 10.58; done = False\n"
     ]
    }
   ],
   "source": [
    "# Let's send our first action (1-slate back into the env) using the env's `step()` method.\n",
    "action = 3  # Discrete(10): 0-9 are all valid actions\n",
    "\n",
    "# This method returns 4 items:\n",
    "# - next observation (after having applied the action)\n",
    "# - reward (after having applied the action)\n",
    "# - `done` flag; if True, the episode is terminated and the environment needs to be `reset()` again.\n",
    "# - info dict (we'll ignore this)\n",
    "next_obs, reward, done, _ = lts_10_1_env.step(action)\n",
    "\n",
    "# Print out the next observation.\n",
    "# We expect the \"doc\" and \"user\" items to be the same as in the previous observation\n",
    "# b/c we set \"resample_documents\" to False.\n",
    "pprint(next_obs)\n",
    "# Print out rewards and the vlaue of the `done` flag.\n",
    "print(f\"reward = {reward:.2f}; done = {done}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "444c7982-b372-4fe9-806a-18a29101c094",
   "metadata": {
    "id": "444c7982-b372-4fe9-806a-18a29101c094",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "\n",
    "### What have we learnt from experimenting with the environment?\n",
    "\n",
    "* User's state (if any) is hidden to agent (not part of observation).\n",
    "* Episodes seem to last at least n timesteps -> user seems to have some time budget to spend.\n",
    "* User always seems to click, no matter what we recommend.\n",
    "* Reward seems to be always identical to the \"engagement\" value (of the clicked item). These values range somewhere between 0.0 and 20.0+.\n",
    "* Weak suspicion: If we always recommend the item with the highest feature value, rewards seem to taper off over time - in most of the episodes.\n",
    "* Weak suspicion: If we always recommend the item with the lowest feature value, rewards seem to increase over time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a05210c-69ea-4c09-acf8-831fffca5f8c",
   "metadata": {
    "id": "8a05210c-69ea-4c09-acf8-831fffca5f8c"
   },
   "source": [
    "### What the environment actually does under the hood\n",
    "\n",
    "Let's take a quick look at a pre-configured RecSim environment: \"Long Term Satisfaction\".\n",
    "\n",
    "<td> <img src=https://drive.google.com/uc?id=14BHvEMPrXWGuW6gZVFMuJnEJnuvkwhUu width=1200/> </td>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5958ff84-f7d0-45c9-aa43-b807243b8452",
   "metadata": {
    "id": "5958ff84-f7d0-45c9-aa43-b807243b8452"
   },
   "source": [
    "Now that we know, that there is a double objective built into the env (a. sweetness -> engagement; b. sweetness -> unhappyness; unhappyness -> low engagement), let's make this effect a tiny bit stronger by slightly modifying the environment. As said above, the effect is very weak and almost not measurable, which is a problem on the env's side. We can use this following `gym.ObservationWrapper` class in the cell below to \"fix\" that problem:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "375a469e-aa46-4038-9df7-02cabccdad50",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "375a469e-aa46-4038-9df7-02cabccdad50",
    "outputId": "d6db808d-98d6-4fa9-c50a-9555bade705a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ok; registered the string 'modified_lts' to be used in RLlib configs (see below)\n"
     ]
    }
   ],
   "source": [
    "# Modifying wrapper around the LTS (Long Term Satisfaction) env:\n",
    "# - allows us to tweak the user model (and thus: reward behavior)\n",
    "# - adds user's current satisfaction value to observation\n",
    "\n",
    "class LTSWithStrongerDissatisfactionEffect(gym.ObservationWrapper):\n",
    "\n",
    "    def __init__(self, env):\n",
    "        # Tweak incoming environment.\n",
    "        env.environment._user_model._user_sampler._state_parameters.update({\n",
    "            \"sensitivity\": 0.058,\n",
    "            \"time_budget\": 120,\n",
    "            \"choc_stddev\": 0.1,\n",
    "            \"kale_stddev\": 0.1,\n",
    "            #\"innovation_stddev\": 0.01,\n",
    "            #\"choc_mean\": 1.25,\n",
    "            #\"kale_mean\": 1.0,\n",
    "            #\"memory_discount\": 0.9,\n",
    "        })\n",
    "\n",
    "        super().__init__(env)\n",
    "\n",
    "        # Adjust observation space.\n",
    "        if \"response\" in self.observation_space.spaces:\n",
    "            self.observation_space.spaces[\"user\"] = gym.spaces.Box(0.0, 1.0, (1, ), dtype=np.float32)\n",
    "            for r in self.observation_space[\"response\"]:\n",
    "                if \"engagement\" in r.spaces:\n",
    "                    r.spaces[\"watch_time\"] = r.spaces[\"engagement\"]\n",
    "                    del r.spaces[\"engagement\"]\n",
    "                    break\n",
    "\n",
    "    def observation(self, observation):\n",
    "        if \"response\" in self.observation_space.spaces:\n",
    "            observation[\"user\"] = np.array([self.env.environment._user_model._user_state.satisfaction])\n",
    "            for r in observation[\"response\"]:\n",
    "                if \"engagement\" in r:\n",
    "                    r[\"watch_time\"] = r[\"engagement\"]\n",
    "                    del r[\"engagement\"]\n",
    "        return observation\n",
    "\n",
    "\n",
    "# Add the wrapping around \n",
    "tune.register_env(\"modified_lts\", lambda env_config: LTSWithStrongerDissatisfactionEffect(LongTermSatisfactionRecSimEnv(env_config)))\n",
    "\n",
    "print(\"ok; registered the string 'modified_lts' to be used in RLlib configs (see below)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbce752b-68a3-4a84-aada-97810039e4e8",
   "metadata": {
    "id": "dbce752b-68a3-4a84-aada-97810039e4e8"
   },
   "source": [
    "Now that we have a stronger effect of the user's satisfaction value on the long-term rewards, we may be able to measure this effect reliably\n",
    "using the following utility code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3b1f1ab8-6b08-47c7-9dfa-3fe9bd672c14",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3b1f1ab8-6b08-47c7-9dfa-3fe9bd672c14",
    "outputId": "7540fffe-4ff8-4995-8714-b577c499fcb5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0006302554520280178\n"
     ]
    }
   ],
   "source": [
    "# This cell should help you with your own analysis of the two above \"suspicions\":\n",
    "# Always chosing the highest/lowest-valued action will lead to a decrease/increase in rewards over the course of an episode.\n",
    "modified_lts_10_1_env = LTSWithStrongerDissatisfactionEffect(lts_10_1_env)\n",
    "\n",
    "# Capture slopes of all trendlines over all episodes.\n",
    "slopes = []\n",
    "# Run 1000 episodes.\n",
    "for _ in range(1000):\n",
    "    obs = modified_lts_10_1_env.reset()  # Reset environment to get initial observation:\n",
    "\n",
    "    # Compute actions that pick doc with highest/lowest feature value.\n",
    "    action_sweetest = np.argmax([value for _, value in obs[\"doc\"].items()])\n",
    "    action_kaleiest = np.argmin([value for _, value in obs[\"doc\"].items()])\n",
    "\n",
    "    # Play one episode.\n",
    "    done = False\n",
    "    rewards = []\n",
    "    while not done:\n",
    "        #action = action_sweetest\n",
    "        action = action_kaleiest\n",
    "        #action = np.random.choice([action_kaleiest, action_sweetest])\n",
    "\n",
    "        obs, reward, done, _ = modified_lts_10_1_env.step(action)\n",
    "        rewards.append(reward)\n",
    "\n",
    "    # Create linear model of rewards over time.\n",
    "    reward_linreg = linregress(np.array((range(len(rewards)))), np.array(rewards))\n",
    "    slopes.append(reward_linreg.slope)\n",
    "\n",
    "print(np.mean(slopes))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "634b4da8-ae75-40b6-9390-afa168b8d6b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('user', array([0.50400919])),\n",
      "             ('doc',\n",
      "              {'0': array([0.5488135], dtype=float32),\n",
      "               '1': array([0.71518934], dtype=float32),\n",
      "               '2': array([0.60276335], dtype=float32),\n",
      "               '3': array([0.5448832], dtype=float32),\n",
      "               '4': array([0.4236548], dtype=float32),\n",
      "               '5': array([0.6458941], dtype=float32),\n",
      "               '6': array([0.4375872], dtype=float32),\n",
      "               '7': array([0.891773], dtype=float32),\n",
      "               '8': array([0.96366274], dtype=float32),\n",
      "               '9': array([0.3834415], dtype=float32)}),\n",
      "             ('response',\n",
      "              ({'click': 1, 'watch_time': array(10.986186, dtype=float32)},))])\n",
      "reward = 10.99; done = True\n"
     ]
    }
   ],
   "source": [
    "# Let's send our first action (1-slate back into the env) using the env's `step()` method.\n",
    "action = 3  # Discrete(10): 0-9 are all valid actions\n",
    "\n",
    "# This method returns 4 items:\n",
    "# - next observation (after having applied the action)\n",
    "# - reward (after having applied the action)\n",
    "# - `done` flag; if True, the episode is terminated and the environment needs to be `reset()` again.\n",
    "# - info dict (we'll ignore this)\n",
    "next_obs, reward, done, _ = modified_lts_10_1_env.step(action)\n",
    "\n",
    "# Print out the next observation.\n",
    "# We expect the \"doc\" and \"user\" items to be the same as in the previous observation\n",
    "# b/c we set \"resample_documents\" to False.\n",
    "pprint(next_obs)\n",
    "# Print out rewards and the vlaue of the `done` flag.\n",
    "print(f\"reward = {reward:.2f}; done = {done}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be848212-87b8-4eb1-9353-74e09ae72310",
   "metadata": {
    "id": "be848212-87b8-4eb1-9353-74e09ae72310"
   },
   "source": [
    "## Measuring random baseline of our environment\n",
    "\n",
    "In the cells above, we created a new environment instance (`lts_10_1_env`). As we have seen above, in order to start \"walking\" through a recommender system episode, we need to perform `reset()` and then several `step()` calls (with different actions) until the returned `done` flag is True.\n",
    "\n",
    "Let's find out how well a randomly acting agent performs in this environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "spatial-geography",
   "metadata": {
    "id": "spatial-geography"
   },
   "outputs": [],
   "source": [
    "# Function that measures and outputs the random baseline reward.\n",
    "# This is the expected accumulated reward per episode, if we act randomly (recommend random items) at each time step.\n",
    "def measure_random_performance_for_env(env, episodes=1000, verbose=False):\n",
    "\n",
    "    # Reset the env.\n",
    "    env.reset()\n",
    "\n",
    "    # Number of episodes already done.\n",
    "    num_episodes = 0\n",
    "    # Current episode's accumulated reward.\n",
    "    episode_reward = 0.0\n",
    "    # Collect all episode rewards here to be able to calculate a random baseline reward.\n",
    "    episode_rewards = []\n",
    "\n",
    "    # Enter while loop (to step through the episode).\n",
    "    while num_episodes < episodes:\n",
    "        # Produce a random action.\n",
    "        action = env.action_space.sample()\n",
    "\n",
    "        # Send the action to the env's `step()` method to receive: obs, reward, done, and info.\n",
    "        obs, reward, done, _ = env.step(action)\n",
    "        episode_reward += reward\n",
    "\n",
    "        # Check, whether the episde is done, if yes, reset and increase episode counter.\n",
    "        if done:\n",
    "            if verbose:\n",
    "                print(f\"Episode done - accumulated reward={episode_reward}\")\n",
    "            elif num_episodes % 100 == 0:\n",
    "                print(f\" {num_episodes} \", end=\"\")\n",
    "            elif num_episodes % 10 == 0:\n",
    "                print(\".\", end=\"\")\n",
    "            num_episodes += 1\n",
    "            env.reset()\n",
    "            episode_rewards.append(episode_reward)\n",
    "            episode_reward = 0.0\n",
    "\n",
    "    # Print out and return mean episode reward (and standard error of the mean).\n",
    "    env_mean_random_reward = np.mean(episode_rewards)\n",
    "\n",
    "    print(f\"\\n\\nMean episode reward when acting randomly: {env_mean_random_reward:.2f}+/-{sem(episode_rewards):.2f}\")\n",
    "\n",
    "    return env_mean_random_reward, sem(episode_rewards)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0e6e63e6-d030-4a45-af5b-ab88eaef3969",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0e6e63e6-d030-4a45-af5b-ab88eaef3969",
    "outputId": "212896b4-4ed4-41f2-c53c-5cd18b837744"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1e+03 ns, sys: 0 ns, total: 1e+03 ns\n",
      "Wall time: 2.15 µs\n",
      " 0 ......... 100 ......... 200 ......... 300 ......... 400 ......... 500 ......... 600 ......... 700 ......... 800 ......... 900 .........\n",
      "\n",
      "Mean episode reward when acting randomly: 1157.63+/-0.37\n"
     ]
    }
   ],
   "source": [
    "%time\n",
    "# Let's create a somewhat tougher version of this with 20 candidates (instead of 10) and a slate-size of 2.\n",
    "# We'll also keep using our wrapper from above to strengthen the dissatisfaction effect on the engagement:\n",
    "lts_20_2_env = LTSWithStrongerDissatisfactionEffect(LongTermSatisfactionRecSimEnv(config={\n",
    "    \"num_candidates\": 20,\n",
    "    \"slate_size\": 2,  # MultiDiscrete([20, 20]) -> Discrete(400)\n",
    "    \"resample_documents\": True,\n",
    "    # Convert to Discrete action space.\n",
    "    \"convert_to_discrete_action_space\": True,\n",
    "    # Wrap observations for RLlib bandit: Only changes dict keys (\"item\" instead of \"doc\").\n",
    "    \"wrap_for_bandits\": True,\n",
    "}))\n",
    "\n",
    "lts_20_2_env_mean_random_reward, _ = measure_random_performance_for_env(lts_20_2_env, episodes=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b20ac75-f3e6-4975-a209-2bf110b4ee13",
   "metadata": {
    "id": "5b20ac75-f3e6-4975-a209-2bf110b4ee13"
   },
   "source": [
    "# Plugging in RLlib\n",
    "<a id='rllib'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a76f02f-ef66-484d-8a1a-074a6e25c84a",
   "metadata": {
    "id": "8a76f02f-ef66-484d-8a1a-074a6e25c84a"
   },
   "source": [
    "## Picking an RLlib algorithm (\"Trainer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65aa24b2-ac17-44a3-b7b1-274ce2f50a87",
   "metadata": {
    "id": "65aa24b2-ac17-44a3-b7b1-274ce2f50a87"
   },
   "source": [
    "https://docs.ray.io/en/master/rllib-algorithms.html#available-algorithms-overview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0194b33a-e031-49ce-9ff2-b32e328f9955",
   "metadata": {
    "id": "0194b33a-e031-49ce-9ff2-b32e328f9955"
   },
   "source": [
    "\n",
    "\n",
    "<td> <img src=https://drive.google.com/uc?id=1CvhB59H2PsmKeFy-4hZuhRVHxymOH0hd /> </td>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62b1b0b3-ec96-41c0-9d5b-93db1c5ce021",
   "metadata": {
    "id": "62b1b0b3-ec96-41c0-9d5b-93db1c5ce021"
   },
   "source": [
    "### Trying a \"Contextual n-armed Bandit\" on our environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4a26e094-9887-4fc6-88b6-d1448e931526",
   "metadata": {
    "id": "4a26e094-9887-4fc6-88b6-d1448e931526"
   },
   "outputs": [],
   "source": [
    "# In order to use one of the above algorithms, you may instantiate its associated Trainer class.\n",
    "# For example, to import a Bandit Trainer w/ Upper Confidence Bound (UCB) exploration, do:\n",
    "\n",
    "from ray.rllib.agents.bandit import BanditLinUCBTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0911f212-523e-4a75-846d-342dd2a681a6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0911f212-523e-4a75-846d-342dd2a681a6",
    "outputId": "cf904666-3fc2-4a83-8147-28c9121aafde"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bandit's default config is:\n",
      "{'_disable_action_flattening': False,\n",
      " '_disable_execution_plan_api': False,\n",
      " '_disable_preprocessor_api': False,\n",
      " '_fake_gpus': False,\n",
      " '_tf_policy_handles_more_than_one_loss': False,\n",
      " 'action_space': None,\n",
      " 'actions_in_input_normalized': False,\n",
      " 'always_attach_evaluation_results': False,\n",
      " 'batch_mode': 'truncate_episodes',\n",
      " 'callbacks': <class 'ray.rllib.agents.callbacks.DefaultCallbacks'>,\n",
      " 'clip_actions': False,\n",
      " 'clip_rewards': None,\n",
      " 'collect_metrics_timeout': -1,\n",
      " 'compress_observations': False,\n",
      " 'create_env_on_driver': False,\n",
      " 'custom_eval_function': None,\n",
      " 'custom_resources_per_worker': {},\n",
      " 'disable_env_checking': False,\n",
      " 'eager_max_retraces': 20,\n",
      " 'eager_tracing': False,\n",
      " 'env': None,\n",
      " 'env_config': {},\n",
      " 'env_task_fn': None,\n",
      " 'evaluation_config': {},\n",
      " 'evaluation_duration': 10,\n",
      " 'evaluation_duration_unit': 'episodes',\n",
      " 'evaluation_interval': None,\n",
      " 'evaluation_num_episodes': -1,\n",
      " 'evaluation_num_workers': 0,\n",
      " 'evaluation_parallel_to_training': False,\n",
      " 'exploration_config': {'type': 'StochasticSampling'},\n",
      " 'explore': True,\n",
      " 'extra_python_environs_for_driver': {},\n",
      " 'extra_python_environs_for_worker': {},\n",
      " 'fake_sampler': False,\n",
      " 'framework': 'torch',\n",
      " 'gamma': 0.99,\n",
      " 'horizon': None,\n",
      " 'ignore_worker_failures': False,\n",
      " 'in_evaluation': False,\n",
      " 'input': 'sampler',\n",
      " 'input_config': {},\n",
      " 'input_evaluation': ['is', 'wis'],\n",
      " 'keep_per_episode_custom_metrics': False,\n",
      " 'local_tf_session_args': {'inter_op_parallelism_threads': 8,\n",
      "                           'intra_op_parallelism_threads': 8},\n",
      " 'log_level': 'WARN',\n",
      " 'log_sys_usage': True,\n",
      " 'logger_config': None,\n",
      " 'lr': 0.0001,\n",
      " 'metrics_episode_collection_timeout_s': 180,\n",
      " 'metrics_num_episodes_for_smoothing': 100,\n",
      " 'metrics_smoothing_episodes': -1,\n",
      " 'min_iter_time_s': -1,\n",
      " 'min_sample_timesteps_per_reporting': None,\n",
      " 'min_time_s_per_reporting': None,\n",
      " 'min_train_timesteps_per_reporting': None,\n",
      " 'model': {'_disable_action_flattening': False,\n",
      "           '_disable_preprocessor_api': False,\n",
      "           '_time_major': False,\n",
      "           '_use_default_native_models': False,\n",
      "           'attention_dim': 64,\n",
      "           'attention_head_dim': 32,\n",
      "           'attention_init_gru_gate_bias': 2.0,\n",
      "           'attention_memory_inference': 50,\n",
      "           'attention_memory_training': 50,\n",
      "           'attention_num_heads': 1,\n",
      "           'attention_num_transformer_units': 1,\n",
      "           'attention_position_wise_mlp_dim': 32,\n",
      "           'attention_use_n_prev_actions': 0,\n",
      "           'attention_use_n_prev_rewards': 0,\n",
      "           'conv_activation': 'relu',\n",
      "           'conv_filters': None,\n",
      "           'custom_action_dist': None,\n",
      "           'custom_model': None,\n",
      "           'custom_model_config': {},\n",
      "           'custom_preprocessor': None,\n",
      "           'dim': 84,\n",
      "           'fcnet_activation': 'tanh',\n",
      "           'fcnet_hiddens': [256, 256],\n",
      "           'framestack': True,\n",
      "           'free_log_std': False,\n",
      "           'grayscale': False,\n",
      "           'lstm_cell_size': 256,\n",
      "           'lstm_use_prev_action': False,\n",
      "           'lstm_use_prev_action_reward': -1,\n",
      "           'lstm_use_prev_reward': False,\n",
      "           'max_seq_len': 20,\n",
      "           'no_final_linear': False,\n",
      "           'post_fcnet_activation': 'relu',\n",
      "           'post_fcnet_hiddens': [],\n",
      "           'use_attention': False,\n",
      "           'use_lstm': False,\n",
      "           'vf_share_layers': True,\n",
      "           'zero_mean': True},\n",
      " 'monitor': -1,\n",
      " 'multiagent': {'count_steps_by': 'env_steps',\n",
      "                'observation_fn': None,\n",
      "                'policies': {},\n",
      "                'policies_to_train': None,\n",
      "                'policy_map_cache': None,\n",
      "                'policy_map_capacity': 100,\n",
      "                'policy_mapping_fn': None,\n",
      "                'replay_mode': 'independent'},\n",
      " 'no_done_at_end': False,\n",
      " 'normalize_actions': True,\n",
      " 'num_cpus_for_driver': 1,\n",
      " 'num_cpus_per_worker': 1,\n",
      " 'num_envs_per_worker': 1,\n",
      " 'num_gpus': 0,\n",
      " 'num_gpus_per_worker': 0,\n",
      " 'num_workers': 0,\n",
      " 'observation_filter': 'NoFilter',\n",
      " 'observation_space': None,\n",
      " 'optimizer': {},\n",
      " 'output': None,\n",
      " 'output_compress_columns': ['obs', 'new_obs'],\n",
      " 'output_config': {},\n",
      " 'output_max_file_size': 67108864,\n",
      " 'placement_strategy': 'PACK',\n",
      " 'postprocess_inputs': False,\n",
      " 'preprocessor_pref': 'deepmind',\n",
      " 'record_env': False,\n",
      " 'remote_env_batch_wait_ms': 0,\n",
      " 'remote_worker_envs': False,\n",
      " 'render_env': False,\n",
      " 'rollout_fragment_length': 1,\n",
      " 'sample_async': False,\n",
      " 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>,\n",
      " 'seed': None,\n",
      " 'shuffle_buffer_size': 0,\n",
      " 'simple_optimizer': -1,\n",
      " 'soft_horizon': False,\n",
      " 'synchronize_filters': True,\n",
      " 'tf_session_args': {'allow_soft_placement': True,\n",
      "                     'device_count': {'CPU': 1},\n",
      "                     'gpu_options': {'allow_growth': True},\n",
      "                     'inter_op_parallelism_threads': 2,\n",
      "                     'intra_op_parallelism_threads': 2,\n",
      "                     'log_device_placement': False},\n",
      " 'timesteps_per_iteration': 100,\n",
      " 'train_batch_size': 1}\n"
     ]
    }
   ],
   "source": [
    "# Configuration dicts for RLlib Trainers.\n",
    "# Where are the default configuration dicts stored?\n",
    "\n",
    "# E.g. Bandit algorithms:\n",
    "from ray.rllib.agents.bandit.bandit import DEFAULT_CONFIG as BANDIT_DEFAULT_CONFIG\n",
    "print(f\"Bandit's default config is:\")\n",
    "pprint(BANDIT_DEFAULT_CONFIG)\n",
    "\n",
    "# DQN algorithm:\n",
    "#from ray.rllib.agents.dqn import DEFAULT_CONFIG as DQN_DEFAULT_CONFIG\n",
    "#print(f\"DQN's default config is:\")\n",
    "#pprint(DQN_DEFAULT_CONFIG)\n",
    "\n",
    "# Common (all algorithms).\n",
    "#from ray.rllib.agents.trainer import COMMON_CONFIG\n",
    "#print(f\"RLlib Trainer's default config is:\")\n",
    "#pprint(COMMON_CONFIG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c9bd9775-f2bb-41d9-8ff6-20be9abd68db",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c9bd9775-f2bb-41d9-8ff6-20be9abd68db",
    "outputId": "5af8d60a-6770-4008-a0cd-ddbc1965b8a8"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-20 11:42:40,661\tWARNING rollout_worker.py:498 -- We've added a module for checking environments that are used in experiments. It will cause your environment to fail if your environment is not set upcorrectly. You can disable check env by setting `disable_env_checking` to True in your experiment config dictionary. You can run the environment checking module standalone by calling ray.rllib.utils.check_env(env).\n",
      "2022-04-20 11:42:40,662\tWARNING env.py:120 -- Your env doesn't have a .spec.max_episode_steps attribute. This is fine if you have set 'horizon' in your config dictionary, or `soft_horizon`. However, if you haven't, 'horizon' will default to infinity, and your environment will not be reset.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BanditLinUCBTrainer"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bandit_config = {\n",
    "    \"env\": \"modified_lts\",\n",
    "    \"env_config\": {\n",
    "        \"num_candidates\": 20,  # 20x19 = ~400 unique slates (arms)\n",
    "        \"slate_size\": 2,\n",
    "        \"resample_documents\": True,\n",
    "\n",
    "        # Bandit-specific flags:\n",
    "        \"convert_to_discrete_action_space\": True,\n",
    "        # Convert \"doc\" key into \"item\" key.\n",
    "        \"wrap_for_bandits\": True,\n",
    "        # Use consistent seeds for the environment ...\n",
    "        \"seed\": 0,\n",
    "    },\n",
    "    # ... and the Trainer itself.\n",
    "    \"seed\": 0,\n",
    "    # Maybe try different seeds to get nicer (more monotonic) plots\n",
    "\n",
    "    # The following settings are affecting the reporting only:\n",
    "    # ---\n",
    "    # Generate a result dict every single time step.\n",
    "    \"timesteps_per_iteration\": 1,\n",
    "    # Report rewards as smoothed mean over this many episodes.\n",
    "    \"metrics_num_episodes_for_smoothing\": 200,\n",
    "}\n",
    "\n",
    "# Create the RLlib Trainer using above config.\n",
    "bandit_trainer = BanditLinUCBTrainer(config=bandit_config)\n",
    "bandit_trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46a22cc0-0efb-40be-85fe-720e62a7a419",
   "metadata": {
    "id": "46a22cc0-0efb-40be-85fe-720e62a7a419"
   },
   "source": [
    "#### Running a single training iteration, by calling the `.train()` method:\n",
    "\n",
    "One iteration for most algos involves:\n",
    "\n",
    "1. Sampling from the environment(s)\n",
    "1. Using the sampled data (observations, actions taken, rewards) to update the policy model (e.g. a neural network), such that it would pick better actions in the future, leading to higher rewards.\n",
    "\n",
    "Let's try it out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ddd18251-2a1a-4822-8744-ca6df4a14787",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ddd18251-2a1a-4822-8744-ca6df4a14787",
    "outputId": "74abdd34-7dcb-4ada-ccf2-2bdbb7687a91"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-20 11:42:40,687\tWARNING bandit_torch_policy.py:48 -- The env did not report `regret` values in its `info` return, ignoring.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'agent_timesteps_total': 1,\n",
      " 'custom_metrics': {},\n",
      " 'date': '2022-04-20_11-42-40',\n",
      " 'done': False,\n",
      " 'episode_len_mean': nan,\n",
      " 'episode_media': {},\n",
      " 'episode_reward_max': nan,\n",
      " 'episode_reward_mean': nan,\n",
      " 'episode_reward_min': nan,\n",
      " 'episodes_this_iter': 0,\n",
      " 'episodes_total': 0,\n",
      " 'experiment_id': '08b7587d26934c65a4677f7fbffb68d2',\n",
      " 'hist_stats': {'episode_lengths': [], 'episode_reward': []},\n",
      " 'hostname': 'Christys-MacBook-Pro.local',\n",
      " 'info': {'learner': {'default_policy': {'learner_stats': {'update_latency': 0.0005447864532470703}}},\n",
      "          'num_agent_steps_sampled': 1,\n",
      "          'num_agent_steps_trained': 1,\n",
      "          'num_steps_sampled': 1,\n",
      "          'num_steps_trained': 1,\n",
      "          'num_steps_trained_this_iter': 1},\n",
      " 'iterations_since_restore': 1,\n",
      " 'node_ip': '127.0.0.1',\n",
      " 'num_healthy_workers': 0,\n",
      " 'off_policy_estimator': {},\n",
      " 'perf': {'cpu_util_percent': 26.0, 'ram_util_percent': 79.7},\n",
      " 'pid': 3139,\n",
      " 'policy_reward_max': {},\n",
      " 'policy_reward_mean': {},\n",
      " 'policy_reward_min': {},\n",
      " 'sampler_perf': {},\n",
      " 'time_since_restore': 0.006966114044189453,\n",
      " 'time_this_iter_s': 0.006966114044189453,\n",
      " 'time_total_s': 0.006966114044189453,\n",
      " 'timers': {'learn_throughput': 1281.878,\n",
      "            'learn_time_ms': 0.78,\n",
      "            'load_throughput': 9915.612,\n",
      "            'load_time_ms': 0.101,\n",
      "            'sample_throughput': 54.463,\n",
      "            'sample_time_ms': 18.361},\n",
      " 'timestamp': 1650480160,\n",
      " 'timesteps_since_restore': 1,\n",
      " 'timesteps_this_iter': 1,\n",
      " 'timesteps_total': 1,\n",
      " 'training_iteration': 1,\n",
      " 'trial_id': 'default',\n",
      " 'warmup_time': 0.03464484214782715}\n"
     ]
    }
   ],
   "source": [
    "# Perform single `.train()` call.\n",
    "result = bandit_trainer.train()\n",
    "# Erase config dict from result (for better overview).\n",
    "del result[\"config\"]\n",
    "# Print out training iteration results.\n",
    "pprint(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6d6f089b-e0cc-47de-af9b-dc05a71e102f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 513
    },
    "id": "6d6f089b-e0cc-47de-af9b-dc05a71e102f",
    "outputId": "e5368ef3-78d2-4973-c6b1-1dc6e7f9191d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 0 .... 500 .... 1000 .... 1500 .... 2000 .... 2500 ...."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/0g/jfs_l_113_356_c0rfp4jd8c0000gn/T/ipykernel_3139/4138562938.py:20: RuntimeWarning: Mean of empty slice\n",
      "  y = [np.nanmean(rewards[max(i - smoothing_win, 0):i + 1]) for i in range(start_at, len(rewards))]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x504 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Train for n more iterations (timesteps) and collect n-arm rewards.\n",
    "rewards = []\n",
    "for i in range(3000):\n",
    "    # Run a single timestep in the environment and update\n",
    "    # the model immediately on the received reward.\n",
    "    result = bandit_trainer.train()\n",
    "    # Extract reward from results.\n",
    "    #rewards.extend(result[\"hist_stats\"][\"episode_reward\"]\n",
    "    rewards.append(result[\"episode_reward_mean\"])\n",
    "    if i % 500 == 0:\n",
    "        print(f\" {i} \", end=\"\")\n",
    "    elif i % 100 == 0:\n",
    "        print(\".\", end=\"\")\n",
    "\n",
    "# Plot per-timestep (episode) rewards.\n",
    "plt.figure(figsize=(10,7))\n",
    "start_at = 0\n",
    "smoothing_win = 200\n",
    "x = list(range(start_at, len(rewards)))\n",
    "y = [np.nanmean(rewards[max(i - smoothing_win, 0):i + 1]) for i in range(start_at, len(rewards))]\n",
    "plt.plot(x, y)\n",
    "plt.title(\"Mean reward\")\n",
    "plt.xlabel(\"Time/Training steps\")\n",
    "\n",
    "# Add mean random baseline reward (red line).\n",
    "plt.axhline(y=lts_20_2_env_mean_random_reward, color=\"r\", linestyle=\"-\")\n",
    "\n",
    "plt.show();\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b77c3cf-226c-4cab-82c6-7d8a54bfe9ea",
   "metadata": {
    "id": "8b77c3cf-226c-4cab-82c6-7d8a54bfe9ea"
   },
   "source": [
    "<a id='bandit_results'></a>\n",
    "### What does our trained Bandit actually recommend?\n",
    "\n",
    "The first method of the RLlib Trainer API we used above was `train()`.\n",
    "We'll now use another method of the Trainer, `compute_single_action(input_dict={})`.\n",
    "It takes a input_dict keyword arg, into which you may pass a single (unbatched!) observation to receive an action for:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6eb62acb-0a16-4412-949a-4851201620bf",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6eb62acb-0a16-4412-949a-4851201620bf",
    "outputId": "e12959d4-ae02-4081-8343-b27ad5c50a9e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action's feature value=0.8916457295417786; max-choc-feature=0.8916457295417786; \n",
      "action's feature value=0.9777311682701111; max-choc-feature=0.9777311682701111; \n",
      "action's feature value=0.9989755749702454; max-choc-feature=0.9989755749702454; \n",
      "action's feature value=0.8999373912811279; max-choc-feature=0.8999373912811279; \n",
      "action's feature value=0.9658776521682739; max-choc-feature=0.9658776521682739; \n",
      "action's feature value=0.9644351601600647; max-choc-feature=0.9644351601600647; \n",
      "action's feature value=0.9434471726417542; max-choc-feature=0.9434471726417542; \n",
      "action's feature value=0.974498987197876; max-choc-feature=0.974498987197876; \n",
      "action's feature value=0.9987043142318726; max-choc-feature=0.9987043142318726; \n",
      "action's feature value=0.9603764414787292; max-choc-feature=0.9603764414787292; \n",
      "action's feature value=0.9986302256584167; max-choc-feature=0.9986302256584167; \n",
      "action's feature value=0.9272821545600891; max-choc-feature=0.9272821545600891; \n",
      "action's feature value=0.9550436735153198; max-choc-feature=0.9550436735153198; \n",
      "action's feature value=0.9447901844978333; max-choc-feature=0.9447901844978333; \n",
      "action's feature value=0.9695131778717041; max-choc-feature=0.9695131778717041; \n",
      "action's feature value=0.965463399887085; max-choc-feature=0.965463399887085; \n",
      "action's feature value=0.8990983963012695; max-choc-feature=0.8990983963012695; \n",
      "action's feature value=0.9040490984916687; max-choc-feature=0.9040490984916687; \n",
      "action's feature value=0.9543625116348267; max-choc-feature=0.9543625116348267; \n",
      "action's feature value=0.9245824813842773; max-choc-feature=0.9245824813842773; \n",
      "action's feature value=0.9099351763725281; max-choc-feature=0.9099351763725281; \n",
      "action's feature value=0.9679779410362244; max-choc-feature=0.9679779410362244; \n",
      "action's feature value=0.9706671833992004; max-choc-feature=0.9706671833992004; \n",
      "action's feature value=0.995324432849884; max-choc-feature=0.995324432849884; \n",
      "action's feature value=0.9396806955337524; max-choc-feature=0.9396806955337524; \n",
      "action's feature value=0.8787536025047302; max-choc-feature=0.8787536025047302; \n",
      "action's feature value=0.9149923920631409; max-choc-feature=0.9149923920631409; \n",
      "action's feature value=0.9406102895736694; max-choc-feature=0.9406102895736694; \n",
      "action's feature value=0.9478465914726257; max-choc-feature=0.9478465914726257; \n",
      "action's feature value=0.9911457300186157; max-choc-feature=0.9911457300186157; \n",
      "action's feature value=0.8890330791473389; max-choc-feature=0.8890330791473389; \n",
      "action's feature value=0.9480608701705933; max-choc-feature=0.9480608701705933; \n",
      "action's feature value=0.9916664958000183; max-choc-feature=0.9916664958000183; \n",
      "action's feature value=0.9847033619880676; max-choc-feature=0.9847033619880676; \n",
      "action's feature value=0.9915906190872192; max-choc-feature=0.9915906190872192; \n",
      "action's feature value=0.9487980008125305; max-choc-feature=0.9487980008125305; \n",
      "action's feature value=0.9980818629264832; max-choc-feature=0.9980818629264832; \n",
      "action's feature value=0.9360242486000061; max-choc-feature=0.9360242486000061; \n",
      "action's feature value=0.9260164499282837; max-choc-feature=0.9260164499282837; \n",
      "action's feature value=0.9987164735794067; max-choc-feature=0.9987164735794067; \n",
      "action's feature value=0.8080435991287231; max-choc-feature=0.8080435991287231; \n",
      "action's feature value=0.9191829562187195; max-choc-feature=0.9191829562187195; \n",
      "action's feature value=0.9863934516906738; max-choc-feature=0.9863934516906738; \n",
      "action's feature value=0.9714748859405518; max-choc-feature=0.9714748859405518; \n",
      "action's feature value=0.9963867664337158; max-choc-feature=0.9963867664337158; \n",
      "action's feature value=0.9447032809257507; max-choc-feature=0.9447032809257507; \n",
      "action's feature value=0.9920045733451843; max-choc-feature=0.9920045733451843; \n",
      "action's feature value=0.9945886731147766; max-choc-feature=0.9945886731147766; \n",
      "action's feature value=0.9901645183563232; max-choc-feature=0.9901645183563232; \n",
      "action's feature value=0.9611482620239258; max-choc-feature=0.9611482620239258; \n",
      "action's feature value=0.8877958059310913; max-choc-feature=0.8877958059310913; \n",
      "action's feature value=0.980872392654419; max-choc-feature=0.980872392654419; \n",
      "action's feature value=0.9676306247711182; max-choc-feature=0.9676306247711182; \n",
      "action's feature value=0.7901422381401062; max-choc-feature=0.7901422381401062; \n",
      "action's feature value=0.9796108603477478; max-choc-feature=0.9796108603477478; \n",
      "action's feature value=0.9665442705154419; max-choc-feature=0.9665442705154419; \n",
      "action's feature value=0.9356783032417297; max-choc-feature=0.9356783032417297; \n",
      "action's feature value=0.971942126750946; max-choc-feature=0.971942126750946; \n",
      "action's feature value=0.9433143734931946; max-choc-feature=0.9433143734931946; \n",
      "action's feature value=0.9321243166923523; max-choc-feature=0.9321243166923523; \n",
      "action's feature value=0.8538448810577393; max-choc-feature=0.8538448810577393; \n",
      "action's feature value=0.9785804152488708; max-choc-feature=0.9785804152488708; \n",
      "action's feature value=0.9912826418876648; max-choc-feature=0.9912826418876648; \n",
      "action's feature value=0.8888525366783142; max-choc-feature=0.8888525366783142; \n",
      "action's feature value=0.9974634051322937; max-choc-feature=0.9974634051322937; \n",
      "action's feature value=0.9976392984390259; max-choc-feature=0.9976392984390259; \n",
      "action's feature value=0.9779126048088074; max-choc-feature=0.9779126048088074; \n",
      "action's feature value=0.9817178249359131; max-choc-feature=0.9817178249359131; \n",
      "action's feature value=0.9769214987754822; max-choc-feature=0.9769214987754822; \n",
      "action's feature value=0.9773058891296387; max-choc-feature=0.9773058891296387; \n",
      "action's feature value=0.9939188361167908; max-choc-feature=0.9939188361167908; \n",
      "action's feature value=0.9815660715103149; max-choc-feature=0.9815660715103149; \n",
      "action's feature value=0.9989805817604065; max-choc-feature=0.9989805817604065; \n",
      "action's feature value=0.9938614368438721; max-choc-feature=0.9938614368438721; \n",
      "action's feature value=0.9548345804214478; max-choc-feature=0.9548345804214478; \n",
      "action's feature value=0.9095875024795532; max-choc-feature=0.9095875024795532; \n",
      "action's feature value=0.9673813581466675; max-choc-feature=0.9673813581466675; \n",
      "action's feature value=0.931671142578125; max-choc-feature=0.931671142578125; \n",
      "action's feature value=0.9167065024375916; max-choc-feature=0.9167065024375916; \n",
      "action's feature value=0.9021305441856384; max-choc-feature=0.9021305441856384; \n",
      "action's feature value=0.999610185623169; max-choc-feature=0.999610185623169; \n",
      "action's feature value=0.8620670437812805; max-choc-feature=0.8620670437812805; \n",
      "action's feature value=0.9598987698554993; max-choc-feature=0.9598987698554993; \n",
      "action's feature value=0.9733729362487793; max-choc-feature=0.9733729362487793; \n",
      "action's feature value=0.9917956590652466; max-choc-feature=0.9917956590652466; \n",
      "action's feature value=0.919234573841095; max-choc-feature=0.919234573841095; \n",
      "action's feature value=0.9918598532676697; max-choc-feature=0.9918598532676697; \n",
      "action's feature value=0.9960801005363464; max-choc-feature=0.9960801005363464; \n",
      "action's feature value=0.9754700064659119; max-choc-feature=0.9754700064659119; \n",
      "action's feature value=0.9660402536392212; max-choc-feature=0.9660402536392212; \n",
      "action's feature value=0.996029257774353; max-choc-feature=0.996029257774353; \n",
      "action's feature value=0.9930408000946045; max-choc-feature=0.9930408000946045; \n",
      "action's feature value=0.9743787050247192; max-choc-feature=0.9743787050247192; \n",
      "action's feature value=0.9861087203025818; max-choc-feature=0.9861087203025818; \n",
      "action's feature value=0.9269380569458008; max-choc-feature=0.9269380569458008; \n",
      "action's feature value=0.9651040434837341; max-choc-feature=0.9651040434837341; \n",
      "action's feature value=0.9919588565826416; max-choc-feature=0.9919588565826416; \n",
      "action's feature value=0.946600079536438; max-choc-feature=0.946600079536438; \n",
      "action's feature value=0.9954290986061096; max-choc-feature=0.9954290986061096; \n",
      "action's feature value=0.916007399559021; max-choc-feature=0.916007399559021; \n",
      "action's feature value=0.9317758679389954; max-choc-feature=0.9317758679389954; \n",
      "action's feature value=0.9713598489761353; max-choc-feature=0.9713598489761353; \n",
      "action's feature value=0.9755606651306152; max-choc-feature=0.9755606651306152; \n",
      "action's feature value=0.986943781375885; max-choc-feature=0.986943781375885; \n",
      "action's feature value=0.9602279663085938; max-choc-feature=0.9602279663085938; \n",
      "action's feature value=0.8469273447990417; max-choc-feature=0.8469273447990417; \n",
      "action's feature value=0.9099560379981995; max-choc-feature=0.9099560379981995; \n",
      "action's feature value=0.9875852465629578; max-choc-feature=0.9875852465629578; \n",
      "action's feature value=0.9507820010185242; max-choc-feature=0.9507820010185242; \n",
      "action's feature value=0.9826569557189941; max-choc-feature=0.9826569557189941; \n",
      "action's feature value=0.9213010668754578; max-choc-feature=0.9213010668754578; \n",
      "action's feature value=0.9418420791625977; max-choc-feature=0.9418420791625977; \n",
      "action's feature value=0.9259132742881775; max-choc-feature=0.9259132742881775; \n",
      "action's feature value=0.9294229745864868; max-choc-feature=0.9294229745864868; \n",
      "action's feature value=0.9663825035095215; max-choc-feature=0.9663825035095215; \n",
      "action's feature value=0.9695129990577698; max-choc-feature=0.9695129990577698; \n",
      "action's feature value=0.9583300948143005; max-choc-feature=0.9583300948143005; \n",
      "action's feature value=0.9953047037124634; max-choc-feature=0.9953047037124634; \n",
      "action's feature value=0.9923339486122131; max-choc-feature=0.9923339486122131; \n",
      "action's feature value=0.9950851202011108; max-choc-feature=0.9950851202011108; \n"
     ]
    }
   ],
   "source": [
    "# Let's see what items our bandit recommends now that it has been trained and achieves good (>> random) rewards.\n",
    "obs = lts_20_2_env.reset()\n",
    "\n",
    "# Run a single episode.\n",
    "done = False\n",
    "while not done:\n",
    "    # Pass the single (unbatched) observation into the `compute_single_action` method of our Trainer.\n",
    "    # This is one way to perform inference on a learned policy.\n",
    "    action = bandit_trainer.compute_single_action(input_dict={\"obs\": obs})\n",
    "    feat_value_of_action = obs[\"item\"][action][0]\n",
    "    max_choc_feat = obs['item'][np.argmax(obs[\"item\"])][0]\n",
    "\n",
    "    # Print out the picked document's feature value and compare that to the highest possible feature value.\n",
    "    print(f\"action's feature value={feat_value_of_action}; max-choc-feature={max_choc_feat}; \")\n",
    "\n",
    "    # Apply the computed action in the environment and continue.\n",
    "    obs, r, done, _ = lts_20_2_env.step(action)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9355c1b-f0f7-4690-a7fe-332b01a651c4",
   "metadata": {
    "id": "c9355c1b-f0f7-4690-a7fe-332b01a651c4"
   },
   "source": [
    "### Ok, Bandits want Chocolate! :)\n",
    "#### Why is that?\n",
    "\n",
    "<td> <img src=https://drive.google.com/uc?id=1qOMMI80Wn8p1jbqEdtM-vnIA9wO1KX1y width=1000/> </td>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84291b69-050f-489a-b822-239294bb3a2e",
   "metadata": {
    "id": "84291b69-050f-489a-b822-239294bb3a2e"
   },
   "source": [
    "### Recap: Advantages and Disatvantages of Bandits:\n",
    "#### Advantages:\n",
    "* Very fast\n",
    "* Very sample-efficient\n",
    "* Easy to understand learning process\n",
    "\n",
    "#### Disadvantages\n",
    "* Need immediate reward (not capable of solving long-term satisfaction problem)\n",
    "* Only models 1 user at a time -> If > 1 user, must train separate bandit per user\n",
    "* Not able to handle components of MultiDiscrete action space separately (works only on flattened Discrete action space).  This means action space length explodes combinatorically."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "108a830b-cc5f-454c-b986-75c59d40df89",
   "metadata": {
    "id": "108a830b-cc5f-454c-b986-75c59d40df89"
   },
   "source": [
    "<a id='slateq'></a>\n",
    "# Switching to Slate-Q\n",
    "\n",
    "\n",
    "https://docs.ray.io/en/master/rllib-algorithms.html#available-algorithms-overview\n",
    "<td> <img src=https://drive.google.com/uc?id=1oEVD57X1MD7Z7D3roOz0kWj-hUUWydwe width=800/> </td>\n",
    "\n",
    "The RLlib team has implemented the Slate-Q algorithm from Google - designed for k-slate, long time horizon, and dynamic user recommendation problems. \n",
    "<br>\n",
    "\n",
    "<a href=\"https://storage.googleapis.com/pub-tools-public-publication-data/pdf/9f91de1fa0ac351ecb12e4062a37afb896aa1463.pdf\">Slate-Q Paper</a> <br>\n",
    "<a href=\"https://slideslive.com/38917655/reinforcement-learning-in-recommender-systems-some-challenges\">Author video</a> about Slate-Q\n",
    "\n",
    "<td> <img src=https://drive.google.com/uc?id=1iG1-lMP6jwh1rgodQoTcO5l67MgHy7qc width=1000/> </td>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "32828a91-2da7-4f5f-9374-d12f32ec0b87",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "32828a91-2da7-4f5f-9374-d12f32ec0b87",
    "outputId": "c5da9bf4-f048-4050-a449-188dbc2d87bd"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-20 11:42:54,671\tWARNING rollout_worker.py:498 -- We've added a module for checking environments that are used in experiments. It will cause your environment to fail if your environment is not set upcorrectly. You can disable check env by setting `disable_env_checking` to True in your experiment config dictionary. You can run the environment checking module standalone by calling ray.rllib.utils.check_env(env).\n",
      "2022-04-20 11:42:54,671\tWARNING env.py:120 -- Your env doesn't have a .spec.max_episode_steps attribute. This is fine if you have set 'horizon' in your config dictionary, or `soft_horizon`. However, if you haven't, 'horizon' will default to infinity, and your environment will not be reset.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/christy/mambaforge/envs/rllib/lib/python3.9/site-packages/tensorflow/python/util/deprecation.py:620: calling map_fn_v2 (from tensorflow.python.ops.map_fn) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use fn_output_signature instead\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/christy/mambaforge/envs/rllib/lib/python3.9/site-packages/tensorflow/python/util/deprecation.py:620: calling map_fn_v2 (from tensorflow.python.ops.map_fn) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use fn_output_signature instead\n",
      "2022-04-20 11:42:54.716891: W tensorflow/core/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n",
      "/Users/christy/mambaforge/envs/rllib/lib/python3.9/site-packages/tensorflow/python/framework/indexed_slices.py:448: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"default_policy/gradients/default_policy/GatherV2_5_grad/Reshape_1:0\", shape=(None,), dtype=int64), values=Tensor(\"default_policy/gradients/default_policy/GatherV2_5_grad/Reshape:0\", shape=(None,), dtype=float32), dense_shape=Tensor(\"default_policy/gradients/default_policy/GatherV2_5_grad/Cast:0\", shape=(1,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/christy/mambaforge/envs/rllib/lib/python3.9/site-packages/tensorflow/python/training/rmsprop.py:192: calling Ones.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/christy/mambaforge/envs/rllib/lib/python3.9/site-packages/tensorflow/python/training/rmsprop.py:192: calling Ones.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SlateQTrainer"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ray.rllib.agents.slateq import SlateQTrainer, DEFAULT_CONFIG\n",
    "\n",
    "slateq_config = {\n",
    "    \"env\": \"modified_lts\",\n",
    "    \"env_config\": {\n",
    "        \"num_candidates\": 20,  # MultiDiscrete([20, 20]) -> no flattening necessary (see `convert_to_discrete_action_space=False` below)\n",
    "        \"slate_size\": 2,\n",
    "        \"resample_documents\": True,\n",
    "        \"wrap_for_bandits\": False,  # SlateQ != Bandit (will keep \"doc\" key, instead of \"items\")\n",
    "        \"convert_to_discrete_action_space\": False,  # SlateQ handles MultiDiscrete action spaces (slate recommendations).\n",
    "    },\n",
    "    # Setup exploratory behavior: Implemented as \"epsilon greedy\" strategy:\n",
    "    # Act randomly `e` percent of the time; `e` gets reduced from 1.0 to almost 0.0 over\n",
    "    # the course of `epsilon_timesteps`.\n",
    "    \"exploration_config\": {\n",
    "        #\"warmup_timesteps\": 20000,  # default\n",
    "        \n",
    "        # Use Ray Tune to run 3 parallel tuning trials\n",
    "        # \"epsilon_timesteps\": tune.grid_search([30000, 60000, 3000]),  # default: 250000\n",
    "        \n",
    "        # Do not use Ray Tune\n",
    "        \"epsilon_timesteps\": 60000\n",
    "    },\n",
    "    #\"learning_starts\": 20000,  # default\n",
    "    \"target_network_update_freq\": 3200,\n",
    "\n",
    "    # Report rewards as smoothed mean over this many episodes.\n",
    "    \"metrics_num_episodes_for_smoothing\": 200,\n",
    "}\n",
    "\n",
    "# Instantiate the Trainer object using the exact same config as in our Bandit experiment above.\n",
    "slateq_trainer = SlateQTrainer(config=slateq_config)\n",
    "slateq_trainer\n",
    "\n",
    "# # You can change timesteps_total here to see more tuning\n",
    "# tune.run(\"SlateQ\", config=slateq_config, stop={\"timesteps_total\":1000, \n",
    "#                                                \"training_iteration\":5})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0845c458-d656-417e-b3d8-60596650c2bb",
   "metadata": {
    "id": "0845c458-d656-417e-b3d8-60596650c2bb"
   },
   "outputs": [],
   "source": [
    "# Optional - View the default configs of slateq\n",
    "# DEFAULT_CONFIG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95395f1a-31c6-4933-b09a-d06959ad5714",
   "metadata": {
    "id": "95395f1a-31c6-4933-b09a-d06959ad5714"
   },
   "source": [
    "<a id='slateq_experiment'></a>\n",
    "Now that we have confirmed we have setup the Trainer correctly, let's call `train()` on it several times:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dc47c75f-4f6f-4806-995e-80ec974cfd86",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dc47c75f-4f6f-4806-995e-80ec974cfd86",
    "outputId": "636becd4-2f93-463e-f8c2-9b4843af1513"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration=1; ts=20000: R(\"return\")=1157.2761583207634\n",
      "Iteration=2; ts=21000: R(\"return\")=1156.91634386882\n",
      "Iteration=3; ts=22000: R(\"return\")=1156.6548763192186\n",
      "Iteration=4; ts=23000: R(\"return\")=1156.9111065290024\n",
      "Iteration=5; ts=24000: R(\"return\")=1157.016263912152\n",
      "Iteration=6; ts=25000: R(\"return\")=1156.999344071749\n",
      "Iteration=7; ts=26000: R(\"return\")=1157.3029139590599\n",
      "Iteration=8; ts=27000: R(\"return\")=1157.633023682805\n",
      "Iteration=9; ts=28000: R(\"return\")=1157.7716835466558\n",
      "Iteration=10; ts=29000: R(\"return\")=1157.7476594847624\n",
      "Iteration=11; ts=30000: R(\"return\")=1158.0848271095376\n",
      "Iteration=12; ts=31000: R(\"return\")=1158.5483817190452\n",
      "Iteration=13; ts=32000: R(\"return\")=1158.758499288382\n",
      "Iteration=14; ts=33000: R(\"return\")=1159.1047279704146\n",
      "Iteration=15; ts=34000: R(\"return\")=1159.421138293356\n",
      "Iteration=16; ts=35000: R(\"return\")=1160.0460802573946\n",
      "Iteration=17; ts=36000: R(\"return\")=1160.7792092706968\n",
      "Iteration=18; ts=37000: R(\"return\")=1161.1634991429016\n",
      "Iteration=19; ts=38000: R(\"return\")=1161.6178904374467\n",
      "Iteration=20; ts=39000: R(\"return\")=1162.0354731536363\n",
      "Iteration=21; ts=40000: R(\"return\")=1162.9768224980396\n",
      "Iteration=22; ts=41000: R(\"return\")=1163.2481729709884\n",
      "Iteration=23; ts=42000: R(\"return\")=1164.0243920650153\n",
      "Iteration=24; ts=43000: R(\"return\")=1164.6135076055443\n",
      "Iteration=25; ts=44000: R(\"return\")=1164.979810086035\n",
      "Iteration=26; ts=45000: R(\"return\")=1166.0123987543818\n",
      "Iteration=27; ts=46000: R(\"return\")=1166.849033657618\n",
      "Iteration=28; ts=47000: R(\"return\")=1167.3162900762545\n",
      "Iteration=29; ts=48000: R(\"return\")=1167.9047870893085\n",
      "Iteration=30; ts=49000: R(\"return\")=1168.4927221905884\n",
      "Iteration=31; ts=50000: R(\"return\")=1168.717732546291\n",
      "Iteration=32; ts=51000: R(\"return\")=1168.980506105776\n",
      "Iteration=33; ts=52000: R(\"return\")=1169.3089780021512\n",
      "Iteration=34; ts=53000: R(\"return\")=1169.6852460930481\n",
      "Iteration=35; ts=54000: R(\"return\")=1170.4845782801528\n",
      "Iteration=36; ts=55000: R(\"return\")=1170.7412553957151\n",
      "Iteration=37; ts=56000: R(\"return\")=1170.9165787656377\n",
      "Iteration=38; ts=57000: R(\"return\")=1171.2715434593315\n",
      "Iteration=39; ts=58000: R(\"return\")=1171.5832863434773\n",
      "Iteration=40; ts=59000: R(\"return\")=1171.5307650855707\n",
      "Iteration=41; ts=60000: R(\"return\")=1171.249095025435\n",
      "Iteration=42; ts=61000: R(\"return\")=1171.3521641581603\n",
      "Iteration=43; ts=62000: R(\"return\")=1171.5065208049853\n",
      "Iteration=44; ts=63000: R(\"return\")=1171.7025607899882\n",
      "Iteration=45; ts=64000: R(\"return\")=1171.73601332917\n",
      "Iteration=46; ts=65000: R(\"return\")=1171.9228611057617\n",
      "Iteration=47; ts=66000: R(\"return\")=1171.6507011478693\n",
      "Iteration=48; ts=67000: R(\"return\")=1171.7013896729522\n",
      "Iteration=49; ts=68000: R(\"return\")=1171.6321846532442\n",
      "Iteration=50; ts=69000: R(\"return\")=1171.4032582002556\n",
      "Iteration=51; ts=70000: R(\"return\")=1171.129857883265\n",
      "Iteration=52; ts=71000: R(\"return\")=1171.156005500214\n",
      "Iteration=53; ts=72000: R(\"return\")=1170.9477882166111\n",
      "Iteration=54; ts=73000: R(\"return\")=1171.0561448092014\n",
      "Iteration=55; ts=74000: R(\"return\")=1171.2412525558393\n",
      "Iteration=56; ts=75000: R(\"return\")=1171.2780846010528\n",
      "Iteration=57; ts=76000: R(\"return\")=1170.9807805781809\n",
      "Iteration=58; ts=77000: R(\"return\")=1170.9289743517727\n",
      "Iteration=59; ts=78000: R(\"return\")=1170.335415764527\n",
      "Iteration=60; ts=79000: R(\"return\")=1169.893503226125\n"
     ]
    }
   ],
   "source": [
    "# See reward progress with time\n",
    "# Run `train()` n times. Repeatedly call `train()` now to see rewards increase.\n",
    "for _ in range(60):\n",
    "    results = slateq_trainer.train()\n",
    "    print(f\"Iteration={slateq_trainer.iteration}; ts={results['timesteps_total']}: R(\\\"return\\\")={results['episode_reward_mean']}\")\n",
    "    \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b86aecb-90ce-4be1-91a2-5c5391ab6adf",
   "metadata": {
    "id": "7b86aecb-90ce-4be1-91a2-5c5391ab6adf"
   },
   "source": [
    "------------------\n",
    "## 10 min break :)\n",
    "\n",
    "while Slate-Q is (hopefully) leaning\n",
    "\n",
    "------------------\n",
    "\n",
    "<a id='slateq_results'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "34bc1113-bd5e-45f5-bd8e-93931b8cee0b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "34bc1113-bd5e-45f5-bd8e-93931b8cee0b",
    "outputId": "a53677eb-4887-42c8-b61e-617b0224c2fb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action's feature value=0.9446688890457153 max-choc-feature=0.978618323802948\n",
      "action's feature value=0.6531082987785339 max-choc-feature=0.9883738160133362\n",
      "action's feature value=0.9767611026763916 max-choc-feature=0.9767611026763916\n",
      "action's feature value=0.8289400339126587 max-choc-feature=0.9292961955070496\n",
      "action's feature value=0.8811032176017761 max-choc-feature=0.9621885418891907\n",
      "action's feature value=0.9560836553573608 max-choc-feature=0.9560836553573608\n",
      "action's feature value=0.9988470077514648 max-choc-feature=0.9988470077514648\n",
      "action's feature value=0.9280812740325928 max-choc-feature=0.9755215048789978\n",
      "action's feature value=0.9443724155426025 max-choc-feature=0.9443724155426025\n",
      "action's feature value=0.990338921546936 max-choc-feature=0.990338921546936\n",
      "action's feature value=0.9473705887794495 max-choc-feature=0.9527916312217712\n",
      "action's feature value=0.9404319524765015 max-choc-feature=0.961936354637146\n",
      "action's feature value=0.9413776993751526 max-choc-feature=0.9774951338768005\n",
      "action's feature value=0.9608346819877625 max-choc-feature=0.9818294048309326\n",
      "action's feature value=0.774047315120697 max-choc-feature=0.9065554738044739\n",
      "action's feature value=0.7885454893112183 max-choc-feature=0.7885454893112183\n",
      "action's feature value=0.9564056992530823 max-choc-feature=0.9594333171844482\n",
      "action's feature value=0.9040443897247314 max-choc-feature=0.9591665863990784\n",
      "action's feature value=0.9589827060699463 max-choc-feature=0.9589827060699463\n",
      "action's feature value=0.9453015327453613 max-choc-feature=0.9453015327453613\n",
      "action's feature value=0.9903450012207031 max-choc-feature=0.9903450012207031\n",
      "action's feature value=0.904948353767395 max-choc-feature=0.9920112490653992\n",
      "action's feature value=0.940209686756134 max-choc-feature=0.9944007992744446\n",
      "action's feature value=0.96779465675354 max-choc-feature=0.96779465675354\n",
      "action's feature value=0.879234790802002 max-choc-feature=0.9241587519645691\n",
      "action's feature value=0.9488610029220581 max-choc-feature=0.9488610029220581\n",
      "action's feature value=0.8869608044624329 max-choc-feature=0.9627703428268433\n",
      "action's feature value=0.968286395072937 max-choc-feature=0.9805801510810852\n",
      "action's feature value=0.979526937007904 max-choc-feature=0.997962236404419\n",
      "action's feature value=0.9992780089378357 max-choc-feature=0.9992780089378357\n",
      "action's feature value=0.9587409496307373 max-choc-feature=0.9762256741523743\n",
      "action's feature value=0.9358320832252502 max-choc-feature=0.9834262132644653\n",
      "action's feature value=0.8224067091941833 max-choc-feature=0.857124924659729\n",
      "action's feature value=0.8427768349647522 max-choc-feature=0.95914226770401\n",
      "action's feature value=0.7814795970916748 max-choc-feature=0.8562761545181274\n",
      "action's feature value=0.987348735332489 max-choc-feature=0.987348735332489\n",
      "action's feature value=0.9663899540901184 max-choc-feature=0.9707314372062683\n",
      "action's feature value=0.9661474227905273 max-choc-feature=0.9918903112411499\n",
      "action's feature value=0.9325612187385559 max-choc-feature=0.9758838415145874\n",
      "action's feature value=0.7699671983718872 max-choc-feature=0.9543338418006897\n",
      "action's feature value=0.914863109588623 max-choc-feature=0.9323939681053162\n",
      "action's feature value=0.9195073843002319 max-choc-feature=0.9195073843002319\n",
      "action's feature value=0.923305332660675 max-choc-feature=0.9631972908973694\n",
      "action's feature value=0.8659454584121704 max-choc-feature=0.8667885661125183\n",
      "action's feature value=0.9804856777191162 max-choc-feature=0.9804856777191162\n",
      "action's feature value=0.9031496644020081 max-choc-feature=0.9031496644020081\n",
      "action's feature value=0.9738187193870544 max-choc-feature=0.9738187193870544\n",
      "action's feature value=0.9998085498809814 max-choc-feature=0.9998085498809814\n",
      "action's feature value=0.8339816331863403 max-choc-feature=0.9384120106697083\n",
      "action's feature value=0.9536756873130798 max-choc-feature=0.9536756873130798\n",
      "action's feature value=0.9438508749008179 max-choc-feature=0.9958152770996094\n",
      "action's feature value=0.9692058563232422 max-choc-feature=0.9692058563232422\n",
      "action's feature value=0.9416677951812744 max-choc-feature=0.9750946760177612\n",
      "action's feature value=0.9670549035072327 max-choc-feature=0.9673377871513367\n",
      "action's feature value=0.9851086735725403 max-choc-feature=0.9851086735725403\n",
      "action's feature value=0.9478219151496887 max-choc-feature=0.9979940056800842\n",
      "action's feature value=0.9555683732032776 max-choc-feature=0.9555683732032776\n",
      "action's feature value=0.923455536365509 max-choc-feature=0.983853816986084\n",
      "action's feature value=0.8668609261512756 max-choc-feature=0.9279761910438538\n",
      "action's feature value=0.8152250647544861 max-choc-feature=0.9421847462654114\n",
      "action's feature value=0.9822477698326111 max-choc-feature=0.992667019367218\n",
      "action's feature value=0.9163403511047363 max-choc-feature=0.9717630743980408\n",
      "action's feature value=0.8877008557319641 max-choc-feature=0.9792863130569458\n",
      "action's feature value=0.9468071460723877 max-choc-feature=0.9960712790489197\n",
      "action's feature value=0.9942330718040466 max-choc-feature=0.9942330718040466\n",
      "action's feature value=0.9824448823928833 max-choc-feature=0.9824448823928833\n",
      "action's feature value=0.6863802075386047 max-choc-feature=0.9492799043655396\n",
      "action's feature value=0.9440324902534485 max-choc-feature=0.9568705558776855\n",
      "action's feature value=0.9853785634040833 max-choc-feature=0.9853785634040833\n",
      "action's feature value=0.8986376523971558 max-choc-feature=0.9903685450553894\n",
      "action's feature value=0.944202721118927 max-choc-feature=0.9980227947235107\n",
      "action's feature value=0.7924988865852356 max-choc-feature=0.9376630783081055\n",
      "action's feature value=0.902131199836731 max-choc-feature=0.9834339618682861\n",
      "action's feature value=0.9832748770713806 max-choc-feature=0.9832748770713806\n",
      "action's feature value=0.789869487285614 max-choc-feature=0.9804664850234985\n",
      "action's feature value=0.9231590032577515 max-choc-feature=0.9825738668441772\n",
      "action's feature value=0.9983548521995544 max-choc-feature=0.9983548521995544\n",
      "action's feature value=0.7697890400886536 max-choc-feature=0.7697890400886536\n",
      "action's feature value=0.9985265731811523 max-choc-feature=0.9985265731811523\n",
      "action's feature value=0.827313244342804 max-choc-feature=0.9906516671180725\n",
      "action's feature value=0.7780388593673706 max-choc-feature=0.9890884160995483\n",
      "action's feature value=0.8701821565628052 max-choc-feature=0.9605224132537842\n",
      "action's feature value=0.9260265231132507 max-choc-feature=0.9260265231132507\n",
      "action's feature value=0.8029753565788269 max-choc-feature=0.9829264879226685\n",
      "action's feature value=0.9454309940338135 max-choc-feature=0.9454309940338135\n",
      "action's feature value=0.9270205497741699 max-choc-feature=0.9270205497741699\n",
      "action's feature value=0.9585323333740234 max-choc-feature=0.9585323333740234\n",
      "action's feature value=0.841815710067749 max-choc-feature=0.9738933444023132\n",
      "action's feature value=0.9793245196342468 max-choc-feature=0.9793245196342468\n",
      "action's feature value=0.9890880584716797 max-choc-feature=0.9890880584716797\n",
      "action's feature value=0.9670467972755432 max-choc-feature=0.9670467972755432\n",
      "action's feature value=0.9703752994537354 max-choc-feature=0.9703752994537354\n",
      "action's feature value=0.8652114868164062 max-choc-feature=0.9550641179084778\n",
      "action's feature value=0.8626660704612732 max-choc-feature=0.9574885368347168\n",
      "action's feature value=0.8878343105316162 max-choc-feature=0.9229142665863037\n",
      "action's feature value=0.9832027554512024 max-choc-feature=0.9832027554512024\n",
      "action's feature value=0.9427794218063354 max-choc-feature=0.998198926448822\n",
      "action's feature value=0.9634699821472168 max-choc-feature=0.9642097353935242\n",
      "action's feature value=0.9004101753234863 max-choc-feature=0.9417421221733093\n",
      "action's feature value=0.9040508270263672 max-choc-feature=0.9040508270263672\n",
      "action's feature value=0.9835554361343384 max-choc-feature=0.9835554361343384\n",
      "action's feature value=0.9334558844566345 max-choc-feature=0.9747744202613831\n",
      "action's feature value=0.7133703827857971 max-choc-feature=0.8408302664756775\n",
      "action's feature value=0.9482967257499695 max-choc-feature=0.9682117700576782\n",
      "action's feature value=0.9669559597969055 max-choc-feature=0.974723219871521\n",
      "action's feature value=0.20339584350585938 max-choc-feature=0.9895250797271729\n",
      "action's feature value=0.9818642139434814 max-choc-feature=0.9818642139434814\n",
      "action's feature value=0.987434983253479 max-choc-feature=0.987434983253479\n",
      "action's feature value=0.9674006104469299 max-choc-feature=0.9674006104469299\n",
      "action's feature value=0.9522157907485962 max-choc-feature=0.9522157907485962\n",
      "action's feature value=0.9103958606719971 max-choc-feature=0.9649279713630676\n",
      "action's feature value=0.9670055508613586 max-choc-feature=0.9949018955230713\n",
      "action's feature value=0.9238783121109009 max-choc-feature=0.9630938768386841\n",
      "action's feature value=0.9101313948631287 max-choc-feature=0.9801590442657471\n",
      "action's feature value=0.9809793829917908 max-choc-feature=0.9809793829917908\n",
      "action's feature value=0.9905391931533813 max-choc-feature=0.9905391931533813\n",
      "action's feature value=0.9747872352600098 max-choc-feature=0.9747872352600098\n",
      "action's feature value=0.9444741606712341 max-choc-feature=0.9961004257202148\n",
      "action's feature value=0.815617561340332 max-choc-feature=0.9473085999488831\n",
      "action's feature value=0.8309087753295898 max-choc-feature=0.9615751504898071\n"
     ]
    }
   ],
   "source": [
    "# Let's see what items our SlateQ recommends now that it has been lightly trained\n",
    "lts_20_2_env = LTSWithStrongerDissatisfactionEffect(LongTermSatisfactionRecSimEnv(config=slateq_config[\"env_config\"]))\n",
    "\n",
    "obs = lts_20_2_env.reset()\n",
    "done = False\n",
    "while not done:\n",
    "    action = slateq_trainer.compute_single_action(input_dict={\"obs\": obs})\n",
    "    feat_value_of_action = obs[\"doc\"][str(action[0])][0]\n",
    "    max_feat_action = np.argmax([value for _, value in obs[\"doc\"].items()])\n",
    "    max_choc_feat = obs['doc'][str(max_feat_action)][0]\n",
    "    # Print out the picked document's feature value and compare that to the highest possible feature value.\n",
    "    print(f\"action's feature value={feat_value_of_action} max-choc-feature={max_choc_feat}\")\n",
    "    \n",
    "    obs, r, done, _ = lts_20_2_env.step(action)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f409efcd-9c5c-4d91-a1ae-121b1b2fa698",
   "metadata": {
    "id": "f409efcd-9c5c-4d91-a1ae-121b1b2fa698"
   },
   "source": [
    "### Optional\n",
    "\n",
    "Feel free to play around with the following code in order to learn how RLlib - under the hood - calculates actions from the environment's observations using the SlateQ Policy and its NN models inside our Trainer object):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b2b62d74-6392-453a-b25f-f8cbc90009d6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b2b62d74-6392-453a-b25f-f8cbc90009d6",
    "outputId": "bae3a3b8-2882-4eee-b6f1-3487bce38130"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our Policy right now is: SlateQTFPolicy\n",
      "Our Policy's observation space is: Dict(user:Box([0.], [1.], (1,), float32), doc:Dict(0:Box([0.], [1.], (1,), float32), 1:Box([0.], [1.], (1,), float32), 2:Box([0.], [1.], (1,), float32), 3:Box([0.], [1.], (1,), float32), 4:Box([0.], [1.], (1,), float32), 5:Box([0.], [1.], (1,), float32), 6:Box([0.], [1.], (1,), float32), 7:Box([0.], [1.], (1,), float32), 8:Box([0.], [1.], (1,), float32), 9:Box([0.], [1.], (1,), float32), 10:Box([0.], [1.], (1,), float32), 11:Box([0.], [1.], (1,), float32), 12:Box([0.], [1.], (1,), float32), 13:Box([0.], [1.], (1,), float32), 14:Box([0.], [1.], (1,), float32), 15:Box([0.], [1.], (1,), float32), 16:Box([0.], [1.], (1,), float32), 17:Box([0.], [1.], (1,), float32), 18:Box([0.], [1.], (1,), float32), 19:Box([0.], [1.], (1,), float32)), response:Tuple(Dict(click:Discrete(2), watch_time:Box(0.0, 100.0, (), float32)), Dict(click:Discrete(2), watch_time:Box(0.0, 100.0, (), float32))))\n",
      "\n",
      "Our Policy's action space is: MultiDiscrete([20 20])\n",
      "\n",
      "q_values_per_candidate=[[25.14553  24.7266   23.989193 24.021303 24.139412 24.08431  24.057482\n",
      "  24.66595  24.561718 24.891829 24.26093  24.373672 24.456326 24.328707\n",
      "  24.424623 24.193018 24.483698 24.282854 23.9544   24.420815]]\n"
     ]
    }
   ],
   "source": [
    "# To get the policy inside the Trainer, use `Trainer.get_policy([policy ID]=\"default_policy\")`:\n",
    "policy = slateq_trainer.get_policy()\n",
    "print(f\"Our Policy right now is: {policy}\")\n",
    "\n",
    "# To get to the model inside any policy, do:\n",
    "model = policy.model\n",
    "#print(f\"Our Policy's model is: {model}\")\n",
    "\n",
    "# Print out the policy's action and observation spaces.\n",
    "print(f\"Our Policy's observation space is: {policy.observation_space}\\n\")\n",
    "print(f\"Our Policy's action space is: {policy.action_space}\\n\")\n",
    "\n",
    "# Produce a random obervation (B=1; batch of size 1).\n",
    "obs = lts_20_2_env.observation_space.sample()\n",
    "\n",
    "# tf-specific code: Use tf1.Session().\n",
    "sess = policy.get_session()\n",
    "\n",
    "# Get the action logits (as torch tensor).\n",
    "with sess.graph.as_default():\n",
    "    q_values_per_candidate = model.q_value_head([\n",
    "        np.expand_dims(obs[\"user\"], 0),\n",
    "        np.expand_dims(np.concatenate([value for value in obs[\"doc\"].values()]), 0),\n",
    "    ])\n",
    "print(f\"q_values_per_candidate={sess.run(q_values_per_candidate)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecce74b8-20ed-43c5-ad88-54a2dec32f71",
   "metadata": {
    "id": "ecce74b8-20ed-43c5-ad88-54a2dec32f71",
    "tags": []
   },
   "source": [
    "### Recap: Advantages and Disadvantages of SlateQ:\n",
    "#### Advantages:\n",
    "* Decomposes MultiDiscrete action space (better understanding of items inside a k-slate)\n",
    "* Handles long-horizon credit assignment better than bandits (Q-learning)\n",
    "* Handles > 1 user problems\n",
    "* Sample efficient (due to replay buffer + off-policy DQN-style learning)\n",
    "\n",
    "#### Disadvantages\n",
    "* Uses larger (deep) model(s): One Q-value NN head per candidate\n",
    "* Slower and heavier feel to it\n",
    "* Requires careful hyperparameter-tuning, e.g. exploration timesteps."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e00f8e5a-d8a8-451d-bb97-b2000dbb2f9d",
   "metadata": {
    "id": "e00f8e5a-d8a8-451d-bb97-b2000dbb2f9d"
   },
   "source": [
    "## Time for Q&A\n",
    "\n",
    "...\n",
    "\n",
    "## Thank you for listening and participating!\n",
    "\n",
    "### Here are a couple of links that you may find useful.\n",
    "\n",
    "- The <a href=\"https://github.com/sven1977/rllib_tutorials/tree/main/production_rl_2022\">github repo of Sven's tutorials</a>.\n",
    "- <a href=\"https://docs.ray.io/en/latest/rllib/index.html\">RLlib's documentation main page</a>.\n",
    "- <a href=\"http://discuss.ray.io\">Our discourse forum</a> to ask questions on Ray and its libraries.\n",
    "- Our <a href=\"https://forms.gle/9TSdDYUgxYs8SA9e8\">Slack channel</a> for interacting with other Ray RLlib users.\n",
    "- The <a href=\"https://github.com/ray-project/ray/blob/master/rllib/examples/\">RLlib examples scripts folder</a> with tons of examples on how to do different stuff with RLlib.\n",
    "- A <a href=\"https://medium.com/distributed-computing-with-ray/reinforcement-learning-with-rllib-in-the-unity-game-engine-1a98080a7c0d\">blog post on training with RLlib inside a Unity3D environment</a>.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "u6ej2NZcIvrv",
   "metadata": {
    "id": "u6ej2NZcIvrv"
   },
   "source": [
    "# Shutdown if you are finished, to release back the resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1b91dd8-cbb1-4c58-bd4f-8779d6a68f4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop the RLlib Trainer\n",
    "# In order to release resources that a Trainer uses, you can call its `stop()` method.\n",
    "\n",
    "slateq_trainer.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfac2bd7-8a01-4a89-a899-c5a33b7ada4c",
   "metadata": {
    "id": "dfac2bd7-8a01-4a89-a899-c5a33b7ada4c"
   },
   "outputs": [],
   "source": [
    "# if you are done with Ray\n",
    "if ray.is_initialized():\n",
    "    ray.shutdown()\n",
    "\n",
    "# # note this is how to start ray manually on your laptop\n",
    "# ray.init()\n",
    "\n",
    "# # note this is how to start ray on a cloud\n",
    "# ray.init(anyscale_cluster_name, [cluster_env | runtime_env] )"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Copy of tutorial_notebook.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
