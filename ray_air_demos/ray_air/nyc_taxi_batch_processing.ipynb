{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Batch Training with Ray Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning objectives\n",
    "In this this tutorial, you will learn about:\n",
    " * [Ray Dataset](#dataset)\n",
    " * [Batch training functions](#train_func)\n",
    " * [Ray Tune](#tune)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Batch training and tuning are common tasks in simple machine learning use-cases such as time series forecasting. They require fitting of simple models on multiple data batches corresponding to locations, products, etc. This notebook showcases how to conduct batch training using [Ray Dataset](https://docs.ray.io/en/latest/data/dataset.html), [Ray AIR Trainers](https://docs.ray.io/en/master/ray-air/trainer.html#air-trainers), and [Ray Tune](https://docs.ray.io/en/master/ray-air/tuner.html).\n",
    "\n",
    "For the data, we will use the [NYC Taxi dataset](https://www1.nyc.gov/site/tlc/about/tlc-trip-record-data.page).  This popular tabular dataset contains historical taxi pickups by timestamp and location in NYC.  <s>The goal is to predict future, hourly taxi demand by location in NYC.</s>  To demonstrate batch training & tuning, we will simplify the data to a linear regression problem to predict `trip_duration` and use Scikit-learn.\n",
    "\n",
    "To demonstrate how data and training can be batch-parallelized, we will train a separate model for each pickup location. This means we can use the pickup_location_id column in the dataset to group the dataset into data batches. Then we will fit a separate model for each batch. \n",
    "\n",
    "Let’s start by importing a few required libraries, including open-source [Ray](https://github.com/ray-project/ray) itself!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of CPUs in this system: 8\n"
     ]
    }
   ],
   "source": [
    "import os, time\n",
    "import random\n",
    "from typing import Tuple, List, Union, Optional, Callable\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pyarrow.dataset as pds\n",
    "from pyarrow import fs\n",
    "from ray.data import Dataset\n",
    "from ray.data.preprocessors import Chain, OrdinalEncoder, StandardScaler\n",
    "\n",
    "num_available_cpus = os.cpu_count()\n",
    "print(f'Number of CPUs in this system: {num_available_cpus}')\n",
    "\n",
    "# import utility functions\n",
    "import local_utils.dataprep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-12 12:42:54,283\tINFO worker.py:1509 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8266 \u001b[39m\u001b[22m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "    <div style=\"margin-left: 50px;display: flex;flex-direction: row;align-items: center\">\n",
       "        <h3 style=\"color: var(--jp-ui-font-color0)\">Ray</h3>\n",
       "        <svg version=\"1.1\" id=\"ray\" width=\"3em\" viewBox=\"0 0 144.5 144.6\" style=\"margin-left: 3em;margin-right: 3em\">\n",
       "            <g id=\"layer-1\">\n",
       "                <path fill=\"#00a2e9\" class=\"st0\" d=\"M97.3,77.2c-3.8-1.1-6.2,0.9-8.3,5.1c-3.5,6.8-9.9,9.9-17.4,9.6S58,88.1,54.8,81.2c-1.4-3-3-4-6.3-4.1\n",
       "                    c-5.6-0.1-9.9,0.1-13.1,6.4c-3.8,7.6-13.6,10.2-21.8,7.6C5.2,88.4-0.4,80.5,0,71.7c0.1-8.4,5.7-15.8,13.8-18.2\n",
       "                    c8.4-2.6,17.5,0.7,22.3,8c1.3,1.9,1.3,5.2,3.6,5.6c3.9,0.6,8,0.2,12,0.2c1.8,0,1.9-1.6,2.4-2.8c3.5-7.8,9.7-11.8,18-11.9\n",
       "                    c8.2-0.1,14.4,3.9,17.8,11.4c1.3,2.8,2.9,3.6,5.7,3.3c1-0.1,2,0.1,3,0c2.8-0.5,6.4,1.7,8.1-2.7s-2.3-5.5-4.1-7.5\n",
       "                    c-5.1-5.7-10.9-10.8-16.1-16.3C84,38,81.9,37.1,78,38.3C66.7,42,56.2,35.7,53,24.1C50.3,14,57.3,2.8,67.7,0.5\n",
       "                    C78.4-2,89,4.7,91.5,15.3c0.1,0.3,0.1,0.5,0.2,0.8c0.7,3.4,0.7,6.9-0.8,9.8c-1.7,3.2-0.8,5,1.5,7.2c6.7,6.5,13.3,13,19.8,19.7\n",
       "                    c1.8,1.8,3,2.1,5.5,1.2c9.1-3.4,17.9-0.6,23.4,7c4.8,6.9,4.6,16.1-0.4,22.9c-5.4,7.2-14.2,9.9-23.1,6.5c-2.3-0.9-3.5-0.6-5.1,1.1\n",
       "                    c-6.7,6.9-13.6,13.7-20.5,20.4c-1.8,1.8-2.5,3.2-1.4,5.9c3.5,8.7,0.3,18.6-7.7,23.6c-7.9,5-18.2,3.8-24.8-2.9\n",
       "                    c-6.4-6.4-7.4-16.2-2.5-24.3c4.9-7.8,14.5-11,23.1-7.8c3,1.1,4.7,0.5,6.9-1.7C91.7,98.4,98,92.3,104.2,86c1.6-1.6,4.1-2.7,2.6-6.2\n",
       "                    c-1.4-3.3-3.8-2.5-6.2-2.6C99.8,77.2,98.9,77.2,97.3,77.2z M72.1,29.7c5.5,0.1,9.9-4.3,10-9.8c0-0.1,0-0.2,0-0.3\n",
       "                    C81.8,14,77,9.8,71.5,10.2c-5,0.3-9,4.2-9.3,9.2c-0.2,5.5,4,10.1,9.5,10.3C71.8,29.7,72,29.7,72.1,29.7z M72.3,62.3\n",
       "                    c-5.4-0.1-9.9,4.2-10.1,9.7c0,0.2,0,0.3,0,0.5c0.2,5.4,4.5,9.7,9.9,10c5.1,0.1,9.9-4.7,10.1-9.8c0.2-5.5-4-10-9.5-10.3\n",
       "                    C72.6,62.3,72.4,62.3,72.3,62.3z M115,72.5c0.1,5.4,4.5,9.7,9.8,9.9c5.6-0.2,10-4.8,10-10.4c-0.2-5.4-4.6-9.7-10-9.7\n",
       "                    c-5.3-0.1-9.8,4.2-9.9,9.5C115,72.1,115,72.3,115,72.5z M19.5,62.3c-5.4,0.1-9.8,4.4-10,9.8c-0.1,5.1,5.2,10.4,10.2,10.3\n",
       "                    c5.6-0.2,10-4.9,9.8-10.5c-0.1-5.4-4.5-9.7-9.9-9.6C19.6,62.3,19.5,62.3,19.5,62.3z M71.8,134.6c5.9,0.2,10.3-3.9,10.4-9.6\n",
       "                    c0.5-5.5-3.6-10.4-9.1-10.8c-5.5-0.5-10.4,3.6-10.8,9.1c0,0.5,0,0.9,0,1.4c-0.2,5.3,4,9.8,9.3,10\n",
       "                    C71.6,134.6,71.7,134.6,71.8,134.6z\"/>\n",
       "            </g>\n",
       "        </svg>\n",
       "        <table>\n",
       "            <tr>\n",
       "                <td style=\"text-align: left\"><b>Python version:</b></td>\n",
       "                <td style=\"text-align: left\"><b>3.8.13</b></td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                <td style=\"text-align: left\"><b>Ray version:</b></td>\n",
       "                <td style=\"text-align: left\"><b> 2.0.0</b></td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "    <td style=\"text-align: left\"><b>Dashboard:</b></td>\n",
       "    <td style=\"text-align: left\"><b><a href=\"http://127.0.0.1:8266\" target=\"_blank\">http://127.0.0.1:8266</a></b></td>\n",
       "</tr>\n",
       "\n",
       "        </table>\n",
       "    </div>\n",
       "</div>\n"
      ],
      "text/plain": [
       "RayContext(dashboard_url='127.0.0.1:8266', python_version='3.8.13', ray_version='2.0.0', ray_commit='cba26cc83f6b5b8a2ff166594a65cb74c0ec8740', address_info={'node_ip_address': '127.0.0.1', 'raylet_ip_address': '127.0.0.1', 'redis_address': None, 'object_store_address': '/tmp/ray/session_2022-10-12_12-42-52_141685_99525/sockets/plasma_store', 'raylet_socket_name': '/tmp/ray/session_2022-10-12_12-42-52_141685_99525/sockets/raylet', 'webui_url': '127.0.0.1:8266', 'session_dir': '/tmp/ray/session_2022-10-12_12-42-52_141685_99525', 'metrics_export_port': 64028, 'gcs_address': '127.0.0.1:63653', 'address': '127.0.0.1:63653', 'dashboard_agent_listen_port': 52365, 'node_id': 'b90f0b559ac1b687594c34a6b1311ba3385a3be579a9330aea2016e3'})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ray\n",
    "if ray.is_initialized():\n",
    "    ray.shutdown()\n",
    "ray.init(ignore_reinit_error=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For benchmarking purposes, we can print the times of various operations. \n",
    "# In order to reduce clutter in the output, this is set to False by default.\n",
    "PRINT_TIMES = True\n",
    "\n",
    "def print_time(msg: str):\n",
    "    if PRINT_TIMES:\n",
    "        print(msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To speed things up, we’ll only use a small subset of the full dataset consisting of two last months of 2019. \n",
    "# You can choose to use the full dataset for 2018-2019 by setting the SMOKE_TEST variable to False.\n",
    "\n",
    "SMOKE_TEST = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data <a class=\"anchor\" id=\"dataset\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, read some data using Ray Dataset.   This will initialize a Ray cluster.  Then we can use the [Ray Dataset](https://docs.ray.io/en/latest/data/getting-started.html#datasets-getting-started) APIs to quickly inspect the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NYC Taxi using 1 file(s)!\n",
      "sample locations: [55, 235, 198]\n"
     ]
    }
   ],
   "source": [
    "# Define some global variables.\n",
    "target = \"trip_duration\"\n",
    "s3  = fs.S3FileSystem(region=\"us-east-2\")\n",
    "s3_partitions = pds.dataset(\"ursa-labs-taxi-data/\", filesystem=s3, partitioning=[\"year\", \"month\"])\n",
    "\n",
    "if SMOKE_TEST:\n",
    "    starting_idx = -1\n",
    "    sample_locations = random.sample(list(local_utils.dataprep.location_ids), 3)\n",
    "else:\n",
    "    starting_idx = -3\n",
    "    sample_locations = list(local_utils.dataprep.location_ids)\n",
    "\n",
    "s3_files = [f\"s3://{file}\" for file in s3_partitions.files][starting_idx:]\n",
    "print(f\"NYC Taxi using {len(s3_files)} file(s)!\")   \n",
    "print(f\"sample locations: {sample_locations}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-12 12:43:01,249\tWARNING read_api.py:291 -- ⚠️  The number of blocks in this dataset (1) limits its parallelism to 1 concurrent tasks. This is much less than the number of available CPU slots in the cluster. Use `.repartition(n)` to increase the number of dataset blocks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'ray.data.dataset.Dataset'>\n"
     ]
    }
   ],
   "source": [
    "# Read some Parquet files in parallel.\n",
    "rds = ray.data.read_parquet(s3_files)\n",
    "print(type(rds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number rows: 6941024\n",
      "Size bytes (from parquet metadata): 602373955\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Read progress: 100%|██████████████████████████████| 1/1 [01:23<00:00, 83.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size bytes (from full data read): 573504109\n",
      "\n",
      "Schema data types:\n",
      "vendor_id: string\n",
      "pickup_at: timestamp[us]\n",
      "dropoff_at: timestamp[us]\n",
      "passenger_count: int8\n",
      "trip_distance: float\n",
      "rate_code_id: string\n",
      "store_and_fwd_flag: string\n",
      "pickup_location_id: int32\n",
      "dropoff_location_id: int32\n",
      "payment_type: string\n",
      "fare_amount: float\n",
      "extra: float\n",
      "mta_tax: float\n",
      "tip_amount: float\n",
      "tolls_amount: float\n",
      "improvement_surcharge: float\n",
      "total_amount: float\n",
      "congestion_surcharge: float\n",
      "\n",
      "Look at a sample row:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[ArrowRow({'vendor_id': '1',\n",
       "           'pickup_at': datetime.datetime(2019, 6, 1, 0, 55, 13),\n",
       "           'dropoff_at': datetime.datetime(2019, 6, 1, 0, 56, 17),\n",
       "           'passenger_count': 1,\n",
       "           'trip_distance': 0.0,\n",
       "           'rate_code_id': '1',\n",
       "           'store_and_fwd_flag': 'N',\n",
       "           'pickup_location_id': 145,\n",
       "           'dropoff_location_id': 145,\n",
       "           'payment_type': '2',\n",
       "           'fare_amount': 3.0,\n",
       "           'extra': 0.5,\n",
       "           'mta_tax': 0.5,\n",
       "           'tip_amount': 0.0,\n",
       "           'tolls_amount': 0.0,\n",
       "           'improvement_surcharge': 0.30000001192092896,\n",
       "           'total_amount': 4.300000190734863,\n",
       "           'congestion_surcharge': 0.0})]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Parquet stores the number of rows per file in the Parquet metadata, \n",
    "# so we can get the number of rows in rds without triggering a full data read!\n",
    "print(f\"Number rows: {rds.count()}\")\n",
    "\n",
    "# Parquet pulls size-in-bytes from its metadata (not triggering a data read)\n",
    "# This could be significantly different than actual in-memory size!\n",
    "print(f\"Size bytes (from parquet metadata): {rds.size_bytes()}\")\n",
    "# Trigger full reading of the dataset and inspect the size in bytes.\n",
    "print(f\"Size bytes (from full data read): {rds.fully_executed().size_bytes()}\")\n",
    "\n",
    "# Fetch the schema from the underlying Parquet metadata.\n",
    "print(\"\\nSchema data types:\")\n",
    "data_types = list(zip(rds.schema().names, rds.schema().types))\n",
    "[print(f\"{s[0]}: {s[1]}\") for s in data_types]\n",
    "\n",
    "# Take a peek at a sample row\n",
    "print(\"\\nLook at a sample row:\")\n",
    "rds.take(1)\n",
    "\n",
    "# Number rows: 6941024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Q. Is there an easier way to get count distinct?\n",
    "\n",
    "# # Num distinct pickup location_ids\n",
    "# groupby_agg = rds.groupby(\"pickup_location_id\").mean(\"trip_distance\").take()\n",
    "# num_location_id = len(groupby_agg)\n",
    "# print(f\"Count distinct pickup location ids: {num_location_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Filter on Read - Projection and Filter Pushdown</b>\n",
    "\n",
    "Note that Ray Datasets' Parquet reader supports projection (column selection) and row filter pushdown, where we can push the above column selection and the row-based filter to the Parquet read. If we specify column selection at Parquet read time, the unselected columns won't even be read from disk!\n",
    "\n",
    "The row-based filter is specified via [Arrow's dataset field expressions](https://arrow.apache.org/docs/6.0/python/generated/pyarrow.dataset.Expression.html#pyarrow.dataset.Expression). \n",
    "\n",
    "<b>Best practice is to filter as much as you can directly in the Ray Dataset read_parquet() statement.</b>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pushdown_read_data(files_list: list,\n",
    "                       sample_ids: list) -> Dataset:\n",
    "    filter_expr = (\n",
    "        (pds.field(\"passenger_count\") > 0)\n",
    "        & (pds.field(\"trip_distance\") > 0)\n",
    "        & (pds.field(\"fare_amount\") > 0)\n",
    "        & (~pds.field(\"pickup_location_id\").isin([264, 265]))\n",
    "        & (~pds.field(\"dropoff_location_id\").isin([264, 265]))\n",
    "        & (pds.field(\"pickup_location_id\").isin(sample_ids))\n",
    "    )\n",
    "\n",
    "    the_dataset = ray.data.read_parquet(\n",
    "        files_list,\n",
    "        columns=[\n",
    "            'pickup_at', 'dropoff_at', \n",
    "            'pickup_location_id', 'dropoff_location_id',\n",
    "            'passenger_count', 'trip_distance', 'fare_amount'], \n",
    "        filter=filter_expr,\n",
    "    )\n",
    "\n",
    "    # Force full execution of both of the file reads.\n",
    "    the_dataset = the_dataset.fully_executed()\n",
    "    return the_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-12 12:44:30,019\tWARNING read_api.py:291 -- ⚠️  The number of blocks in this dataset (1) limits its parallelism to 1 concurrent tasks. This is much less than the number of available CPU slots in the cluster. Use `.repartition(n)` to increase the number of dataset blocks.\n",
      "Read progress: 100%|██████████████████████████████| 1/1 [00:40<00:00, 40.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number rows: 737\n",
      "\n",
      "Metadata: \n",
      "Dataset(num_blocks=1, num_rows=737, schema={pickup_at: timestamp[us], dropoff_at: timestamp[us], pickup_location_id: int32, dropoff_location_id: int32, passenger_count: int8, trip_distance: float, fare_amount: float})\n",
      "\n",
      "Schema:\n",
      "pickup_at: timestamp[us]\n",
      "dropoff_at: timestamp[us]\n",
      "pickup_location_id: int32\n",
      "dropoff_location_id: int32\n",
      "passenger_count: int8\n",
      "trip_distance: float\n",
      "fare_amount: float\n",
      "-- schema metadata --\n",
      "pandas: '{\"index_columns\": [{\"kind\": \"range\", \"name\": null, \"start\": 0, \"' + 2548\n",
      "\n",
      "Look at a sample row:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[ArrowRow({'pickup_at': datetime.datetime(2019, 6, 1, 0, 19, 59),\n",
       "           'dropoff_at': datetime.datetime(2019, 6, 1, 0, 21, 53),\n",
       "           'pickup_location_id': 235,\n",
       "           'dropoff_location_id': 243,\n",
       "           'passenger_count': 1,\n",
       "           'trip_distance': 0.6399999856948853,\n",
       "           'fare_amount': 4.0})]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test the pushdown_read_data function\n",
    "pushdown_ds = pushdown_read_data(s3_files, sample_locations)\n",
    "\n",
    "print(f\"Number rows: {pushdown_ds.count()}\")\n",
    "# Display some metadata about the dataset.\n",
    "print(\"\\nMetadata: \")\n",
    "print(pushdown_ds)\n",
    "# Fetch the schema from the underlying Parquet metadata.\n",
    "print(\"\\nSchema:\")\n",
    "print(pushdown_ds.schema())\n",
    "# Take a peek at a single row\n",
    "print(\"\\nLook at a sample row:\")\n",
    "pushdown_ds.take(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    trip_distance\n",
      "pickup_location_id               \n",
      "55                            196\n",
      "198                           253\n",
      "235                           288\n"
     ]
    }
   ],
   "source": [
    "# check sampling\n",
    "df = pushdown_ds.to_pandas(limit=pushdown_ds.count())\n",
    "print(df[[\"pickup_location_id\", \"trip_distance\"]].groupby(\"pickup_location_id\").count())\n",
    "\n",
    "# # How many ids in all the data?\n",
    "# df = rds.to_pandas(limit=rds.count())\n",
    "# print(\"\\nCount distinct location_ids in original data\")\n",
    "# print(df[[\"pickup_location_id\", \"trip_distance\"]].groupby(\"pickup_location_id\").count().shape[0])\n",
    "# # print(df[[\"pickup_location_id\", \"trip_distance\"]].groupby(\"pickup_location_id\").count())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Custom data transform functions</b>\n",
    "\n",
    "Ray Datasets allows you to specify custom data transform functions using familiar syntax, such as Pandas.  These <b>custom functions, or UDFs,</b> can be called using `rds.map_batches(my_UDF, batch_format=\"pandas\")`.  It is necessary to specify the language you are using the `batch_format parameter`.\n",
    "\n",
    "TODO: Reference link for syntax supported in Datasets UDFs <br>\n",
    "TODO: Mention chaining UDFs using [BatchMapper](https://docs.ray.io/en/latest/ray-air/check-ingest.html) <br>\n",
    "TODO: Add standard scaler step here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normally there is some data exploration to determine the cleaning steps.  Let's just assume we know the data cleaning steps are:\n",
    "- Drop negative trip distances, 0 fares, 0 passengers, less than 1min trip durations\n",
    "- Drop 2 unknown zones ['264', '265']\n",
    "- Calculate trip duration in minutes and add it as a new column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A Pandas DataFrame UDF for transforming the underlying blocks of a Dataset in parallel.\n",
    "def transform_batch(the_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = the_df.copy()    \n",
    "    df[\"trip_duration\"] = (df[\"dropoff_at\"] - df[\"pickup_at\"]).dt.seconds    \n",
    "    df = df[df[\"trip_duration\"] > 60]    \n",
    "    df.drop([\"dropoff_at\", \"pickup_at\", \"dropoff_location_id\"], axis=1, inplace=True)\n",
    "    df['pickup_location_id'] = df['pickup_location_id'].fillna(-1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before transform number rows: 737\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map_Batches: 100%|████████████████████████████████| 1/1 [00:00<00:00,  7.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After transform number rows: 721\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Test the transform UDF function\n",
    "print(f\"Before transform number rows: {pushdown_ds.count()}\")\n",
    "\n",
    "# batch_format=\"pandas\" tells Datasets to provide the transformer with blocks\n",
    "# represented as Pandas DataFrames.\n",
    "pushdown_ds = pushdown_ds.map_batches(transform_batch, batch_format=\"pandas\")\n",
    "\n",
    "# verify row count\n",
    "pushdown_rows = pushdown_ds.count()\n",
    "print(f\"After transform number rows: {pushdown_rows}\")\n",
    "\n",
    "# Looks good. Replace ds with pushdown\n",
    "rds = pushdown_ds\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Random shuffle</b>\n",
    "\n",
    "Randomly shuffling data is an important part of training machine learning models: it decorrelates samples, preventing overfitting and improving generalization. For many models, even between-epoch shuffling can drastically improve the precision gain per step/epoch. Datasets has a hyper-scalable distributed random shuffle that allows you to realize the model accuracy benefits of per-epoch shuffling without sacrificing training throughput, even at large data scales and even when doing distributed data-parallel training across multiple GPUs/nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Shuffle Map: 100%|████████████████████████████████| 1/1 [00:00<00:00, 49.06it/s]\n",
      "Shuffle Reduce: 100%|████████████████████████████| 1/1 [00:00<00:00, 150.67it/s]\n"
     ]
    }
   ],
   "source": [
    "# do a full global random shuffle to decorrelate the data\n",
    "rds = rds.random_shuffle()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete data to free up memory in our Ray cluster\n",
    "del rds\n",
    "del pushdown_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Tidying up</b>\n",
    "\n",
    "To make our code easier to read, let's summarize the data processing functions again here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pushdown_read_data(files_list: list,\n",
    "                       sample_ids: list) -> Dataset:\n",
    "    \n",
    "    start = time.time()\n",
    "    \n",
    "    filter_expr = (\n",
    "        (pds.field(\"passenger_count\") > 0)\n",
    "        & (pds.field(\"trip_distance\") > 0)\n",
    "        & (pds.field(\"fare_amount\") > 0)\n",
    "        & (~pds.field(\"pickup_location_id\").isin([264, 265]))\n",
    "        & (~pds.field(\"dropoff_location_id\").isin([264, 265]))\n",
    "        & (pds.field(\"pickup_location_id\").isin(sample_ids))\n",
    "    )\n",
    "\n",
    "    the_dataset = ray.data.read_parquet(\n",
    "        files_list,\n",
    "        columns=[\n",
    "            'pickup_at', 'dropoff_at', \n",
    "            'pickup_location_id', 'dropoff_location_id',\n",
    "            'passenger_count', 'trip_distance', 'fare_amount'], \n",
    "        filter=filter_expr,\n",
    "    )\n",
    "\n",
    "    # Force full execution of both of the file reads.\n",
    "    the_dataset = the_dataset.fully_executed()\n",
    "    \n",
    "    data_loading_time = time.time() - start\n",
    "    print_time(f\"Data loading time: {data_loading_time:.2f} seconds\")\n",
    "    return the_dataset\n",
    "\n",
    "# A Pandas DataFrame UDF for transforming the underlying blocks of a Dataset in parallel.\n",
    "def transform_batch(the_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    start = time.time()\n",
    "    \n",
    "    df = the_df.copy()    \n",
    "    df[\"trip_duration\"] = (df[\"dropoff_at\"] - df[\"pickup_at\"]).dt.seconds    \n",
    "    df = df[df[\"trip_duration\"] > 60]    \n",
    "    df.drop([\"dropoff_at\", \"pickup_at\", \"dropoff_location_id\"], axis=1, inplace=True)\n",
    "    df['pickup_location_id'] = df['pickup_location_id'].fillna(-1)\n",
    "    \n",
    "    data_transform_time = time.time() - start\n",
    "    # print_time(f\"Data transform time: {data_transform_time:.2f} seconds\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define batch training functions <a class=\"anchor\" id=\"train_func\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've learned more about our data and we have cleaned our data, we now look at how we can feed this dataset into some model trainers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q. scikit-learn is supposed to be pre-installed, but might have to run this in terminal !?\n",
    "# python3 -m pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn.base import BaseEstimator \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from ray.train.sklearn import SklearnTrainer, SklearnPredictor\n",
    "from ray.train.batch_predictor import BatchPredictor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Define training functions</b>\n",
    "\n",
    "- TODO: double-check Core batch example to make sure using same metrics!\n",
    "- TODO: Add more explanations here for each function.\n",
    "\n",
    "We define a `fit_and_score_sklearn` actor, where each Scikit-learn training task will consume a dataset shard in batches.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ray task to fit and score a scikit-learn model.\n",
    "@ray.remote\n",
    "def fit_and_score_sklearn(\n",
    "    train_df: pd.DataFrame, test_df: pd.DataFrame, model: BaseEstimator\n",
    ") -> Tuple[BaseEstimator, float]:\n",
    "    \n",
    "    # Assemble train/test pandas dfs\n",
    "    train_X = train_df[[\"passenger_count\", \"trip_distance\", \"fare_amount\"]]\n",
    "    train_y = train_df.trip_duration\n",
    "    test_X = test_df[[\"passenger_count\", \"trip_distance\", \"fare_amount\"]]\n",
    "    test_y = test_df.trip_duration\n",
    "    \n",
    "    # Start training.\n",
    "    model = model.fit(train_X, train_y)\n",
    "    pred_y = model.predict(test_X)\n",
    "    error = sklearn.metrics.mean_absolute_error(test_y, pred_y)\n",
    "    \n",
    "    return str(model), error\n",
    "\n",
    "def train_and_evaluate(\n",
    "    the_df: pd.DataFrame, \n",
    "    models: List[BaseEstimator]\n",
    ") -> List[Tuple[BaseEstimator, float]]:\n",
    "    \n",
    "    # check if input df is big enough for training\n",
    "    if len(the_df) < 4:\n",
    "        print(f\"Dataframe for LocID: {i} is empty or smaller than 4\")\n",
    "        return None\n",
    "    else:\n",
    "        loc_id = the_df.pickup_location_id[0]\n",
    "        # print(f\"Processing location {loc_id}...\")\n",
    "    \n",
    "    start = time.time()\n",
    "\n",
    "    # Train / test split\n",
    "    # Randomly split the data into 80/20 train/test.\n",
    "    train_df, test_df = train_test_split(the_df, test_size=0.2)\n",
    "    \n",
    "    # We put the train & test dataframes into Ray object store\n",
    "    # so that they can be reused by all models fitted here.\n",
    "    # https://docs.ray.io/en/latest/ray-core/tips-for-first-time.html#tip-3-avoid-passing-same-object-repeatedly-to-remote-tasks\n",
    "    train_ref = ray.put(train_df)\n",
    "    test_ref = ray.put(test_df)\n",
    "\n",
    "    # Launch a fit and score task for each model.\n",
    "    results = ray.get(\n",
    "        [fit_and_score_sklearn.remote(train_ref, test_ref, model) for model in models]\n",
    "    )\n",
    "    # results.sort(key=lambda x: x[1])  # sort by error\n",
    "    \n",
    "    # Assemble name of model and metrics in a pandas DataFrame\n",
    "    results = [loc_id] + list(results[0])\n",
    "    results_return = pd.DataFrame(columns=['location_id', 'model', 'error'])\n",
    "    results_return.loc[0] = results\n",
    "\n",
    "    training_time = time.time() - start\n",
    "    print_time(f\"Training time for LocID {loc_id}: {training_time:.2f} seconds\")\n",
    "    \n",
    "    return results_return\n",
    "\n",
    "def agg_func(the_df: pd.DataFrame):\n",
    "    \n",
    "    models = [LinearRegression()]\n",
    "    \n",
    "    # transform the input pandas AND fit_and_evaluate the transformed pandas\n",
    "    ret = train_and_evaluate(transform_batch(the_df), models)\n",
    "    \n",
    "    # print(f\"agg_func returned type: {type(ret)}\")\n",
    "    return ret\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Main driver code</b>\n",
    "\n",
    "During groupby, each grouped dataset can be mapped to a custom aggregation function.  Using the pattern [groupby-map_groups(agg_func, \"pands\")](https://docs.ray.io/en/latest/data/api/grouped_dataset.html).  This implements an accumulator-based aggregation.  Similar to Ray Datasets UDFs, which you learned about in the `Data` section earlier in this notebook, you can write custom aggregation functions using familiar syntax, such as Pandas. It is necessary to specify the language you are using the `batch_format` parameter.\n",
    "\n",
    "See the main driver code below for an example how `map_groups` is used with Ray Dataset to batch transform-train-fit in parallel separate shards of data.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NYC Taxi using 1 file(s)!\n",
      "sample locations: [233, 39, 85]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-12 12:45:58,248\tWARNING read_api.py:291 -- ⚠️  The number of blocks in this dataset (1) limits its parallelism to 1 concurrent tasks. This is much less than the number of available CPU slots in the cluster. Use `.repartition(n)` to increase the number of dataset blocks.\n",
      "Read progress: 100%|██████████████████████████████| 1/1 [00:36<00:00, 36.17s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loading time: 41.24 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Shuffle Map: 100%|████████████████████████████████| 1/1 [00:00<00:00, 18.82it/s]\n",
      "Shuffle Reduce: 100%|█████████████████████████████| 1/1 [00:00<00:00, 93.93it/s]\n",
      "Sort Sample: 100%|████████████████████████████████| 1/1 [00:00<00:00, 82.47it/s]\n",
      "Shuffle Map: 100%|████████████████████████████████| 1/1 [00:00<00:00, 62.90it/s]\n",
      "Shuffle Reduce: 100%|█████████████████████████████| 1/1 [00:00<00:00, 64.07it/s]\n",
      "Map_Batches: 100%|████████████████████████████████| 1/1 [00:04<00:00,  4.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "groupby.map_groups() returned type: <class 'ray.data.dataset.Dataset'>\n",
      "Total number of models: 3\n",
      "TOTAL TIME TAKEN: 46.35 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(_map_block_nosplit pid=99553)\u001b[0m Training time for LocID 39: 0.60 seconds"
     ]
    }
   ],
   "source": [
    "# Driver code to run this.\n",
    "\n",
    "SMOKE_TEST = True\n",
    "if SMOKE_TEST:\n",
    "    starting_idx = -1\n",
    "    sample_locations = random.sample(list(local_utils.dataprep.location_ids), 3)\n",
    "else:\n",
    "    starting_idx = -3\n",
    "    sample_locations = list(local_utils.dataprep.location_ids)\n",
    "\n",
    "s3_files = [f\"s3://{file}\" for file in s3_partitions.files][starting_idx:]\n",
    "print(f\"NYC Taxi using {len(s3_files)} file(s)!\")   \n",
    "print(f\"sample locations: {sample_locations}\")\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "# Read data into Ray Dataset\n",
    "rds = pushdown_read_data(s3_files, sample_locations)\n",
    "\n",
    "# Do a full global random shuffle to decorrelate the data\n",
    "rds = rds.random_shuffle()\n",
    "\n",
    "# This returns a Ray Datset\n",
    "results = rds.groupby(\"pickup_location_id\").map_groups(\n",
    "            agg_func, batch_format=\"pandas\")\n",
    "print(f\"groupby.map_groups() returned type: {type(results)}\")\n",
    "\n",
    "total_time_taken = time.time() - start\n",
    "print(f\"Total number of models: {len(sample_locations)}\")\n",
    "print_time(f\"TOTAL TIME TAKEN: {total_time_taken:.2f} seconds\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[2m\u001b[36m(_map_block_nosplit pid=99553)\u001b[0m Training time for LocID 85: 0.00 seconds\n",
      "\u001b[2m\u001b[36m(_map_block_nosplit pid=99553)\u001b[0m Training time for LocID 233: 0.02 seconds\n",
      "<class 'ray.data.dataset.Dataset'>\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>location_id</th>\n",
       "      <th>model</th>\n",
       "      <th>error</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>39</td>\n",
       "      <td>LinearRegression()</td>\n",
       "      <td>426.493309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>233</td>\n",
       "      <td>LinearRegression()</td>\n",
       "      <td>466.174030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>85</td>\n",
       "      <td>LinearRegression()</td>\n",
       "      <td>529.220534</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   location_id               model       error\n",
       "0           39  LinearRegression()  426.493309\n",
       "2          233  LinearRegression()  466.174030\n",
       "1           85  LinearRegression()  529.220534"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sort results ascending by error\n",
    "\n",
    "print(type(results))\n",
    "\n",
    "# sort values by ascending error\n",
    "results_df = results.to_pandas(limit=results.count())\n",
    "results_df.sort_values(by=[\"error\"], ascending=True, inplace=True)\n",
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Main driver code, running on all the data</b>\n",
    "\n",
    "The Smoke test worked, so now let us run the main driver code again, to batch train every location_id in parallel, with all the data files this time!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NYC Taxi using 3 file(s)!\n",
      "sample locations: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 105, 106, 107, 108, 109, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-12 12:46:46,502\tWARNING read_api.py:291 -- ⚠️  The number of blocks in this dataset (3) limits its parallelism to 3 concurrent tasks. This is much less than the number of available CPU slots in the cluster. Use `.repartition(n)` to increase the number of dataset blocks.\n",
      "Read progress: 100%|██████████████████████████████| 3/3 [01:28<00:00, 29.55s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loading time: 95.47 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Shuffle Map: 100%|████████████████████████████████| 3/3 [00:04<00:00,  1.48s/it]\n",
      "Shuffle Reduce: 100%|█████████████████████████████| 3/3 [00:11<00:00,  3.87s/it]\n",
      "Sort Sample: 100%|████████████████████████████████| 3/3 [00:00<00:00,  3.62it/s]\n",
      "Shuffle Map: 100%|████████████████████████████████| 3/3 [00:06<00:00,  2.19s/it]\n",
      "Shuffle Reduce:  67%|███████████████████▎         | 2/3 [00:01<00:01,  1.12s/it]\u001b[2m\u001b[36m(raylet)\u001b[0m Spilled 2350 MiB, 21 objects, write throughput 205 MiB/s. Set RAY_verbose_spill_logs=0 to disable this message.\n",
      "Shuffle Reduce: 100%|█████████████████████████████| 3/3 [00:03<00:00,  1.07s/it]\n",
      "Map_Batches:  33%|██████████▋                     | 1/3 [00:20<00:40, 20.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(_map_block_nosplit pid=99548)\u001b[0m Training time for LocID 263: 1.02 seconds\n",
      "\u001b[2m\u001b[36m(_map_block_nosplit pid=99552)\u001b[0m Training time for LocID 186: 0.35 seconds\n",
      "\u001b[2m\u001b[36m(_map_block_nosplit pid=99552)\u001b[0m Training time for LocID 187: 0.01 seconds\n",
      "\u001b[2m\u001b[36m(_map_block_nosplit pid=99552)\u001b[0m Training time for LocID 188: 0.02 seconds\n",
      "\u001b[2m\u001b[36m(_map_block_nosplit pid=99552)\u001b[0m Training time for LocID 189: 0.01 seconds\n",
      "\u001b[2m\u001b[36m(_map_block_nosplit pid=99552)\u001b[0m Training time for LocID 190: 0.01 seconds\n",
      "\u001b[2m\u001b[36m(_map_block_nosplit pid=99552)\u001b[0m Training time for LocID 191: 0.01 seconds\n",
      "\u001b[2m\u001b[36m(_map_block_nosplit pid=99552)\u001b[0m Training time for LocID 192: 0.01 seconds\n",
      "\u001b[2m\u001b[36m(_map_block_nosplit pid=99552)\u001b[0m Training time for LocID 193: 0.01 seconds\n",
      "\u001b[2m\u001b[36m(_map_block_nosplit pid=99552)\u001b[0m Training time for LocID 194: 0.01 seconds\n",
      "\u001b[2m\u001b[36m(_map_block_nosplit pid=99552)\u001b[0m Training time for LocID 195: 0.01 seconds\n",
      "\u001b[2m\u001b[36m(_map_block_nosplit pid=99552)\u001b[0m Training time for LocID 196: 0.01 seconds\n",
      "\u001b[2m\u001b[36m(_map_block_nosplit pid=99552)\u001b[0m Training time for LocID 197: 0.01 seconds\n",
      "\u001b[2m\u001b[36m(_map_block_nosplit pid=99552)\u001b[0m Training time for LocID 198: 0.01 seconds\n",
      "\u001b[2m\u001b[36m(_map_block_nosplit pid=99552)\u001b[0m Training time for LocID 200: 0.01 seconds\n",
      "\u001b[2m\u001b[36m(_map_block_nosplit pid=99552)\u001b[0m Training time for LocID 201: 0.01 seconds\n",
      "\u001b[2m\u001b[36m(_map_block_nosplit pid=99552)\u001b[0m Training time for LocID 202: 0.01 seconds\n",
      "\u001b[2m\u001b[36m(_map_block_nosplit pid=99552)\u001b[0m Training time for LocID 203: 0.01 seconds\n",
      "\u001b[2m\u001b[36m(_map_block_nosplit pid=99552)\u001b[0m Training time for LocID 204: 0.01 seconds\n",
      "\u001b[2m\u001b[36m(_map_block_nosplit pid=99552)\u001b[0m Training time for LocID 205: 0.01 seconds\n",
      "\u001b[2m\u001b[36m(_map_block_nosplit pid=99552)\u001b[0m Training time for LocID 206: 0.01 seconds\n",
      "\u001b[2m\u001b[36m(_map_block_nosplit pid=99552)\u001b[0m Training time for LocID 207: 0.01 seconds\n",
      "\u001b[2m\u001b[36m(_map_block_nosplit pid=99552)\u001b[0m Training time for LocID 208: 0.01 seconds\n",
      "\u001b[2m\u001b[36m(_map_block_nosplit pid=99552)\u001b[0m Training time for LocID 209: 0.01 seconds\n",
      "\u001b[2m\u001b[36m(_map_block_nosplit pid=99552)\u001b[0m Training time for LocID 210: 0.01 seconds\n",
      "\u001b[2m\u001b[36m(_map_block_nosplit pid=99552)\u001b[0m Training time for LocID 211: 0.02 seconds\n",
      "\u001b[2m\u001b[36m(_map_block_nosplit pid=99552)\u001b[0m Training time for LocID 212: 0.01 seconds\n",
      "\u001b[2m\u001b[36m(_map_block_nosplit pid=99552)\u001b[0m Training time for LocID 213: 0.01 seconds\n",
      "\u001b[2m\u001b[36m(_map_block_nosplit pid=99552)\u001b[0m Training time for LocID 214: 0.01 seconds\n",
      "\u001b[2m\u001b[36m(_map_block_nosplit pid=99552)\u001b[0m Training time for LocID 215: 0.01 seconds\n",
      "\u001b[2m\u001b[36m(_map_block_nosplit pid=99552)\u001b[0m Training time for LocID 216: 0.01 seconds\n",
      "\u001b[2m\u001b[36m(_map_block_nosplit pid=99552)\u001b[0m Training time for LocID 217: 0.01 seconds\n",
      "\u001b[2m\u001b[36m(_map_block_nosplit pid=99552)\u001b[0m Training time for LocID 218: 0.01 seconds\n",
      "\u001b[2m\u001b[36m(_map_block_nosplit pid=99552)\u001b[0m Training time for LocID 219: 0.01 seconds\n",
      "\u001b[2m\u001b[36m(_map_block_nosplit pid=99552)\u001b[0m Training time for LocID 220: 0.01 seconds\n",
      "\u001b[2m\u001b[36m(_map_block_nosplit pid=99552)\u001b[0m Training time for LocID 221: 0.01 seconds\n",
      "\u001b[2m\u001b[36m(_map_block_nosplit pid=99552)\u001b[0m Training time for LocID 222: 0.01 seconds\n",
      "\u001b[2m\u001b[36m(_map_block_nosplit pid=99552)\u001b[0m Training time for LocID 223: 0.01 seconds\n",
      "\u001b[2m\u001b[36m(_map_block_nosplit pid=99552)\u001b[0m Training time for LocID 224: 0.01 seconds\n",
      "\u001b[2m\u001b[36m(_map_block_nosplit pid=99552)\u001b[0m Training time for LocID 225: 0.01 seconds\n",
      "\u001b[2m\u001b[36m(_map_block_nosplit pid=99552)\u001b[0m Training time for LocID 226: 0.01 seconds\n",
      "\u001b[2m\u001b[36m(_map_block_nosplit pid=99552)\u001b[0m Training time for LocID 227: 0.01 seconds\n",
      "\u001b[2m\u001b[36m(_map_block_nosplit pid=99552)\u001b[0m Training time for LocID 228: 0.01 seconds\n",
      "\u001b[2m\u001b[36m(_map_block_nosplit pid=99552)\u001b[0m Training time for LocID 229: 0.05 seconds\n",
      "\u001b[2m\u001b[36m(_map_block_nosplit pid=99552)\u001b[0m Training time for LocID 230: 0.09 seconds\n",
      "\u001b[2m\u001b[36m(_map_block_nosplit pid=99552)\u001b[0m Training time for LocID 231: 0.06 seconds\n",
      "\u001b[2m\u001b[36m(_map_block_nosplit pid=99552)\u001b[0m Training time for LocID 232: 0.01 seconds\n",
      "\u001b[2m\u001b[36m(_map_block_nosplit pid=99552)\u001b[0m Training time for LocID 233: 0.05 seconds\n",
      "\u001b[2m\u001b[36m(_map_block_nosplit pid=99552)\u001b[0m Training time for LocID 234: 0.09 seconds\n",
      "\u001b[2m\u001b[36m(_map_block_nosplit pid=99552)\u001b[0m Training time for LocID 235: 0.01 seconds\n",
      "\u001b[2m\u001b[36m(_map_block_nosplit pid=99552)\u001b[0m Training time for LocID 236: 0.09 seconds\n",
      "\u001b[2m\u001b[36m(_map_block_nosplit pid=99552)\u001b[0m Training time for LocID 237: 0.11 seconds\n",
      "\u001b[2m\u001b[36m(_map_block_nosplit pid=99552)\u001b[0m Training time for LocID 238: 0.05 seconds\n",
      "\u001b[2m\u001b[36m(_map_block_nosplit pid=99552)\u001b[0m Training time for LocID 239: 0.06 seconds\n",
      "\u001b[2m\u001b[36m(_map_block_nosplit pid=99552)\u001b[0m Training time for LocID 240: 0.01 seconds\n",
      "\u001b[2m\u001b[36m(_map_block_nosplit pid=99552)\u001b[0m Training time for LocID 241: 0.01 seconds\n",
      "\u001b[2m\u001b[36m(_map_block_nosplit pid=99552)\u001b[0m Training time for LocID 242: 0.01 seconds\n",
      "\u001b[2m\u001b[36m(_map_block_nosplit pid=99552)\u001b[0m Training time for LocID 243: 0.01 seconds\n",
      "\u001b[2m\u001b[36m(_map_block_nosplit pid=99552)\u001b[0m Training time for LocID 244: 0.01 seconds\n",
      "\u001b[2m\u001b[36m(_map_block_nosplit pid=99552)\u001b[0m Training time for LocID 245: 0.01 seconds\n",
      "\u001b[2m\u001b[36m(_map_block_nosplit pid=99552)\u001b[0m Training time for LocID 246: 0.05 seconds\n",
      "\u001b[2m\u001b[36m(_map_block_nosplit pid=99552)\u001b[0m Training time for LocID 247: 0.01 seconds\n",
      "\u001b[2m\u001b[36m(_map_block_nosplit pid=99552)\u001b[0m Training time for LocID 248: 0.01 seconds\n",
      "\u001b[2m\u001b[36m(_map_block_nosplit pid=99552)\u001b[0m Training time for LocID 249: 0.05 seconds\n",
      "\u001b[2m\u001b[36m(_map_block_nosplit pid=99552)\u001b[0m Training time for LocID 250: 0.01 seconds\n",
      "\u001b[2m\u001b[36m(_map_block_nosplit pid=99552)\u001b[0m Training time for LocID 251: 0.01 seconds\n",
      "\u001b[2m\u001b[36m(_map_block_nosplit pid=99552)\u001b[0m Training time for LocID 252: 0.01 seconds\n",
      "\u001b[2m\u001b[36m(_map_block_nosplit pid=99552)\u001b[0m Training time for LocID 253: 0.01 seconds\n",
      "\u001b[2m\u001b[36m(_map_block_nosplit pid=99552)\u001b[0m Training time for LocID 254: 0.01 seconds\n",
      "\u001b[2m\u001b[36m(_map_block_nosplit pid=99552)\u001b[0m Training time for LocID 255: 0.01 seconds\n",
      "\u001b[2m\u001b[36m(_map_block_nosplit pid=99552)\u001b[0m Training time for LocID 256: 0.01 seconds\n",
      "\u001b[2m\u001b[36m(_map_block_nosplit pid=99552)\u001b[0m Training time for LocID 257: 0.01 seconds\n",
      "\u001b[2m\u001b[36m(_map_block_nosplit pid=99552)\u001b[0m Training time for LocID 258: 0.01 seconds\n",
      "\u001b[2m\u001b[36m(_map_block_nosplit pid=99552)\u001b[0m Training time for LocID 259: 0.01 seconds\n",
      "\u001b[2m\u001b[36m(_map_block_nosplit pid=99552)\u001b[0m Training time for LocID 260: 0.01 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map_Batches:  67%|████████████████████▋          | 2/3 [05:25<03:08, 188.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(_map_block_nosplit pid=99552)\u001b[0m Training time for LocID 261: 0.04 seconds\n",
      "\u001b[2m\u001b[36m(_map_block_nosplit pid=99552)\u001b[0m Training time for LocID 262: 0.03 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(_map_block_nosplit pid=99553)\u001b[0m 2022-10-12 12:58:00,840\tINFO worker.py:756 -- Task failed with retryable exception: TaskID(6efb86ef2d286c40ffffffffffffffffffffffff01000000).\n",
      "\u001b[2m\u001b[36m(_map_block_nosplit pid=99553)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[2m\u001b[36m(_map_block_nosplit pid=99553)\u001b[0m   File \"/Users/christy/miniforge3/envs/rllib/lib/python3.8/site-packages/pandas/core/indexes/base.py\", line 3629, in get_loc\n",
      "\u001b[2m\u001b[36m(_map_block_nosplit pid=99553)\u001b[0m     return self._engine.get_loc(casted_key)\n",
      "\u001b[2m\u001b[36m(_map_block_nosplit pid=99553)\u001b[0m   File \"pandas/_libs/index.pyx\", line 136, in pandas._libs.index.IndexEngine.get_loc\n",
      "\u001b[2m\u001b[36m(_map_block_nosplit pid=99553)\u001b[0m   File \"pandas/_libs/index.pyx\", line 163, in pandas._libs.index.IndexEngine.get_loc\n",
      "\u001b[2m\u001b[36m(_map_block_nosplit pid=99553)\u001b[0m   File \"pandas/_libs/hashtable_class_helper.pxi\", line 2131, in pandas._libs.hashtable.Int64HashTable.get_item\n",
      "\u001b[2m\u001b[36m(_map_block_nosplit pid=99553)\u001b[0m   File \"pandas/_libs/hashtable_class_helper.pxi\", line 2140, in pandas._libs.hashtable.Int64HashTable.get_item\n",
      "\u001b[2m\u001b[36m(_map_block_nosplit pid=99553)\u001b[0m KeyError: 0\n",
      "\u001b[2m\u001b[36m(_map_block_nosplit pid=99553)\u001b[0m \n",
      "\u001b[2m\u001b[36m(_map_block_nosplit pid=99553)\u001b[0m The above exception was the direct cause of the following exception:\n",
      "\u001b[2m\u001b[36m(_map_block_nosplit pid=99553)\u001b[0m \n",
      "\u001b[2m\u001b[36m(_map_block_nosplit pid=99553)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[2m\u001b[36m(_map_block_nosplit pid=99553)\u001b[0m   File \"python/ray/_raylet.pyx\", line 662, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(_map_block_nosplit pid=99553)\u001b[0m   File \"python/ray/_raylet.pyx\", line 666, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(_map_block_nosplit pid=99553)\u001b[0m   File \"/Users/christy/miniforge3/envs/rllib/lib/python3.8/site-packages/ray/data/_internal/compute.py\", line 449, in _map_block_nosplit\n",
      "\u001b[2m\u001b[36m(_map_block_nosplit pid=99553)\u001b[0m     for new_block in block_fn(block, *fn_args, **fn_kwargs):\n",
      "\u001b[2m\u001b[36m(_map_block_nosplit pid=99553)\u001b[0m   File \"/Users/christy/miniforge3/envs/rllib/lib/python3.8/site-packages/ray/data/dataset.py\", line 457, in transform\n",
      "\u001b[2m\u001b[36m(_map_block_nosplit pid=99553)\u001b[0m     applied = batch_fn(view, *fn_args, **fn_kwargs)\n",
      "\u001b[2m\u001b[36m(_map_block_nosplit pid=99553)\u001b[0m   File \"/Users/christy/miniforge3/envs/rllib/lib/python3.8/site-packages/ray/data/grouped_dataset.py\", line 281, in group_fn\n",
      "\u001b[2m\u001b[36m(_map_block_nosplit pid=99553)\u001b[0m     applied = fn(group)\n",
      "\u001b[2m\u001b[36m(_map_block_nosplit pid=99553)\u001b[0m   File \"/var/folders/0g/jfs_l_113_356_c0rfp4jd8c0000gn/T/ipykernel_99525/3937913345.py\", line 66, in agg_func\n",
      "\u001b[2m\u001b[36m(_map_block_nosplit pid=99553)\u001b[0m   File \"/var/folders/0g/jfs_l_113_356_c0rfp4jd8c0000gn/T/ipykernel_99525/3937913345.py\", line 30, in train_and_evaluate\n",
      "\u001b[2m\u001b[36m(_map_block_nosplit pid=99553)\u001b[0m   File \"/Users/christy/miniforge3/envs/rllib/lib/python3.8/site-packages/pandas/core/series.py\", line 958, in __getitem__\n",
      "\u001b[2m\u001b[36m(_map_block_nosplit pid=99553)\u001b[0m     return self._get_value(key)\n",
      "\u001b[2m\u001b[36m(_map_block_nosplit pid=99553)\u001b[0m   File \"/Users/christy/miniforge3/envs/rllib/lib/python3.8/site-packages/pandas/core/series.py\", line 1069, in _get_value\n",
      "\u001b[2m\u001b[36m(_map_block_nosplit pid=99553)\u001b[0m     loc = self.index.get_loc(label)\n",
      "\u001b[2m\u001b[36m(_map_block_nosplit pid=99553)\u001b[0m   File \"/Users/christy/miniforge3/envs/rllib/lib/python3.8/site-packages/pandas/core/indexes/base.py\", line 3631, in get_loc\n",
      "\u001b[2m\u001b[36m(_map_block_nosplit pid=99553)\u001b[0m     raise KeyError(key) from err\n",
      "\u001b[2m\u001b[36m(_map_block_nosplit pid=99553)\u001b[0m KeyError: 0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [23]\u001b[0m, in \u001b[0;36m<cell line: 22>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m rds \u001b[38;5;241m=\u001b[39m rds\u001b[38;5;241m.\u001b[39mrandom_shuffle()\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# This returns a Ray Datset\u001b[39;00m\n\u001b[0;32m---> 22\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mrds\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroupby\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpickup_location_id\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_groups\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[43m            \u001b[49m\u001b[43magg_func\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpandas\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgroupby.map_groups() returned type: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(results)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     26\u001b[0m total_time_taken \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start\n",
      "File \u001b[0;32m~/miniforge3/envs/rllib/lib/python3.8/site-packages/ray/data/grouped_dataset.py:290\u001b[0m, in \u001b[0;36mGroupedDataset.map_groups\u001b[0;34m(self, fn, compute, batch_format, **ray_remote_args)\u001b[0m\n\u001b[1;32m    286\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m rs\n\u001b[1;32m    288\u001b[0m \u001b[38;5;66;03m# Note we set batch_size=None here, so it will use the entire block as a batch,\u001b[39;00m\n\u001b[1;32m    289\u001b[0m \u001b[38;5;66;03m# which ensures that each group will be contained within a batch in entirety.\u001b[39;00m\n\u001b[0;32m--> 290\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msorted_ds\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_batches\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    291\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgroup_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    292\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    293\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompute\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompute\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    294\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    295\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mray_remote_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    296\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/rllib/lib/python3.8/site-packages/ray/data/dataset.py:495\u001b[0m, in \u001b[0;36mDataset.map_batches\u001b[0;34m(self, fn, batch_size, compute, batch_format, fn_args, fn_kwargs, fn_constructor_args, fn_constructor_kwargs, **ray_remote_args)\u001b[0m\n\u001b[1;32m    480\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m output_buffer\u001b[38;5;241m.\u001b[39mnext()\n\u001b[1;32m    482\u001b[0m plan \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_plan\u001b[38;5;241m.\u001b[39mwith_stage(\n\u001b[1;32m    483\u001b[0m     OneToOneStage(\n\u001b[1;32m    484\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmap_batches\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    493\u001b[0m     )\n\u001b[1;32m    494\u001b[0m )\n\u001b[0;32m--> 495\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mDataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mplan\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_epoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_lazy\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/rllib/lib/python3.8/site-packages/ray/data/dataset.py:201\u001b[0m, in \u001b[0;36mDataset.__init__\u001b[0;34m(self, plan, epoch, lazy, defer_execution)\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lazy \u001b[38;5;241m=\u001b[39m lazy\n\u001b[1;32m    200\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m lazy \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m defer_execution:\n\u001b[0;32m--> 201\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_plan\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mallow_clear_input_blocks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/rllib/lib/python3.8/site-packages/ray/data/_internal/plan.py:308\u001b[0m, in \u001b[0;36mExecutionPlan.execute\u001b[0;34m(self, allow_clear_input_blocks, force_read)\u001b[0m\n\u001b[1;32m    306\u001b[0m     clear_input_blocks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    307\u001b[0m stats_builder \u001b[38;5;241m=\u001b[39m stats\u001b[38;5;241m.\u001b[39mchild_builder(stage\u001b[38;5;241m.\u001b[39mname)\n\u001b[0;32m--> 308\u001b[0m blocks, stage_info \u001b[38;5;241m=\u001b[39m \u001b[43mstage\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    309\u001b[0m \u001b[43m    \u001b[49m\u001b[43mblocks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclear_input_blocks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_by_consumer\u001b[49m\n\u001b[1;32m    310\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    311\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m stage_info:\n\u001b[1;32m    312\u001b[0m     stats \u001b[38;5;241m=\u001b[39m stats_builder\u001b[38;5;241m.\u001b[39mbuild_multistage(stage_info)\n",
      "File \u001b[0;32m~/miniforge3/envs/rllib/lib/python3.8/site-packages/ray/data/_internal/plan.py:662\u001b[0m, in \u001b[0;36mOneToOneStage.__call__\u001b[0;34m(self, blocks, clear_input_blocks, run_by_consumer)\u001b[0m\n\u001b[1;32m    657\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m blocks\u001b[38;5;241m.\u001b[39m_owned_by_consumer:\n\u001b[1;32m    658\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m (\n\u001b[1;32m    659\u001b[0m         run_by_consumer\n\u001b[1;32m    660\u001b[0m     ), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBlocks owned by consumer can only be consumed by consumer\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 662\u001b[0m blocks \u001b[38;5;241m=\u001b[39m \u001b[43mcompute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    663\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mblock_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    664\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mray_remote_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    665\u001b[0m \u001b[43m    \u001b[49m\u001b[43mblocks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    666\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclear_input_blocks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    667\u001b[0m \u001b[43m    \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    668\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    669\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfn_args\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfn_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    670\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfn_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfn_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    671\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfn_constructor_args\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfn_constructor_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    672\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfn_constructor_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfn_constructor_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    673\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    674\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(blocks, BlockList), blocks\n\u001b[1;32m    675\u001b[0m blocks\u001b[38;5;241m.\u001b[39m_owned_by_consumer \u001b[38;5;241m=\u001b[39m run_by_consumer\n",
      "File \u001b[0;32m~/miniforge3/envs/rllib/lib/python3.8/site-packages/ray/data/_internal/compute.py:126\u001b[0m, in \u001b[0;36mTaskPoolStrategy._apply\u001b[0;34m(self, block_fn, remote_args, block_list, clear_input_blocks, name, fn, fn_args, fn_kwargs, fn_constructor_args, fn_constructor_kwargs)\u001b[0m\n\u001b[1;32m    124\u001b[0m             \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m    125\u001b[0m     \u001b[38;5;66;03m# Reraise the original task failure exception.\u001b[39;00m\n\u001b[0;32m--> 126\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m    128\u001b[0m new_blocks, new_metadata \u001b[38;5;241m=\u001b[39m [], []\n\u001b[1;32m    129\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m context\u001b[38;5;241m.\u001b[39mblock_splitting_enabled:\n",
      "File \u001b[0;32m~/miniforge3/envs/rllib/lib/python3.8/site-packages/ray/data/_internal/compute.py:113\u001b[0m, in \u001b[0;36mTaskPoolStrategy._apply\u001b[0;34m(self, block_fn, remote_args, block_list, clear_input_blocks, name, fn, fn_args, fn_kwargs, fn_constructor_args, fn_constructor_kwargs)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;66;03m# Common wait for non-data refs.\u001b[39;00m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 113\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[43mmap_bar\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch_until_complete\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrefs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (ray\u001b[38;5;241m.\u001b[39mexceptions\u001b[38;5;241m.\u001b[39mRayTaskError, \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;66;03m# One or more mapper tasks failed, or we received a SIGINT signal\u001b[39;00m\n\u001b[1;32m    116\u001b[0m     \u001b[38;5;66;03m# while waiting; either way, we cancel all map tasks.\u001b[39;00m\n\u001b[1;32m    117\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m ref \u001b[38;5;129;01min\u001b[39;00m refs:\n",
      "File \u001b[0;32m~/miniforge3/envs/rllib/lib/python3.8/site-packages/ray/data/_internal/progress_bar.py:74\u001b[0m, in \u001b[0;36mProgressBar.fetch_until_complete\u001b[0;34m(self, refs)\u001b[0m\n\u001b[1;32m     72\u001b[0m t \u001b[38;5;241m=\u001b[39m threading\u001b[38;5;241m.\u001b[39mcurrent_thread()\n\u001b[1;32m     73\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m remaining:\n\u001b[0;32m---> 74\u001b[0m     done, remaining \u001b[38;5;241m=\u001b[39m \u001b[43mray\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mremaining\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfetch_local\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     75\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m ref, result \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(done, ray\u001b[38;5;241m.\u001b[39mget(done)):\n\u001b[1;32m     76\u001b[0m         ref_to_result[ref] \u001b[38;5;241m=\u001b[39m result\n",
      "File \u001b[0;32m~/miniforge3/envs/rllib/lib/python3.8/site-packages/ray/_private/client_mode_hook.py:105\u001b[0m, in \u001b[0;36mclient_mode_hook.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    103\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m func\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minit\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m is_client_mode_enabled_by_default:\n\u001b[1;32m    104\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(ray, func\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 105\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/rllib/lib/python3.8/site-packages/ray/_private/worker.py:2461\u001b[0m, in \u001b[0;36mwait\u001b[0;34m(object_refs, num_returns, timeout, fetch_local)\u001b[0m\n\u001b[1;32m   2459\u001b[0m timeout \u001b[38;5;241m=\u001b[39m timeout \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m10\u001b[39m \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;241m6\u001b[39m\n\u001b[1;32m   2460\u001b[0m timeout_milliseconds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(timeout \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m1000\u001b[39m)\n\u001b[0;32m-> 2461\u001b[0m ready_ids, remaining_ids \u001b[38;5;241m=\u001b[39m \u001b[43mworker\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcore_worker\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2462\u001b[0m \u001b[43m    \u001b[49m\u001b[43mobject_refs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2463\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_returns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2464\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout_milliseconds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2465\u001b[0m \u001b[43m    \u001b[49m\u001b[43mworker\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcurrent_task_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2466\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfetch_local\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2467\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2468\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ready_ids, remaining_ids\n",
      "File \u001b[0;32mpython/ray/_raylet.pyx:1414\u001b[0m, in \u001b[0;36mray._raylet.CoreWorker.wait\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpython/ray/_raylet.pyx:173\u001b[0m, in \u001b[0;36mray._raylet.check_status\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Driver code to run this.\n",
    "\n",
    "SMOKE_TEST = False\n",
    "if SMOKE_TEST:\n",
    "    starting_idx = -1\n",
    "    sample_locations = random.sample(list(local_utils.dataprep.location_ids), 3)\n",
    "else:\n",
    "    starting_idx = -3\n",
    "    sample_locations = list(local_utils.dataprep.location_ids)\n",
    "\n",
    "s3_files = [f\"s3://{file}\" for file in s3_partitions.files][starting_idx:]\n",
    "print(f\"NYC Taxi using {len(s3_files)} file(s)!\")   \n",
    "print(f\"sample locations: {sample_locations}\")\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "# Read data into Ray Dataset\n",
    "rds = pushdown_read_data(s3_files, sample_locations)\n",
    "\n",
    "# Do a full global random shuffle to decorrelate the data\n",
    "rds = rds.random_shuffle()\n",
    "\n",
    "# This returns a Ray Datset\n",
    "results = rds.groupby(\"pickup_location_id\").map_groups(\n",
    "            agg_func, batch_format=\"pandas\")\n",
    "print(f\"groupby.map_groups() returned type: {type(results)}\")\n",
    "\n",
    "total_time_taken = time.time() - start\n",
    "print(f\"Total number of models: {len(sample_locations)}\")\n",
    "print_time(f\"TOTAL TIME TAKEN: {total_time_taken:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'ray.data.dataset.Dataset'>\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>location_id</th>\n",
       "      <th>model</th>\n",
       "      <th>error</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>39</td>\n",
       "      <td>LinearRegression()</td>\n",
       "      <td>426.493309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>233</td>\n",
       "      <td>LinearRegression()</td>\n",
       "      <td>466.174030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>85</td>\n",
       "      <td>LinearRegression()</td>\n",
       "      <td>529.220534</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   location_id               model       error\n",
       "0           39  LinearRegression()  426.493309\n",
       "2          233  LinearRegression()  466.174030\n",
       "1           85  LinearRegression()  529.220534"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sort results ascending by error\n",
    "\n",
    "print(type(results))\n",
    "\n",
    "# sort values by ascending error\n",
    "results_df = results.to_pandas(limit=results.count())\n",
    "results_df.sort_values(by=[\"error\"], ascending=True, inplace=True)\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
