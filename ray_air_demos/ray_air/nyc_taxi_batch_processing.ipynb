{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Batch Training on Ray AIR/Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning objectives\n",
    "In this this tutorial, you will learn about:\n",
    " * [Ray Dataset](#dataset)\n",
    " * [AIR Trainer](#trainer)\n",
    " * [Ray Tune](#tune)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Batch training and tuning are common tasks in simple machine learning use-cases such as time series forecasting. They require fitting of simple models on multiple data batches corresponding to locations, products, etc. This notebook showcases how to conduct batch training using [Ray Dataset](https://docs.ray.io/en/latest/data/dataset.html), [Ray AIR Trainers](https://docs.ray.io/en/master/ray-air/trainer.html#air-trainers), and [Ray Tune](https://docs.ray.io/en/master/ray-air/tuner.html).\n",
    "\n",
    "For the data, we will use the [NYC Taxi dataset](https://www1.nyc.gov/site/tlc/about/tlc-trip-record-data.page).  This popular tabular dataset contains historical taxi pickups by timestamp and location in NYC.  <s>The goal is to predict future, hourly taxi demand by location in NYC.</s>  To demonstrate batch training & tuning, we will simplify the data to a linear regression problem to predict `trip_duration`) and use Scikit-learn.\n",
    "\n",
    "To demonstrate how data and training can be batch-parallelized, we will train a separate model for each pickup location. This means we can use the pickup_location_id column in the dataset to group the dataset into data batches. Then we will fit a separate model for each batch. \n",
    "\n",
    "Let’s start by importing a few required libraries, including open-source [Ray](https://github.com/ray-project/ray) itself!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of CPUs in this system: 8\n"
     ]
    }
   ],
   "source": [
    "import sys, os\n",
    "import pandas as pd\n",
    "# import numpy as np\n",
    "import random\n",
    "import pyarrow as pa\n",
    "import pyarrow.dataset as pads\n",
    "import ray\n",
    "from ray.data import Dataset\n",
    "\n",
    "# import utility functions\n",
    "import local_utils.dataprep\n",
    "\n",
    "num_available_cpus = os.cpu_count()\n",
    "print(f'Number of CPUs in this system: {num_available_cpus}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data <a class=\"anchor\" id=\"dataset\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, read some data using Ray Dataset.   This will initialize a Ray cluster.  Then we can use the [Ray Dataset](https://docs.ray.io/en/latest/data/getting-started.html#datasets-getting-started) APIs to quickly inspect the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To speed things up, we’ll only use a small subset of the full dataset consisting of two last months of 2019. \n",
    "# You can choose to use the full dataset for 2018-2019 by setting the SMOKE_TEST variable to False.\n",
    "\n",
    "SMOKE_TEST = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obtained 2 files!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['s3://air-example-data/ursa-labs-taxi-data/by_year/2019/05/data.parquet/359c21b3e28f40328e68cf66f7ba40e2_000000.parquet',\n",
       " 's3://air-example-data/ursa-labs-taxi-data/by_year/2019/06/data.parquet/ab5b9d2b8cc94be19346e260b543ec35_000000.parquet']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read some Parquet files in parallel.\n",
    "\n",
    "dataset = pads.dataset(\n",
    "    \"s3://anonymous@air-example-data/ursa-labs-taxi-data/by_year/\",\n",
    "    partitioning=[\"year\", \"month\"],\n",
    ")\n",
    "starting_idx = -2 if SMOKE_TEST else 0\n",
    "\n",
    "data_files = [f\"s3://{file}\" for file in dataset.files][starting_idx:]\n",
    "print(f\"Obtained {len(data_files)} files!\")\n",
    "data_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-09 12:49:50,903\tINFO worker.py:1223 -- Using address localhost:9031 set in the environment variable RAY_ADDRESS\n",
      "2022-10-09 12:49:54,199\tINFO worker.py:1333 -- Connecting to existing Ray cluster at address: 172.31.113.136:9031...\n",
      "2022-10-09 12:49:54,208\tINFO worker.py:1509 -- Connected to Ray cluster. View the dashboard at \u001b[1m\u001b[32mhttps://console.anyscale-staging.com/api/v2/sessions/ses_H1JcaTJ4HC9zz1U32BnfSwYa/services?redirect_to=dashboard \u001b[39m\u001b[22m\n",
      "2022-10-09 12:49:54,397\tINFO packaging.py:342 -- Pushing file package 'gcs://_ray_pkg_bf59b22914c31152b1df18d55f17ac18.zip' (63.00MiB) to Ray cluster...\n",
      "2022-10-09 12:49:55,425\tINFO packaging.py:351 -- Successfully pushed file package 'gcs://_ray_pkg_bf59b22914c31152b1df18d55f17ac18.zip'.\n",
      "2022-10-09 12:49:57,406\tWARNING read_api.py:291 -- ⚠️  The number of blocks in this dataset (1) limits its parallelism to 1 concurrent tasks. This is much less than the number of available CPU slots in the cluster. Use `.repartition(n)` to increase the number of dataset blocks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'ray.data.dataset.Dataset'>\n"
     ]
    }
   ],
   "source": [
    "# Read some Parquet files in parallel.\n",
    "\n",
    "# TODO get rid of this sampling later\n",
    "data_files = data_files[0]\n",
    "\n",
    "ds = ray.data.read_parquet(data_files)\n",
    "print(type(ds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number rows: 7565261\n",
      "Size bytes (from parquet metadata): 1002980150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Read progress: 100%|██████████| 1/1 [00:03<00:00,  3.13s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size bytes (from full data read): 625081194\n",
      "\n",
      "Schema data types:\n",
      "vendor_id: string\n",
      "pickup_at: timestamp[us]\n",
      "dropoff_at: timestamp[us]\n",
      "passenger_count: int8\n",
      "trip_distance: float\n",
      "rate_code_id: string\n",
      "store_and_fwd_flag: string\n",
      "pickup_location_id: int32\n",
      "dropoff_location_id: int32\n",
      "payment_type: string\n",
      "fare_amount: float\n",
      "extra: float\n",
      "mta_tax: float\n",
      "tip_amount: float\n",
      "tolls_amount: float\n",
      "improvement_surcharge: float\n",
      "total_amount: float\n",
      "congestion_surcharge: float\n",
      "\n",
      "Look at a sample row:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[ArrowRow({'vendor_id': '1',\n",
       "           'pickup_at': datetime.datetime(2019, 5, 1, 0, 14, 50),\n",
       "           'dropoff_at': datetime.datetime(2019, 5, 1, 0, 16, 48),\n",
       "           'passenger_count': 1,\n",
       "           'trip_distance': 0.0,\n",
       "           'rate_code_id': '1',\n",
       "           'store_and_fwd_flag': 'N',\n",
       "           'pickup_location_id': 145,\n",
       "           'dropoff_location_id': 145,\n",
       "           'payment_type': '2',\n",
       "           'fare_amount': 3.0,\n",
       "           'extra': 0.5,\n",
       "           'mta_tax': 0.5,\n",
       "           'tip_amount': 0.0,\n",
       "           'tolls_amount': 0.0,\n",
       "           'improvement_surcharge': 0.30000001192092896,\n",
       "           'total_amount': 4.300000190734863,\n",
       "           'congestion_surcharge': 0.0})]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Parquet stores the number of rows per file in the Parquet metadata, \n",
    "# so we can get the number of rows in ds without triggering a full data read!\n",
    "print(f\"Number rows: {ds.count()}\")\n",
    "\n",
    "# Parquet pulls size-in-bytes from its metadata (not triggering a data read)\n",
    "# This could be significantly different than actual in-memory size!\n",
    "print(f\"Size bytes (from parquet metadata): {ds.size_bytes()}\")\n",
    "# Trigger full reading of the dataset and inspect the size in bytes.\n",
    "print(f\"Size bytes (from full data read): {ds.fully_executed().size_bytes()}\")\n",
    "\n",
    "# Fetch the schema from the underlying Parquet metadata.\n",
    "print(\"\\nSchema data types:\")\n",
    "data_types = list(zip(ds.schema().names, ds.schema().types))\n",
    "[print(f\"{s[0]}: {s[1]}\") for s in data_types]\n",
    "\n",
    "# Take a peek at a sample row\n",
    "print(\"\\nLook at a sample row:\")\n",
    "ds.take(1)\n",
    "\n",
    "# Number rows: 7565261"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normally there is some data exploration to determine the cleaning steps.  Let's just assume we know the data cleaning steps are:\n",
    "- Drop negative trip distances, 0 fares, 0 passengers, less than 1min trip durations\n",
    "- Drop 2 unknown zones ['264', '265']\n",
    "- Calculate trip duration in minutes and add it as a new column\n",
    "- Groupby, aggregate sum taxi rides, hourly per pickup location\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7565261\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map_Batches: 100%|██████████| 1/1 [00:24<00:00, 24.52s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final number rows: 7328413\n"
     ]
    }
   ],
   "source": [
    "# A Pandas DataFrame UDF for transforming the underlying blocks of a Dataset in parallel.\n",
    "def transform_batch_scratch(the_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = the_df.copy()\n",
    "    df[\"trip_duration\"] = (df[\"dropoff_at\"] - df[\"pickup_at\"]).dt.seconds\n",
    "    df = df[df[\"trip_distance\"] > 0]\n",
    "    df = df[df[\"fare_amount\"] > 0]\n",
    "    df = df[df[\"passenger_count\"] > 0]\n",
    "    df = df[df[\"trip_duration\"] >= 60]\n",
    "    return df\n",
    "\n",
    "# batch_format=\"pandas\" tells Datasets to provide the transformer with blocks\n",
    "# represented as Pandas DataFrames.\n",
    "print(ds.count())\n",
    "ds = ds.map_batches(transform_batch_scratch, batch_format=\"pandas\")\n",
    "\n",
    "# verify row count\n",
    "ds_rows = ds.count()\n",
    "print(f\"Final number rows: {ds_rows}\")\n",
    "\n",
    "# 236,848 rows were dropped with that cleaning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Q. Is there an easier way to get count distinct?\n",
    "\n",
    "# # Num distinct pickup location_ids\n",
    "# groupby_agg = ds.groupby(\"pickup_location_id\").mean(\"trip_distance\").take()\n",
    "# num_location_id = len(groupby_agg)\n",
    "# print(f\"Count distinct pickup location ids: {num_location_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7, 18, 17]\n"
     ]
    }
   ],
   "source": [
    "# This data has simplified location_ids 1 ... 20\n",
    "sample_locations = list(range(1, 21))\n",
    "\n",
    "if SMOKE_TEST:\n",
    "    sample_locations = random.sample(sample_locations, 3)\n",
    "    \n",
    "print(sample_locations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Filter on Read - Projection and Filter Pushdown</b>\n",
    "\n",
    "Note that Ray Datasets' Parquet reader supports projection (column selection) and row filter pushdown, where we can push the above column selection and the row-based filter to the Parquet read. If we specify column selection at Parquet read time, the unselected columns won't even be read from disk!\n",
    "\n",
    "The row-based filter is specified via [Arrow's dataset field expressions](https://arrow.apache.org/docs/6.0/python/generated/pyarrow.dataset.Expression.html#pyarrow.dataset.Expression). \n",
    "\n",
    "<b>Best practice is to filter as much as you can directly in the Ray Dataset read_parquet() statement.</b>\n",
    "\n",
    "TODO: repartition data to 2 CPU\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pushdown_read_data(files_list: list,\n",
    "                       sample_ids: list) -> Dataset:\n",
    "    filter_expr = (\n",
    "        (pads.field(\"passenger_count\") > 0)\n",
    "        & (pads.field(\"trip_distance\") > 0)\n",
    "        & (pads.field(\"fare_amount\") > 0)\n",
    "        & (pads.field(\"pickup_location_id\").isin(sample_ids))\n",
    "    )\n",
    "\n",
    "    the_dataset = ray.data.read_parquet(\n",
    "        files_list,\n",
    "        columns=[\n",
    "            'pickup_at', 'dropoff_at', 'pickup_location_id',\n",
    "            'passenger_count', 'trip_distance', 'fare_amount'], \n",
    "        filter=filter_expr,\n",
    "    )\n",
    "\n",
    "    # Force full execution of both of the file reads.\n",
    "    the_dataset = the_dataset.fully_executed()\n",
    "    return the_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-09 12:54:18,500\tWARNING read_api.py:291 -- ⚠️  The number of blocks in this dataset (1) limits its parallelism to 1 concurrent tasks. This is much less than the number of available CPU slots in the cluster. Use `.repartition(n)` to increase the number of dataset blocks.\n",
      "Read progress: 100%|██████████| 1/1 [00:01<00:00,  1.71s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number rows: 11495\n",
      "\n",
      "Metadata: \n",
      "Dataset(num_blocks=1, num_rows=11495, schema={pickup_at: timestamp[us], dropoff_at: timestamp[us], pickup_location_id: int32, passenger_count: int8, trip_distance: float, fare_amount: float})\n",
      "\n",
      "Schema:\n",
      "pickup_at: timestamp[us]\n",
      "dropoff_at: timestamp[us]\n",
      "pickup_location_id: int32\n",
      "passenger_count: int8\n",
      "trip_distance: float\n",
      "fare_amount: float\n",
      "-- schema metadata --\n",
      "pandas: '{\"index_columns\": [{\"kind\": \"range\", \"name\": null, \"start\": 0, \"' + 2548\n",
      "\n",
      "Look at a sample row:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[ArrowRow({'pickup_at': datetime.datetime(2019, 5, 1, 0, 7, 43),\n",
       "           'dropoff_at': datetime.datetime(2019, 5, 1, 0, 10, 56),\n",
       "           'pickup_location_id': 7,\n",
       "           'passenger_count': 1,\n",
       "           'trip_distance': 0.8600000143051147,\n",
       "           'fare_amount': 4.5})]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test the pushdown_read_data function\n",
    "pushdown_ds = pushdown_read_data(data_files, sample_locations)\n",
    "\n",
    "print(f\"Number rows: {pushdown_ds.count()}\")\n",
    "# Display some metadata about the dataset.\n",
    "print(\"\\nMetadata: \")\n",
    "print(pushdown_ds)\n",
    "# Fetch the schema from the underlying Parquet metadata.\n",
    "print(\"\\nSchema:\")\n",
    "print(pushdown_ds.schema())\n",
    "# Take a peek at a single row\n",
    "print(\"\\nLook at a sample row:\")\n",
    "pushdown_ds.take(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    trip_distance\n",
      "pickup_location_id               \n",
      "7                           10143\n",
      "17                           1199\n",
      "18                            153\n",
      "\n",
      "Count distinct location_ids in original data\n",
      "261\n",
      "                    trip_distance\n",
      "pickup_location_id               \n",
      "1                              61\n",
      "2                              17\n",
      "3                             135\n",
      "4                           12215\n",
      "5                              14\n",
      "...                           ...\n",
      "261                         41438\n",
      "262                         90327\n",
      "263                        134458\n",
      "264                         56325\n",
      "265                          1660\n",
      "\n",
      "[261 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "# check sampling\n",
    "# Q. Some sort of mapping is happening for location_ids ?\n",
    "df = pushdown_ds.to_pandas()\n",
    "print(df[[\"pickup_location_id\", \"trip_distance\"]].groupby(\"pickup_location_id\").count())\n",
    "\n",
    "df = ds.to_pandas(limit=ds.count())\n",
    "# How many ids in all the data?\n",
    "print(\"\\nCount distinct location_ids in original data\")\n",
    "print(df[[\"pickup_location_id\", \"trip_distance\"]].groupby(\"pickup_location_id\").count().shape[0])\n",
    "print(df[[\"pickup_location_id\", \"trip_distance\"]].groupby(\"pickup_location_id\").count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Custom data transform functions</b>\n",
    "\n",
    "Ray Datasets allows you to specify custom data transform functions using familiar syntax, such as Pandas.  These <b>custom functions, or UDFs,</b> can be called using `ds.map_batches(my_UDF, batch_format=\"pandas\")`.  It is necessary to specify the language you are using the `batch_format parameter`.\n",
    "\n",
    "TODO: Reference link for syntax supported in Datasets UDFs <br>\n",
    "TODO: Mention chaining UDFs using [BatchMapper](https://docs.ray.io/en/latest/ray-air/check-ingest.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A Pandas DataFrame UDF for transforming the underlying blocks of a Dataset in parallel.\n",
    "def transform_batch(the_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = the_df.copy()    \n",
    "    df[\"trip_duration\"] = (df[\"dropoff_at\"] - df[\"pickup_at\"]).dt.seconds    \n",
    "    df = df[df[\"trip_duration\"] >= 60]    \n",
    "    df.drop([\"dropoff_at\", \"pickup_at\"], axis=1, inplace=True)\n",
    "    df['pickup_location_id'] = df['pickup_location_id'].fillna(-1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final number rows: 11398\n"
     ]
    }
   ],
   "source": [
    "# Test the transform UDF function\n",
    "print(pushdown_ds.count())\n",
    "\n",
    "# batch_format=\"pandas\" tells Datasets to provide the transformer with blocks\n",
    "# represented as Pandas DataFrames.\n",
    "pushdown_ds = pushdown_ds.map_batches(transform_batch, batch_format=\"pandas\")\n",
    "\n",
    "# verify row count\n",
    "pushdown_rows = pushdown_ds.count()\n",
    "print(f\"Final number rows: {pushdown_rows}\")\n",
    "\n",
    "# Looks good. Replace ds with pushdown\n",
    "ds = pushdown_ds\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Random shuffle</b>\n",
    "\n",
    "Randomly shuffling data is an important part of training machine learning models: it decorrelates samples, preventing overfitting and improving generalization. For many models, even between-epoch shuffling can drastically improve the precision gain per step/epoch. Datasets has a hyper-scalable distributed random shuffle that allows you to realize the model accuracy benefits of per-epoch shuffling without sacrificing training throughput, even at large data scales and even when doing distributed data-parallel training across multiple GPUs/nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Shuffle Map: 100%|██████████| 1/1 [00:00<00:00, 91.34it/s]\n",
      "Shuffle Reduce: 100%|██████████| 1/1 [00:00<00:00, 113.94it/s]\n"
     ]
    }
   ],
   "source": [
    "# do a full global random shuffle to decorrelate the data\n",
    "ds = ds.random_shuffle()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Split data into train/valid/test </b> \n",
    "\n",
    "We are ready to split the data into train/valid/test.  For now, we will just randomly split the data into 80/20 train/test.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map_Batches: 100%|██████████| 1/1 [00:00<00:00, 105.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number rows train, test: 9118, 2280\n"
     ]
    }
   ],
   "source": [
    "target = \"trip_duration\"\n",
    "\n",
    "# Split data into train and validation.\n",
    "train_ds, valid_ds = ds.train_test_split(test_size=0.2)\n",
    "\n",
    "# Create a test dataset by dropping the target column.\n",
    "test_ds = valid_ds.drop_columns(cols=[target])\n",
    "\n",
    "assert train_ds.count() + valid_ds.count() == ds.count()\n",
    "print(f\"Number rows train, test: \", end=\"\")\n",
    "print(f\"{train_ds.count()}, {test_ds.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete data to free up memory in our Ray cluster\n",
    "del ds\n",
    "del train_ds\n",
    "del valid_ds\n",
    "del test_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Tidying up</b>\n",
    "\n",
    "To make our code more modular and easier to read, let's put all those data processing steps into a single function called `prepare_data()`.  See [scripts](../scripts/dataprep.py) for the full code.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q. Seeing errors calling included functions this way?\n",
    "\n",
    "# # Single function call for preparing data using Ray Dataset\n",
    "# train_ds, valid_ds, test_ds = local_utils.dataprep.prepare_data(data_files, target)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "def prepare_data(files_list: list, \n",
    "                 target: str, \n",
    "                 sample_locations: list) -> Tuple[Dataset, Dataset, Dataset]:\n",
    "    # Pushdown - filter on read data from parquet\n",
    "    the_dataset = pushdown_read_data(files_list, sample_locations)\n",
    "    \n",
    "    # Perform transformation using pandas UDF `transform_batch`\n",
    "    the_dataset = the_dataset.map_batches(\n",
    "            transform_batch, \n",
    "            batch_format=\"pandas\")   \n",
    "        \n",
    "    # Perform a global shuffle\n",
    "    the_dataset = the_dataset.random_shuffle()\n",
    "    \n",
    "    # Split data into train/valid\n",
    "    train_dataset, valid_dataset = \\\n",
    "        the_dataset.train_test_split(test_size=0.2)\n",
    "    \n",
    "    # Create test data same as valid\n",
    "    test_dataset = valid_dataset.drop_columns([target])\n",
    "    \n",
    "    # Return train, valid, test\n",
    "    return train_dataset, valid_dataset, test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-09 12:57:04,648\tWARNING read_api.py:291 -- ⚠️  The number of blocks in this dataset (1) limits its parallelism to 1 concurrent tasks. This is much less than the number of available CPU slots in the cluster. Use `.repartition(n)` to increase the number of dataset blocks.\n",
      "Read progress: 100%|██████████| 1/1 [00:01<00:00,  1.50s/it]\n",
      "Map_Batches: 100%|██████████| 1/1 [00:00<00:00, 32.97it/s]\n",
      "Shuffle Map: 100%|██████████| 1/1 [00:00<00:00, 152.11it/s]\n",
      "Shuffle Reduce: 100%|██████████| 1/1 [00:00<00:00, 147.95it/s]\n",
      "Map_Batches: 100%|██████████| 1/1 [00:00<00:00, 79.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number rows train, test: 9118, 2280\n"
     ]
    }
   ],
   "source": [
    "# Test the prepare_data function\n",
    "target = \"trip_duration\"\n",
    "train_ds, valid_ds, test_ds = prepare_data(data_files, target, sample_locations)\n",
    "\n",
    "print(f\"Number rows train, test: \", end=\"\")\n",
    "print(f\"{train_ds.count()}, {test_ds.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PandasRow({'pickup_location_id': 7,\n",
       "            'passenger_count': 1,\n",
       "            'trip_distance': 1.159999966621399,\n",
       "            'fare_amount': 7.5})]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_ds.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AIR Trainer <a class=\"anchor\" id=\"trainer\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ray AI Runtime (AIR) is a scalable and unified toolkit for ML applications.  AIR builds on Ray’s best-in-class libraries for Preprocessing, Training, Tuning, Scoring, Serving, and Reinforcement Learning to bring together an ecosystem of integrations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Scaling</b>\n",
    "\n",
    "By default, Dataset tasks use all available cluster CPU resources for execution. This can sometimes conflict with Trainer resource requests. For example, if Trainers allocate all CPU resources in the cluster, then no Datasets tasks can run.\n",
    "\n",
    "A good rule of thumb, if you know you need to do other things besides Train, is to reserve a couple CPUs for those other purposes.\n",
    "\n",
    "TODO:  Better explanation!  This doesn't quite make sense yet.  Trainer output looks like it used 2 cpu, even though 5 were specified, why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of CPUs in this system: 8\n",
      "Dataset CPUs: 2, Trainer CPUs: 5\n"
     ]
    }
   ],
   "source": [
    "print(f'Number of CPUs in this system: {num_available_cpus}')\n",
    "\n",
    "# decide how many processors to use for training\n",
    "# always reserve 1 cpu for the Ray head node\n",
    "num_datasets_cpus = 2\n",
    "num_training_cpus = num_available_cpus - num_datasets_cpus - 1\n",
    "print(f\"Dataset CPUs: {num_datasets_cpus}, Trainer CPUs: {num_training_cpus}\")\n",
    "\n",
    "# assign resources for AIR Trainer\n",
    "trainer_resources = {\"CPU\": num_training_cpus}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Training preprocessor</b>\n",
    "\n",
    "After data preparation, often there are special transformations required per algorithm.  For example:\n",
    "- One-hot encoding for categorical variables\n",
    "- Variable encoding for categorical variables\n",
    "- Standard scaler for numeric variables\n",
    "\n",
    "You can pass these preprocessors to a trainer. Ray Train will take care of applying the preprocessor to the dataset in a distributed fashion.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a preprocessor to scale some columns.\n",
    "from ray.data.preprocessors import Chain, OrdinalEncoder, StandardScaler\n",
    "\n",
    "preprocessor = StandardScaler(\n",
    "    columns=[\"passenger_count\", \"fare_amount\", ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AIR Trainer\n",
    "\n",
    "Ray Train has several Trainer classes that make it possible to do distributed training. Trainers are wrapper classes around third-party training frameworks like Scikit-learn, XGBoost, LightGBM, HuggingFace, Tensorflow, Horovod, Pytorch, RLlib, and more. AIR Trainers provide integration with core Ray actors (for distribution), Tune, and Dataset.\n",
    "\n",
    "Q. Below why do I see 2 outputs for Train?  I expected 5 outputs because of 5 parallel CPUs for Training? <br>\n",
    "Q. Why do you have to explicitly do ray.init() before train but you don't have to for Datasets?\n",
    "\n",
    "Tip: Below, you might want to turn on scrolling for the output. In the cell below, right-click, select Enable Scrolling for Outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/bin/python3: No module named pip\n"
     ]
    }
   ],
   "source": [
    "# Not sure why, but might have to run this in terminal!\n",
    "# python3 -m pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-10-09 13:03:44 (running for 00:00:04.96)<br>Memory usage on this node: 5.4/30.9 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/8 CPUs, 0/0 GPUs, 0.0/16.72 GiB heap, 0.0/8.36 GiB objects<br>Result logdir: /home/ray/ray_results/SklearnTrainer_2022-10-09_13-03-38<br>Number of trials: 1/1 (1 TERMINATED)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status    </th><th>loc                 </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  fit_time</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>SklearnTrainer_77252_00000</td><td>TERMINATED</td><td>172.31.113.136:11001</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         2.72673</td><td style=\"text-align: right;\">   1.16454</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for SklearnTrainer_77252_00000:\n",
      "  cv:\n",
      "    fit_time: [0.0037877559661865234, 0.0030989646911621094, 0.002981901168823242, 0.0030295848846435547,\n",
      "      0.003016948699951172]\n",
      "    fit_time_mean: 0.0031830310821533204\n",
      "    fit_time_std: 0.0003047430651245634\n",
      "    score_time: [0.0014061927795410156, 0.0013928413391113281, 0.001375436782836914,\n",
      "      0.001344442367553711, 0.0013527870178222656]\n",
      "    score_time_mean: 0.0013743400573730468\n",
      "    score_time_std: 2.3308803112227476e-05\n",
      "    test_score: [0.011420282770100765, 0.1048737937677513, 0.0862853510099334, 0.030362331879895788,\n",
      "      0.014760074659894928]\n",
      "    test_score_mean: 0.04954036681751524\n",
      "    test_score_std: 0.03858131297863796\n",
      "  date: 2022-10-09_13-03-44\n",
      "  done: false\n",
      "  experiment_id: c3dd0da438c44e9f9adf0cdd32babc11\n",
      "  fit_time: 1.1645383834838867\n",
      "  hostname: ip-172-31-113-136\n",
      "  iterations_since_restore: 1\n",
      "  node_ip: 172.31.113.136\n",
      "  pid: 11001\n",
      "  should_checkpoint: true\n",
      "  time_since_restore: 2.726732015609741\n",
      "  time_this_iter_s: 2.726732015609741\n",
      "  time_total_s: 2.726732015609741\n",
      "  timestamp: 1665345824\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 1\n",
      "  trial_id: '77252_00000'\n",
      "  valid:\n",
      "    score_time: 0.0017538070678710938\n",
      "    test_score: 0.02429958051266823\n",
      "  warmup_time: 0.0041773319244384766\n",
      "  \n",
      "Result for SklearnTrainer_77252_00000:\n",
      "  cv:\n",
      "    fit_time: [0.0037877559661865234, 0.0030989646911621094, 0.002981901168823242, 0.0030295848846435547,\n",
      "      0.003016948699951172]\n",
      "    fit_time_mean: 0.0031830310821533204\n",
      "    fit_time_std: 0.0003047430651245634\n",
      "    score_time: [0.0014061927795410156, 0.0013928413391113281, 0.001375436782836914,\n",
      "      0.001344442367553711, 0.0013527870178222656]\n",
      "    score_time_mean: 0.0013743400573730468\n",
      "    score_time_std: 2.3308803112227476e-05\n",
      "    test_score: [0.011420282770100765, 0.1048737937677513, 0.0862853510099334, 0.030362331879895788,\n",
      "      0.014760074659894928]\n",
      "    test_score_mean: 0.04954036681751524\n",
      "    test_score_std: 0.03858131297863796\n",
      "  date: 2022-10-09_13-03-44\n",
      "  done: true\n",
      "  experiment_id: c3dd0da438c44e9f9adf0cdd32babc11\n",
      "  experiment_tag: '0'\n",
      "  fit_time: 1.1645383834838867\n",
      "  hostname: ip-172-31-113-136\n",
      "  iterations_since_restore: 1\n",
      "  node_ip: 172.31.113.136\n",
      "  pid: 11001\n",
      "  should_checkpoint: true\n",
      "  time_since_restore: 2.726732015609741\n",
      "  time_this_iter_s: 2.726732015609741\n",
      "  time_total_s: 2.726732015609741\n",
      "  timestamp: 1665345824\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 1\n",
      "  trial_id: '77252_00000'\n",
      "  valid:\n",
      "    score_time: 0.0017538070678710938\n",
      "    test_score: 0.02429958051266823\n",
      "  warmup_time: 0.0041773319244384766\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-09 13:03:45,009\tINFO tune.py:758 -- Total run time: 6.11 seconds (4.96 seconds for the tuning loop).\n"
     ]
    }
   ],
   "source": [
    "import sklearn\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from ray.train.sklearn import SklearnTrainer, SklearnPredictor\n",
    "from ray.air.config import ScalingConfig\n",
    "from ray.air.result import Result\n",
    "\n",
    "trainer = SklearnTrainer(\n",
    "    # SKlearn specific params\n",
    "    estimator=LinearRegression(),\n",
    "    # Scaling params\n",
    "    scaling_config=ScalingConfig(trainer_resources=trainer_resources),\n",
    "    label_column=target,\n",
    "    cv=5,\n",
    "    # Ray Datasets to use for train/valid\n",
    "    datasets={\"train\": train_ds, \"valid\": valid_ds},\n",
    "    # Ray Datasets preprocessor before training\n",
    "    preprocessor=preprocessor,\n",
    ")\n",
    "result = trainer.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def train_sklearn(num_cpus: int, use_gpu: bool = False) -> Result:\n",
    "#     if use_gpu and not cuMLRandomForestClassifier:\n",
    "#         raise RuntimeError(\"cuML must be installed for GPU enabled sklearn estimators.\")\n",
    "\n",
    "#     train_dataset, valid_dataset, _ = prepare_data()\n",
    "\n",
    "#     # Scale some random columns\n",
    "#     columns_to_scale = [\"mean radius\", \"mean texture\"]\n",
    "#     preprocessor = Chain(\n",
    "#         OrdinalEncoder([\"categorical_column\"]), StandardScaler(columns=columns_to_scale)\n",
    "#     )\n",
    "\n",
    "#     if use_gpu:\n",
    "#         trainer_resources = {\"CPU\": 1, \"GPU\": 1}\n",
    "#         estimator = cuMLRandomForestClassifier()\n",
    "#     else:\n",
    "#         trainer_resources = {\"CPU\": num_cpus}\n",
    "#         estimator = RandomForestClassifier()\n",
    "\n",
    "#     trainer = SklearnTrainer(\n",
    "#         estimator=estimator,\n",
    "#         label_column=\"target\",\n",
    "#         datasets={\"train\": train_dataset, \"valid\": valid_dataset},\n",
    "#         preprocessor=preprocessor,\n",
    "#         cv=5,\n",
    "#         scaling_config=ScalingConfig(trainer_resources=trainer_resources),\n",
    "#     )\n",
    "#     result = trainer.fit()\n",
    "#     print(result.metrics)\n",
    "\n",
    "#     return result\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
