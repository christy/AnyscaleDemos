{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batch Training with Ray Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning objectives\n",
    "In this this tutorial, you will learn about:\n",
    " * [Ray Dataset](#dataset)\n",
    " * [Batch training with Ray Dataset](#train_func)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Batch training and tuning are common tasks in simple machine learning use-cases such as time series forecasting. They require fitting of simple models on multiple data batches corresponding to locations, products, etc. \n",
    "\n",
    "Batch training in the context of this notebook is understood as creating the same model(s) for different and separate datasets or subsets of a dataset. This notebook showcases how to conduct batch training using [Ray Dataset](https://docs.ray.io/en/latest/data/dataset.html).\n",
    "\n",
    "<img src=\"./images/embarrassingly_parallel.png\" width=\"50%\" />\n",
    "\n",
    "For the data, we will use the [NYC Taxi dataset](https://www1.nyc.gov/site/tlc/about/tlc-trip-record-data.page).  This popular tabular dataset contains historical taxi pickups by timestamp and location in NYC.  <s>The goal is to predict future, hourly taxi demand by location in NYC.</s>  To demonstrate batch training & tuning, we will simplify the data to a linear regression problem to predict `trip_duration` and use Scikit-learn.\n",
    "\n",
    "To demonstrate how data and training can be batch-parallelized, we will train a separate model for each dropoff location. This means we can use the dropoff_location_id column in the dataset to group the dataset into data batches. Then we will fit a separate model for each batch. \n",
    "\n",
    "Let’s start by importing a few required libraries, including open-source [Ray](https://github.com/ray-project/ray) itself!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of CPUs in this system: 8\n"
     ]
    }
   ],
   "source": [
    "import os, time\n",
    "import random\n",
    "from typing import Tuple, List, Union, Optional, Callable\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pyarrow.dataset as pds\n",
    "from pyarrow import fs\n",
    "from ray.data import Dataset\n",
    "from ray.data.preprocessors import Chain, OrdinalEncoder, StandardScaler\n",
    "\n",
    "num_available_cpus = os.cpu_count()\n",
    "print(f'Number of CPUs in this system: {num_available_cpus}')\n",
    "\n",
    "# import utility functions\n",
    "import local_utils.dataprep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.8.10\n",
      "ray: 2.0.0\n"
     ]
    }
   ],
   "source": [
    "import ray\n",
    "\n",
    "!python3 --version\n",
    "print(f\"ray: {ray.__version__}\")\n",
    "\n",
    "# if ray.is_initialized():\n",
    "#     ray.shutdown()\n",
    "# ray.init(ignore_reinit_error=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For benchmarking purposes, we can print the times of various operations. \n",
    "# In order to reduce clutter in the output, this is set to False by default.\n",
    "PRINT_TIMES = True\n",
    "\n",
    "def print_time(msg: str):\n",
    "    if PRINT_TIMES:\n",
    "        print(msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To speed things up, we’ll only use a small subset of the full dataset consisting of two last months of 2019. \n",
    "# You can choose to use the full dataset for 2018-2019 by setting the SMOKE_TEST variable to False.\n",
    "\n",
    "SMOKE_TEST = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ray Dataset <a class=\"anchor\" id=\"dataset\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, read some data using Ray Dataset.   This will initialize a Ray cluster.  Then we can use the [Ray Dataset](https://docs.ray.io/en/latest/data/getting-started.html#datasets-getting-started) APIs to quickly inspect the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NYC Taxi using 1 file(s)!\n",
      "s3_files: ['s3://ursa-labs-taxi-data/2019/06/data.parquet']\n",
      "sample locations: [111, 181, 108]\n"
     ]
    }
   ],
   "source": [
    "# Define some global variables.\n",
    "target = \"trip_duration\"\n",
    "s3  = fs.S3FileSystem(region=\"us-east-2\")\n",
    "s3_partitions = pds.dataset(\"ursa-labs-taxi-data/\", filesystem=s3, partitioning=[\"year\", \"month\"])\n",
    "\n",
    "if SMOKE_TEST:\n",
    "    starting_idx = -1\n",
    "    sample_locations = random.sample(list(local_utils.dataprep.location_ids), 3)\n",
    "else:\n",
    "    starting_idx = -3\n",
    "    sample_locations = list(local_utils.dataprep.location_ids)\n",
    "\n",
    "s3_files = [f\"s3://{file}\" for file in s3_partitions.files][starting_idx:]\n",
    "print(f\"NYC Taxi using {len(s3_files)} file(s)!\")\n",
    "print(f\"s3_files: {s3_files}\")\n",
    "print(f\"sample locations: {sample_locations}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-12 20:44:20,042\tINFO worker.py:1223 -- Using address localhost:9031 set in the environment variable RAY_ADDRESS\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Git repo detected in sub-directory AnyscaleDemos. Please ensure that your git repo is cloned in the top-level workspace directory with 'git clone <repo> .' at /home/ray/default.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [10]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Read some Parquet files in parallel.\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m rds \u001b[38;5;241m=\u001b[39m \u001b[43mray\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_parquet\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms3_files\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mtype\u001b[39m(rds))\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/ray/data/read_api.py:368\u001b[0m, in \u001b[0;36mread_parquet\u001b[0;34m(paths, filesystem, columns, parallelism, ray_remote_args, tensor_column_schema, meta_provider, **arrow_parquet_args)\u001b[0m\n\u001b[1;32m    333\u001b[0m \u001b[38;5;124;03m\"\"\"Create an Arrow dataset from parquet files.\u001b[39;00m\n\u001b[1;32m    334\u001b[0m \n\u001b[1;32m    335\u001b[0m \u001b[38;5;124;03mExamples:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    362\u001b[0m \u001b[38;5;124;03m    Dataset holding Arrow records read from the specified paths.\u001b[39;00m\n\u001b[1;32m    363\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    364\u001b[0m arrow_parquet_args \u001b[38;5;241m=\u001b[39m _resolve_parquet_args(\n\u001b[1;32m    365\u001b[0m     tensor_column_schema,\n\u001b[1;32m    366\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39marrow_parquet_args,\n\u001b[1;32m    367\u001b[0m )\n\u001b[0;32m--> 368\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mread_datasource\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    369\u001b[0m \u001b[43m    \u001b[49m\u001b[43mParquetDatasource\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    370\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparallelism\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparallelism\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    371\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpaths\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpaths\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    372\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfilesystem\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilesystem\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    373\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    374\u001b[0m \u001b[43m    \u001b[49m\u001b[43mray_remote_args\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mray_remote_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    375\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmeta_provider\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmeta_provider\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    376\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43marrow_parquet_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    377\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/ray/data/read_api.py:246\u001b[0m, in \u001b[0;36mread_datasource\u001b[0;34m(datasource, parallelism, ray_remote_args, **read_args)\u001b[0m\n\u001b[1;32m    244\u001b[0m \u001b[38;5;66;03m# TODO(ekl) remove this feature flag.\u001b[39;00m\n\u001b[1;32m    245\u001b[0m force_local \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRAY_DATASET_FORCE_LOCAL_METADATA\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m os\u001b[38;5;241m.\u001b[39menviron\n\u001b[0;32m--> 246\u001b[0m cur_pg \u001b[38;5;241m=\u001b[39m \u001b[43mray\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mutil\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_current_placement_group\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    247\u001b[0m pa_ds \u001b[38;5;241m=\u001b[39m _lazy_import_pyarrow_dataset()\n\u001b[1;32m    248\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pa_ds:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/ray/util/placement_group.py:315\u001b[0m, in \u001b[0;36mget_current_placement_group\u001b[0;34m()\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[38;5;129m@PublicAPI\u001b[39m\n\u001b[1;32m    285\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_current_placement_group\u001b[39m() \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Optional[PlacementGroup]:\n\u001b[1;32m    286\u001b[0m     \u001b[38;5;124;03m\"\"\"Get the current placement group which a task or actor is using.\u001b[39;00m\n\u001b[1;32m    287\u001b[0m \n\u001b[1;32m    288\u001b[0m \u001b[38;5;124;03m    It returns None if there's no current placement group for the worker.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    313\u001b[0m \u001b[38;5;124;03m            created with any placement group.\u001b[39;00m\n\u001b[1;32m    314\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 315\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mclient_mode_should_convert\u001b[49m\u001b[43m(\u001b[49m\u001b[43mauto_init\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m:\n\u001b[1;32m    316\u001b[0m         \u001b[38;5;66;03m# Client mode is only a driver.\u001b[39;00m\n\u001b[1;32m    317\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    318\u001b[0m     worker \u001b[38;5;241m=\u001b[39m ray\u001b[38;5;241m.\u001b[39m_private\u001b[38;5;241m.\u001b[39mworker\u001b[38;5;241m.\u001b[39mglobal_worker\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/ray/_private/client_mode_hook.py:124\u001b[0m, in \u001b[0;36mclient_mode_should_convert\u001b[0;34m(auto_init)\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mray\u001b[39;00m\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    121\u001b[0m         os\u001b[38;5;241m.\u001b[39menviron\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRAY_ENABLE_AUTO_CONNECT\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m0\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    122\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m ray\u001b[38;5;241m.\u001b[39mis_initialized()\n\u001b[1;32m    123\u001b[0m     ):\n\u001b[0;32m--> 124\u001b[0m         \u001b[43mray\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minit\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    126\u001b[0m \u001b[38;5;66;03m# `is_client_mode_enabled_by_default` is used for testing with\u001b[39;00m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;66;03m# `RAY_CLIENT_MODE=1`. This flag means all tests run with client mode.\u001b[39;00m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[1;32m    129\u001b[0m     is_client_mode_enabled \u001b[38;5;129;01mor\u001b[39;00m is_client_mode_enabled_by_default\n\u001b[1;32m    130\u001b[0m ) \u001b[38;5;129;01mand\u001b[39;00m _get_client_hook_status_on_thread()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/ray/_private/client_mode_hook.py:105\u001b[0m, in \u001b[0;36mclient_mode_hook.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    103\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m func\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minit\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m is_client_mode_enabled_by_default:\n\u001b[1;32m    104\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(ray, func\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 105\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/ray/_private/worker.py:1315\u001b[0m, in \u001b[0;36minit\u001b[0;34m(address, num_cpus, num_gpus, resources, object_store_memory, local_mode, ignore_reinit_error, include_dashboard, dashboard_host, dashboard_port, job_config, configure_logging, logging_level, logging_format, log_to_driver, namespace, runtime_env, storage, **kwargs)\u001b[0m\n\u001b[1;32m   1311\u001b[0m \u001b[38;5;66;03m# RAY_JOB_CONFIG_JSON_ENV_VAR is only set at ray job manager level and has\u001b[39;00m\n\u001b[1;32m   1312\u001b[0m \u001b[38;5;66;03m# higher priority in case user also provided runtime_env for ray.init()\u001b[39;00m\n\u001b[1;32m   1313\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1314\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ray_constants\u001b[38;5;241m.\u001b[39mRAY_RUNTIME_ENV_HOOK \u001b[38;5;129;01min\u001b[39;00m os\u001b[38;5;241m.\u001b[39menviron \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _skip_env_hook:\n\u001b[0;32m-> 1315\u001b[0m         runtime_env \u001b[38;5;241m=\u001b[39m \u001b[43m_load_class\u001b[49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menviron\u001b[49m\u001b[43m[\u001b[49m\u001b[43mray_constants\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mRAY_RUNTIME_ENV_HOOK\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1316\u001b[0m \u001b[43m            \u001b[49m\u001b[43mruntime_env\u001b[49m\n\u001b[1;32m   1317\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1319\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m runtime_env:\n\u001b[1;32m   1320\u001b[0m         \u001b[38;5;66;03m# Set runtime_env in job_config if passed in as part of ray.init()\u001b[39;00m\n\u001b[1;32m   1321\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m job_config \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/anyscale/snapshot_util.py:429\u001b[0m, in \u001b[0;36menv_hook\u001b[0;34m(runtime_env)\u001b[0m\n\u001b[1;32m    427\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m runtime_env\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mworking_dir\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m os\u001b[38;5;241m.\u001b[39mlistdir(WORKING_DIR):\n\u001b[1;32m    428\u001b[0m     optimize_git_repo(WORKING_DIR, EFS_OBJECTS_DIR)\n\u001b[0;32m--> 429\u001b[0m     zipfile \u001b[38;5;241m=\u001b[39m \u001b[43mget_or_create_snapshot_zip\u001b[49m\u001b[43m(\u001b[49m\u001b[43mWORKING_DIR\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mauto\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    430\u001b[0m     \u001b[38;5;66;03m# Move the zip file to a consistent path so that we don't leak zip files as\u001b[39;00m\n\u001b[1;32m    431\u001b[0m     \u001b[38;5;66;03m# jobs are run over time. This isn't thread safe: we assume one job at a time.\u001b[39;00m\n\u001b[1;32m    432\u001b[0m     \u001b[38;5;66;03m# Note that the zip file isn't needed after the job starts successfully.\u001b[39;00m\n\u001b[1;32m    433\u001b[0m     final_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/tmp/ray_latest_runtime_env.zip\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/anyscale/snapshot_util.py:175\u001b[0m, in \u001b[0;36mget_or_create_snapshot_zip\u001b[0;34m(directory, auto)\u001b[0m\n\u001b[1;32m    170\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_or_create_snapshot_zip\u001b[39m(directory: \u001b[38;5;28mstr\u001b[39m, auto: \u001b[38;5;28mbool\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[1;32m    171\u001b[0m     \u001b[38;5;124;03m\"\"\"Create a snapshot zip, or return the last snapshot if unchanged.\u001b[39;00m\n\u001b[1;32m    172\u001b[0m \n\u001b[1;32m    173\u001b[0m \u001b[38;5;124;03m    A corresponding .md5 file is created alongside the snapshot zip.\u001b[39;00m\n\u001b[1;32m    174\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 175\u001b[0m     new_zip \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_snapshot_zip\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdirectory\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mauto\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    176\u001b[0m     new_hash \u001b[38;5;241m=\u001b[39m compute_content_hash(new_zip)\n\u001b[1;32m    177\u001b[0m     \u001b[38;5;66;03m# Ignore the base snapshot in auto save mode. This means we will always generate\u001b[39;00m\n\u001b[1;32m    178\u001b[0m     \u001b[38;5;66;03m# a new snapshot within the autosave interval, allowing the base snapshot to be\u001b[39;00m\n\u001b[1;32m    179\u001b[0m     \u001b[38;5;66;03m# safely garbage collected.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/anyscale/snapshot_util.py:121\u001b[0m, in \u001b[0;36mcreate_snapshot_zip\u001b[0;34m(directory, auto)\u001b[0m\n\u001b[1;32m    119\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m child \u001b[38;5;129;01min\u001b[39;00m os\u001b[38;5;241m.\u001b[39mlistdir(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    120\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(child, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.git\u001b[39m\u001b[38;5;124m\"\u001b[39m)):\n\u001b[0;32m--> 121\u001b[0m                 \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    122\u001b[0m                     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGit repo detected in sub-directory \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mchild\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. Please ensure \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    123\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthat your git repo is cloned in the top-level workspace \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    124\u001b[0m                     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdirectory with \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgit clone <repo> .\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdirectory\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    125\u001b[0m                 )\n\u001b[1;32m    126\u001b[0m         subprocess\u001b[38;5;241m.\u001b[39mcheck_call(  \u001b[38;5;66;03m# noqa\u001b[39;00m\n\u001b[1;32m    127\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfind . | zip --symlinks -@ -0 -q \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtarget\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, shell\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    128\u001b[0m         )\n\u001b[1;32m    129\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "\u001b[0;31mValueError\u001b[0m: Git repo detected in sub-directory AnyscaleDemos. Please ensure that your git repo is cloned in the top-level workspace directory with 'git clone <repo> .' at /home/ray/default."
     ]
    }
   ],
   "source": [
    "# Read some Parquet files in parallel.\n",
    "rds = ray.data.read_parquet(s3_files)\n",
    "print(type(rds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parquet stores the number of rows per file in the Parquet metadata, \n",
    "# so we can get the number of rows in rds without triggering a full data read!\n",
    "print(f\"Number rows: {rds.count()}\")\n",
    "\n",
    "# Parquet pulls size-in-bytes from its metadata (not triggering a data read)\n",
    "# This could be significantly different than actual in-memory size!\n",
    "print(f\"Size bytes (from parquet metadata): {rds.size_bytes()}\")\n",
    "# Trigger full reading of the dataset and inspect the size in bytes.\n",
    "print(f\"Size bytes (from full data read): {rds.fully_executed().size_bytes()}\")\n",
    "\n",
    "# Fetch the schema from the underlying Parquet metadata.\n",
    "print(\"\\nSchema data types:\")\n",
    "data_types = list(zip(rds.schema().names, rds.schema().types))\n",
    "[print(f\"{s[0]}: {s[1]}\") for s in data_types]\n",
    "\n",
    "# Take a peek at a sample row\n",
    "print(\"\\nLook at a sample row:\")\n",
    "rds.take(1)\n",
    "\n",
    "# Number rows: 6941024"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Filter on Read - Projection and Filter Pushdown</b>\n",
    "\n",
    "Note that Ray Datasets' Parquet reader supports projection (column selection) and row filter pushdown, where we can push the above column selection and the row-based filter to the Parquet read. If we specify column selection at Parquet read time, the unselected columns won't even be read from disk!\n",
    "\n",
    "The row-based filter is specified via [Arrow's dataset field expressions](https://arrow.apache.org/docs/6.0/python/generated/pyarrow.dataset.Expression.html#pyarrow.dataset.Expression). \n",
    "\n",
    "<b>Best practice is to filter as much as you can directly in the Ray Dataset read_parquet() statement! </b>\n",
    "\n",
    "Normally there is some data exploration to determine the cleaning steps.  Let's just assume we know the data cleaning steps are:\n",
    "- Drop negative trip distances, 0 fares, 0 passengers, less than 1min trip durations\n",
    "- Drop 2 unknown zones ['264', '265']\n",
    "- Calculate trip duration and add it as a new column\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pushdown_read_data(files_list: list,\n",
    "                       sample_ids: list) -> Dataset:\n",
    "    filter_expr = (\n",
    "        (pds.field(\"passenger_count\") > 0)\n",
    "        & (pds.field(\"trip_distance\") > 0)\n",
    "        & (pds.field(\"fare_amount\") > 0)\n",
    "        & (~pds.field(\"pickup_location_id\").isin([264, 265]))\n",
    "        & (~pds.field(\"dropoff_location_id\").isin([264, 265]))\n",
    "        & (pds.field(\"dropoff_location_id\").isin(sample_ids))\n",
    "    )\n",
    "\n",
    "    the_dataset = ray.data.read_parquet(\n",
    "        files_list,\n",
    "        columns=[\n",
    "            'pickup_at', 'dropoff_at', \n",
    "            'pickup_location_id', 'dropoff_location_id',\n",
    "            'passenger_count', 'trip_distance', 'fare_amount'], \n",
    "        filter=filter_expr,\n",
    "    )\n",
    "\n",
    "    # Force full execution of both of the file reads.\n",
    "    the_dataset = the_dataset.fully_executed()\n",
    "    return the_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the pushdown_read_data function\n",
    "pushdown_ds = pushdown_read_data(s3_files, sample_locations)\n",
    "\n",
    "print(f\"Number rows: {pushdown_ds.count()}\")\n",
    "# Display some metadata about the dataset.\n",
    "print(\"\\nMetadata: \")\n",
    "print(pushdown_ds)\n",
    "# Fetch the schema from the underlying Parquet metadata.\n",
    "print(\"\\nSchema:\")\n",
    "print(pushdown_ds.schema())\n",
    "# Take a peek at a single row\n",
    "print(\"\\nLook at a sample row:\")\n",
    "pushdown_ds.take(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check sampling\n",
    "df = pushdown_ds.to_pandas(limit=pushdown_ds.count())\n",
    "print(df[[\"dropoff_location_id\", \"trip_distance\"]].groupby(\"dropoff_location_id\").count())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Custom data transform functions</b>\n",
    "\n",
    "Ray Datasets allows you to specify custom data transform functions using familiar syntax, such as Pandas.  These [<b>custom functions, or UDFs,</b>](https://docs.ray.io/en/latest/data/transforming-datasets.html) can be called using `rds.map_batches(my_UDF, batch_format=\"pandas\")`.  It is necessary to specify the data processing API you are using in the `batch_format parameter`.\n",
    "\n",
    "Available data processing APIs you can specify in the batch_format paramater include <b>\"pandas\", “pyarrow”, “numpy”</b>, and simple Python <b>list</b>.  Tabular data will be passed into your UDF by default as a pandas dataframe.  Tensor data will be passed into your UDF as a numpy array.\n",
    "\n",
    "It is also possible to chain UDFs using [BatchMapper](https://docs.ray.io/en/latest/ray-air/check-ingest.html).  This is convenient when you want to chain a standard scaler step before training, for example.\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A Pandas DataFrame UDF for transforming the underlying blocks of a Dataset in parallel.\n",
    "def transform_batch(the_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = the_df.copy()    \n",
    "    df[\"trip_duration\"] = (df[\"dropoff_at\"] - df[\"pickup_at\"]).dt.seconds    \n",
    "    df = df[df[\"trip_duration\"] > 60]    \n",
    "    df.drop([\"dropoff_at\", \"pickup_at\", \"pickup_location_id\"], axis=1, inplace=True)\n",
    "    df['dropoff_location_id'] = df['dropoff_location_id'].fillna(-1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the transform UDF function\n",
    "print(f\"Before transform number rows: {pushdown_ds.count()}\")\n",
    "\n",
    "# batch_format=\"pandas\" tells Datasets to provide the transformer with blocks\n",
    "# represented as Pandas DataFrames.\n",
    "pushdown_ds = pushdown_ds.map_batches(transform_batch, batch_format=\"pandas\")\n",
    "\n",
    "# verify row count\n",
    "pushdown_rows = pushdown_ds.count()\n",
    "print(f\"After transform number rows: {pushdown_rows}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete data to free up memory in our Ray cluster\n",
    "del rds\n",
    "del pushdown_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Tidying up</b>\n",
    "\n",
    "To make our code easier to read, let's summarize the data processing functions again here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter parquet data using Ray Datasets read_parquet()\n",
    "def pushdown_read_data(files_list: list,\n",
    "                       sample_ids: list) -> Dataset:\n",
    "    \n",
    "    start = time.time()\n",
    "    \n",
    "    filter_expr = (\n",
    "        (pds.field(\"passenger_count\") > 0)\n",
    "        & (pds.field(\"trip_distance\") > 0)\n",
    "        & (pds.field(\"fare_amount\") > 0)\n",
    "        & (~pds.field(\"pickup_location_id\").isin([264, 265]))\n",
    "        & (~pds.field(\"dropoff_location_id\").isin([264, 265]))\n",
    "        & (pds.field(\"dropoff_location_id\").isin(sample_ids))\n",
    "    )\n",
    "\n",
    "    the_dataset = ray.data.read_parquet(\n",
    "        files_list,\n",
    "        columns=[\n",
    "            'pickup_at', 'dropoff_at', \n",
    "            'pickup_location_id', 'dropoff_location_id',\n",
    "            'passenger_count', 'trip_distance', 'fare_amount'], \n",
    "        filter=filter_expr,\n",
    "    )\n",
    "\n",
    "    # Force full execution of both of the file reads.\n",
    "    the_dataset = the_dataset.fully_executed()\n",
    "    \n",
    "    data_loading_time = time.time() - start\n",
    "    print_time(f\"Data loading time: {data_loading_time:.2f} seconds\")\n",
    "    return the_dataset\n",
    "\n",
    "# A Pandas DataFrame UDF for transforming the underlying blocks of a Dataset in parallel.\n",
    "def transform_batch(the_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    start = time.time()\n",
    "    \n",
    "    df = the_df.copy()    \n",
    "    df[\"trip_duration\"] = (df[\"dropoff_at\"] - df[\"pickup_at\"]).dt.seconds    \n",
    "    df = df[df[\"trip_duration\"] > 60]    \n",
    "    df.drop([\"dropoff_at\", \"pickup_at\", \"pickup_location_id\"], axis=1, inplace=True)\n",
    "    df['dropoff_location_id'] = df['dropoff_location_id'].fillna(-1)\n",
    "    \n",
    "    data_transform_time = time.time() - start\n",
    "    # print_time(f\"Data transform time: {data_transform_time:.2f} seconds\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batch training with Ray Dataset <a class=\"anchor\" id=\"train_func\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have learned more about our data and written a pandas UDF to transform our data, we are ready to train a model on batches of this data in parallel.\n",
    "\n",
    "To simplify the model training part, we will use linear regression in Scikit-learn.  \n",
    "- We will use the `dropoff_location_id` column in the dataset to group the dataset into data batches. \n",
    "- Then we will fit a separate model for each batch to predict `trip_duration`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If necessary pip install scikit-learn from your terminal\n",
    "\n",
    "import sklearn\n",
    "from sklearn.base import BaseEstimator \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from ray.train.sklearn import SklearnTrainer, SklearnPredictor\n",
    "from ray.train.batch_predictor import BatchPredictor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Define training functions</b>\n",
    "\n",
    "We want to fit a linear regression model to the trip duration for each drop-off location.  For scoring, we will calculate mean absolute error on the validation set, and report that as model error per drop-off location.\n",
    "\n",
    "We define `fit_and_score_sklearn` function [<b><i>as a Ray task</i></b>](https://docs.ray.io/en/latest/ray-core/tasks.html), where each Scikit-learn training task will consume a dataset shard in batches.  Ray is able to automatically distribute ray tasks on your Ray cluster, to utilize parallel compute.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ray task to fit and score a scikit-learn model.\n",
    "@ray.remote\n",
    "def fit_and_score_sklearn(\n",
    "    train_df: pd.DataFrame, test_df: pd.DataFrame, model: BaseEstimator\n",
    ") -> Tuple[BaseEstimator, float]:\n",
    "    \n",
    "    # Assemble train/test pandas dfs\n",
    "    train_X = train_df[[\"passenger_count\", \"trip_distance\", \"fare_amount\"]]\n",
    "    train_y = train_df.trip_duration\n",
    "    test_X = test_df[[\"passenger_count\", \"trip_distance\", \"fare_amount\"]]\n",
    "    test_y = test_df.trip_duration\n",
    "    \n",
    "    # Start training.\n",
    "    model = model.fit(train_X, train_y)\n",
    "    pred_y = model.predict(test_X)\n",
    "    error = sklearn.metrics.mean_absolute_error(test_y, pred_y)\n",
    "    \n",
    "    return str(model), error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "The `train_and_evaluate` function contains the logic for train-test splitting and fitting of multiple models in parallel on each data batch, for purposes of comparison. Thanks to this, we can evaluate several models and choose the best one for each data batch.\n",
    "\n",
    "This function takes as input, batches of Ray Dataset data.  Each batch of data is placed into Ray's distributed shared-memory object store, using the command [<b><i>ray.put()</i></b>](https://docs.ray.io/en/latest/ray-core/objects.html). Then the remote `fit_and_score_sklearn` Ray task is run simultaneously for all batches of data at once. Function `fit_and_score_sklearn` return values are all retrieved outside the loop using a single [<b><i>ray.get()</i></b>](https://docs.ray.io/en/latest/ray-core/tasks/patterns/ray-get-loop.html) call.\n",
    "<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate(\n",
    "    the_df: pd.DataFrame, \n",
    "    models: List[BaseEstimator]\n",
    ") -> List[Tuple[BaseEstimator, float]]:\n",
    "    \n",
    "    # check if input df is big enough for training\n",
    "    if len(the_df) < 4:\n",
    "        print(f\"Dataframe for LocID: {i} is empty or smaller than 4\")\n",
    "        return None\n",
    "    else:\n",
    "        loc_id = the_df.dropoff_location_id[0]\n",
    "        # print(f\"Processing location {loc_id}...\")\n",
    "    \n",
    "    start = time.time()\n",
    "\n",
    "    # Train / test split\n",
    "    # Randomly split the data into 80/20 train/test.\n",
    "    train_df, test_df = train_test_split(the_df, test_size=0.2)\n",
    "    \n",
    "    # We put the train & test dataframes into Ray object store\n",
    "    # so that they can be reused by all models fitted here.\n",
    "    # https://docs.ray.io/en/latest/ray-core/tips-for-first-time.html#tip-3-avoid-passing-same-object-repeatedly-to-remote-tasks\n",
    "    train_ref = ray.put(train_df)\n",
    "    test_ref = ray.put(test_df)\n",
    "\n",
    "    # Launch a fit and score task for each model.\n",
    "    results = ray.get(\n",
    "        [fit_and_score_sklearn.remote(train_ref, test_ref, model) for model in models]\n",
    "    )\n",
    "    # results.sort(key=lambda x: x[1])  # sort by error\n",
    "    \n",
    "    # Assemble loc_id, name of model, and metrics in a pandas DataFrame\n",
    "    results = [loc_id] + list(results[0])\n",
    "    results_return = pd.DataFrame(columns=['location_id', 'model', 'error'])\n",
    "    results_return.loc[0] = results\n",
    "\n",
    "    training_time = time.time() - start\n",
    "    print_time(f\"Training time for LocID {loc_id}: {training_time:.2f} seconds\")\n",
    "    \n",
    "    return results_return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "Recall how we wrote a data transform <i>UDF_func</i> using Pandas syntax?  It was called with pattern:\n",
    "- `rds.map_batches(UDF_func, batch_format)`\n",
    "\n",
    "Similarly, a groupby-agg function can be used later when we perform a Ray Dataset <b>groupby</b>.  Below, the function <i>agg_func</i> will be called using a pattern: \n",
    "- `rds.groupby.map_groups(agg_func, batch_format)`.\n",
    "<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A Pandas DataFrame aggregation function for processing grouped batches of Ray Dataset data.\n",
    "def agg_func(the_df: pd.DataFrame):\n",
    "    \n",
    "    # possible to add more trainers in this list\n",
    "    models = [LinearRegression()]\n",
    "    ret = pd.DataFrame()\n",
    "    \n",
    "    # Handle errors in data groups\n",
    "    try:\n",
    "        # Transform the input pandas AND fit_and_evaluate the transformed pandas\n",
    "        ret = train_and_evaluate(transform_batch(the_df), models)\n",
    "    except Exception:\n",
    "        pass\n",
    "    \n",
    "    # Process null data groups\n",
    "    if ret.shape[0] == 0:\n",
    "        loc_id = the_df.dropoff_location_id[0]\n",
    "        print(f\"failed on {loc_id}\")\n",
    "        # assemble a null entry\n",
    "        ret = [loc_id, None, None]\n",
    "        results_return = pd.DataFrame(columns=['location_id', 'model', 'error'])\n",
    "        results_return.loc[0] = ret\n",
    "        return results_return\n",
    "    \n",
    "    # print(f\"agg_func returned type: {type(ret)}\")\n",
    "    return ret\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Main driver code using Ray Datasets `map_groups`</b> \n",
    "\n",
    "Finally, the main \"driver code\" reads each Parquet file (each file corresponds to one month of NYC taxi data) into a Ray Dataset, called `rds`. Then we use Ray Dataset <b>groupby</b> to map each group into a batch of data, on which `agg_func` can run, using the pattern [groupby-map_groups(agg_func, batch_format)](https://docs.ray.io/en/latest/data/api/grouped_dataset.html).  This implements an accumulator-based aggregation, which can run on each batch of data in parallel. \n",
    "<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Driver code to run this.\n",
    "\n",
    "SMOKE_TEST = True\n",
    "if SMOKE_TEST:\n",
    "    starting_idx = -1\n",
    "    sample_locations = random.sample(list(local_utils.dataprep.location_ids), 3)\n",
    "else:\n",
    "    starting_idx = -3\n",
    "    sample_locations = list(local_utils.dataprep.location_ids)\n",
    "\n",
    "s3_files = [f\"s3://{file}\" for file in s3_partitions.files][starting_idx:]\n",
    "print(f\"NYC Taxi using {len(s3_files)} file(s)!\")   \n",
    "print(f\"sample locations: {sample_locations}\")\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "# Read data into Ray Dataset\n",
    "rds = pushdown_read_data(s3_files, sample_locations)\n",
    "\n",
    "# Use Ray Dataset groupby.map_groups() to parallel process each grouped data batch\n",
    "# Returns a Ray Datset\n",
    "results = rds.groupby(\"dropoff_location_id\").map_groups(\n",
    "            agg_func, \n",
    "            batch_format=\"pandas\")\n",
    "# print(f\"groupby.map_groups() returned type: {type(results)}\")\n",
    "\n",
    "# Print number models trained and total time.\n",
    "total_time_taken = time.time() - start\n",
    "print(f\"Total number of models: {len(sample_locations)}\")\n",
    "print_time(f\"TOTAL TIME TAKEN: {total_time_taken:.2f} seconds\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort results ascending by error\n",
    "\n",
    "print(type(results))\n",
    "\n",
    "# sort values by ascending error\n",
    "results_df = results.to_pandas(limit=results.count())\n",
    "results_df.sort_values(by=[\"error\"], ascending=True, inplace=True)\n",
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Main driver code using Ray Datasets `map_groups`, on all the data!</b>\n",
    "\n",
    "The Smoke test worked, so now let us run the main driver code again, to batch train every location_id in parallel, with all the data files this time!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Driver code to run this.\n",
    "\n",
    "SMOKE_TEST = False\n",
    "if SMOKE_TEST:\n",
    "    starting_idx = -1\n",
    "    sample_locations = random.sample(list(local_utils.dataprep.location_ids), 3)\n",
    "else:\n",
    "    starting_idx = -3\n",
    "    sample_locations = list(local_utils.dataprep.location_ids)\n",
    "\n",
    "s3_files = [f\"s3://{file}\" for file in s3_partitions.files][starting_idx:]\n",
    "print(f\"NYC Taxi using {len(s3_files)} file(s)!\")   \n",
    "print(f\"sample locations: {sample_locations}\")\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "# Read data into Ray Dataset\n",
    "# rds = pushdown_read_data(s3_files, sample_locations)\n",
    "\n",
    "# Use Ray Dataset groupby.map_groups() to parallel process each group\n",
    "# Returns a Ray Datset\n",
    "results = rds.groupby(\"dropoff_location_id\").map_groups(\n",
    "            agg_func, batch_format=\"pandas\")\n",
    "print(f\"groupby.map_groups() returned type: {type(results)}\")\n",
    "\n",
    "total_time_taken = time.time() - start\n",
    "print(f\"Total number of models: {len(sample_locations)}\")\n",
    "print_time(f\"TOTAL TIME TAKEN: {total_time_taken:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Total number of models: {len(sample_locations)}\")\n",
    "print_time(f\"TOTAL TIME TAKEN: {total_time_taken:.2f} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort results ascending by error\n",
    "\n",
    "print(type(results))\n",
    "\n",
    "# sort values by ascending error\n",
    "results_df = results.to_pandas(limit=results.count())\n",
    "results_df.sort_values(by=[\"error\"], ascending=True, inplace=True)\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
