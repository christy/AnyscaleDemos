{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Batch Training on Ray AIR/Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning objectives\n",
    "In this this tutorial, you will learn about:\n",
    " * [Ray Dataset](#dataset)\n",
    " * [AIR Trainer](#trainer)\n",
    " * [Ray Tune](#tune)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Batch training and tuning are common tasks in simple machine learning use-cases such as time series forecasting. They require fitting of simple models on multiple data batches corresponding to locations, products, etc. This notebook showcases how to conduct batch training using [Ray Dataset](https://docs.ray.io/en/latest/data/dataset.html), [Ray AIR Trainers](https://docs.ray.io/en/master/ray-air/trainer.html#air-trainers), and [Ray Tune](https://docs.ray.io/en/master/ray-air/tuner.html).\n",
    "\n",
    "For the data, we will use the [NYC Taxi dataset](https://www1.nyc.gov/site/tlc/about/tlc-trip-record-data.page).  This popular tabular dataset contains historical taxi pickups by timestamp and location in NYC.  <s>The goal is to predict future, hourly taxi demand by location in NYC.</s>\n",
    "The goal is to perform batch training & tuning using scikit-learn (treating this as a simple regression problem to predict `trip_duration`).\n",
    "\n",
    "A separate model will be trained for each pickup location. We can use the pickup_location_id column in the dataset to group the dataset into data batches. We will then fit models for each batch and choose the best one.\n",
    "\n",
    "Let’s start by importing a few required libraries, including open-source [Ray](https://github.com/ray-project/ray) itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of CPUs in this system: 8\n"
     ]
    }
   ],
   "source": [
    "import sys, os\n",
    "import pandas as pd\n",
    "# import numpy as np\n",
    "import pyarrow as pa\n",
    "import ray\n",
    "\n",
    "# Local code\n",
    "# sys.path.insert( 0, os.path.abspath(\"../local_util\") )\n",
    "# import dataprep  # From this repository's SSML/local_utils folder\n",
    "# import utility functions\n",
    "from local_utils import dataprep\n",
    "\n",
    "# import os, warnings\n",
    "# warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "num_available_cpus = os.cpu_count()\n",
    "print(f'Number of CPUs in this system: {num_available_cpus}')\n",
    "\n",
    "# # AWS\n",
    "# import boto3              # AWS SDK for Python\n",
    "# import s3fs               # AWS SDK for s3-to-pandas "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data <a class=\"anchor\" id=\"dataset\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, read some data using Ray Dataset.   This will initialize a Ray cluster.  Then we can use the [Ray Dataset](https://docs.ray.io/en/latest/data/getting-started.html#datasets-getting-started) APIs to quickly inspect the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-08 08:28:43,023\tINFO worker.py:1509 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8266 \u001b[39m\u001b[22m\n",
      "2022-10-08 08:28:44,744\tWARNING read_api.py:291 -- ⚠️  The number of blocks in this dataset (1) limits its parallelism to 1 concurrent tasks. This is much less than the number of available CPU slots in the cluster. Use `.repartition(n)` to increase the number of dataset blocks.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'ray.data.dataset.Dataset'>\n"
     ]
    }
   ],
   "source": [
    "# Read some Parquet files in parallel.\n",
    "data_files = \\\n",
    "[\n",
    "    \"s3://anonymous@air-example-data/ursa-labs-taxi-data/downsampled_2009_01_data.parquet\",\n",
    "    # \"s3://anonymous@air-example-data/ursa-labs-taxi-data/downsampled_2009_02_data.parquet\"\n",
    "]\n",
    "\n",
    "ds = ray.data.read_parquet(data_files)\n",
    "print(type(ds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number rows: 1410617\n",
      "\n",
      "Metadata: \n",
      "Dataset(num_blocks=1, num_rows=1410617, schema={vendor_id: string, pickup_at: timestamp[us], dropoff_at: timestamp[us], passenger_count: int8, trip_distance: float, pickup_longitude: float, pickup_latitude: float, rate_code_id: null, store_and_fwd_flag: string, dropoff_longitude: float, dropoff_latitude: float, payment_type: string, fare_amount: float, extra: float, mta_tax: float, tip_amount: float, tolls_amount: float, total_amount: float})\n",
      "\n",
      "Schema:\n",
      "vendor_id: string\n",
      "pickup_at: timestamp[us]\n",
      "dropoff_at: timestamp[us]\n",
      "passenger_count: int8\n",
      "trip_distance: float\n",
      "pickup_longitude: float\n",
      "pickup_latitude: float\n",
      "rate_code_id: null\n",
      "store_and_fwd_flag: string\n",
      "dropoff_longitude: float\n",
      "dropoff_latitude: float\n",
      "payment_type: string\n",
      "fare_amount: float\n",
      "extra: float\n",
      "mta_tax: float\n",
      "tip_amount: float\n",
      "tolls_amount: float\n",
      "total_amount: float\n",
      "-- schema metadata --\n",
      "pandas: '{\"index_columns\": [{\"kind\": \"range\", \"name\": null, \"start\": 0, \"' + 2524\n",
      "\n",
      "Look at a sample row:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[ArrowRow({'vendor_id': 'VTS',\n",
       "           'pickup_at': datetime.datetime(2009, 1, 21, 14, 58),\n",
       "           'dropoff_at': datetime.datetime(2009, 1, 21, 15, 3),\n",
       "           'passenger_count': 1,\n",
       "           'trip_distance': 0.5299999713897705,\n",
       "           'pickup_longitude': -73.99270629882812,\n",
       "           'pickup_latitude': 40.7529411315918,\n",
       "           'rate_code_id': None,\n",
       "           'store_and_fwd_flag': None,\n",
       "           'dropoff_longitude': -73.98814392089844,\n",
       "           'dropoff_latitude': 40.75956344604492,\n",
       "           'payment_type': 'CASH',\n",
       "           'fare_amount': 4.5,\n",
       "           'extra': 0.0,\n",
       "           'mta_tax': None,\n",
       "           'tip_amount': 0.0,\n",
       "           'tolls_amount': 0.0,\n",
       "           'total_amount': 4.5})]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Parquet stores the number of rows per file in the Parquet metadata, \n",
    "# so we can get the number of rows in ds without triggering a full data read!\n",
    "print(f\"Number rows: {ds.count()}\")\n",
    "\n",
    "# Display some metadata about the dataset.\n",
    "print(\"\\nMetadata: \")\n",
    "print(ds)\n",
    "\n",
    "# Fetch the schema from the underlying Parquet metadata.\n",
    "print(\"\\nSchema:\")\n",
    "print(ds.schema())\n",
    "\n",
    "# Take a peek at a single row\n",
    "print(\"\\nLook at a sample row:\")\n",
    "ds.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normally there is some data exploration to determine the cleaning steps.  Let's just assume we know the data cleaning steps are:\n",
    "- Drop negative trip distances, 0 fares, 0 passengers, less than 1min trip durations\n",
    "- Drop 2 unknown zones ['264', '265']\n",
    "- Calculate trip duration in minutes and add it as a new column\n",
    "- Groupby, aggregate sum taxi rides, hourly per pickup location\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1410617\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Read->Map_Batches: 100%|██████████████████████████| 1/1 [00:11<00:00, 11.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final number rows: 1390337\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# A Pandas DataFrame UDF for transforming the underlying blocks of a Dataset in parallel.\n",
    "def transform_batch(the_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = the_df.copy()\n",
    "    df[\"trip_duration\"] = (df[\"dropoff_at\"] - df[\"pickup_at\"]).dt.seconds\n",
    "    df = df[df[\"trip_distance\"] > 0]\n",
    "    df = df[df[\"fare_amount\"] > 0]\n",
    "    df = df[df[\"passenger_count\"] > 0]\n",
    "    df = df[df[\"trip_duration\"] >= 60]\n",
    "    return df\n",
    "\n",
    "# batch_format=\"pandas\" tells Datasets to provide the transformer with blocks\n",
    "# represented as Pandas DataFrames.\n",
    "print(ds.count())\n",
    "ds = ds.map_batches(transform_batch, batch_format=\"pandas\")\n",
    "\n",
    "# verify row count\n",
    "ds_rows = ds.count()\n",
    "print(f\"Final number rows: {ds_rows}\")\n",
    "\n",
    "# approx 20K rows were dropped with that cleaning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Filter on Read - Projection and Filter Pushdown</b>\n",
    "\n",
    "Note that Ray Datasets' Parquet reader supports projection (column selection) and row filter pushdown, where we can push the above column selection and the row-based filter to the Parquet read. If we specify column selection at Parquet read time, the unselected columns won't even be read from disk!\n",
    "\n",
    "The row-based filter is specified via [Arrow's dataset field expressions](https://arrow.apache.org/docs/6.0/python/generated/pyarrow.dataset.Expression.html#pyarrow.dataset.Expression). See the {ref}feature guide for reading Parquet data <dataset_supported_file_formats> for more information.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-08 08:29:03,727\tWARNING read_api.py:291 -- ⚠️  The number of blocks in this dataset (1) limits its parallelism to 1 concurrent tasks. This is much less than the number of available CPU slots in the cluster. Use `.repartition(n)` to increase the number of dataset blocks.\n",
      "Read progress: 100%|██████████████████████████████| 1/1 [00:04<00:00,  4.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number rows: 1398850\n",
      "\n",
      "Metadata: \n",
      "Dataset(num_blocks=1, num_rows=1398850, schema={pickup_at: timestamp[us], dropoff_at: timestamp[us], passenger_count: int8, trip_distance: float, fare_amount: float})\n",
      "\n",
      "Schema:\n",
      "pickup_at: timestamp[us]\n",
      "dropoff_at: timestamp[us]\n",
      "passenger_count: int8\n",
      "trip_distance: float\n",
      "fare_amount: float\n",
      "-- schema metadata --\n",
      "pandas: '{\"index_columns\": [{\"kind\": \"range\", \"name\": null, \"start\": 0, \"' + 2524\n",
      "\n",
      "Look at a sample row:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[ArrowRow({'pickup_at': datetime.datetime(2009, 1, 21, 14, 58),\n",
       "           'dropoff_at': datetime.datetime(2009, 1, 21, 15, 3),\n",
       "           'passenger_count': 1,\n",
       "           'trip_distance': 0.5299999713897705,\n",
       "           'fare_amount': 4.5})]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pyarrow as pa\n",
    "filter_expr = (\n",
    "    (pa.dataset.field(\"passenger_count\") > 0)\n",
    "    & (pa.dataset.field(\"trip_distance\") > 0)\n",
    "    & (pa.dataset.field(\"fare_amount\") > 0)\n",
    ")\n",
    "\n",
    "pushdown_ds = ray.data.read_parquet(\n",
    "    data_files,\n",
    "    columns=['pickup_at', 'dropoff_at',\n",
    "    'passenger_count', 'trip_distance', 'fare_amount'], \n",
    "    filter=filter_expr,\n",
    ")\n",
    "\n",
    "# Force full execution of both of the file reads.\n",
    "pushdown_ds = pushdown_ds.fully_executed()\n",
    "\n",
    "print(f\"Number rows: {pushdown_ds.count()}\")\n",
    "# Display some metadata about the dataset.\n",
    "print(\"\\nMetadata: \")\n",
    "print(pushdown_ds)\n",
    "# Fetch the schema from the underlying Parquet metadata.\n",
    "print(\"\\nSchema:\")\n",
    "print(pushdown_ds.schema())\n",
    "# Take a peek at a single row\n",
    "print(\"\\nLook at a sample row:\")\n",
    "pushdown_ds.take(1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Custom data transform functions</b>\n",
    "\n",
    "Ray Datasets allows you to specify custom data transform functions using familiar syntax, such as Pandas.  These <b>custom functions, or UDFs,</b> can be called using `ds.map_batches(my_UDF, batch_format=\"pandas\")`.  It is necessary to specify the language you are using the `batch_format parameter`.\n",
    "\n",
    "TODO: Reference link for syntax supported in Datasets UDFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1398850\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map_Batches: 100%|████████████████████████████████| 1/1 [00:00<00:00,  1.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final number rows: 1390337\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# perform a simpler filter and compare \n",
    "# A Pandas DataFrame UDF for transforming the underlying blocks of a Dataset in parallel.\n",
    "def transform_batch2(the_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = the_df.copy()\n",
    "    df[\"trip_duration\"] = (df[\"dropoff_at\"] - df[\"pickup_at\"]).dt.seconds\n",
    "    df = df[df[\"trip_duration\"] >= 60]\n",
    "    df.drop([\"dropoff_at\", \"pickup_at\"], axis=1, inplace=True)\n",
    "    return df\n",
    "\n",
    "# batch_format=\"pandas\" tells Datasets to provide the transformer with blocks\n",
    "# represented as Pandas DataFrames.\n",
    "print(pushdown_ds.count())\n",
    "pushdown_ds = pushdown_ds.map_batches(transform_batch2, batch_format=\"pandas\")\n",
    "\n",
    "# verify row count\n",
    "pushdown_rows = pushdown_ds.count()\n",
    "print(f\"Final number rows: {pushdown_rows}\")\n",
    "assert ds_rows == pushdown_rows\n",
    "\n",
    "# Replace ds with pushdown\n",
    "ds = pushdown_ds\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Random shuffle</b>\n",
    "\n",
    "Randomly shuffling data is an important part of training machine learning models: it decorrelates samples, preventing overfitting and improving generalization. For many models, even between-epoch shuffling can drastically improve the precision gain per step/epoch. Datasets has a hyper-scalable distributed random shuffle that allows you to realize the model accuracy benefits of per-epoch shuffling without sacrificing training throughput, even at large data scales and even when doing distributed data-parallel training across multiple GPUs/nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Shuffle Map: 100%|████████████████████████████████| 1/1 [00:00<00:00, 12.74it/s]\n",
      "Shuffle Reduce: 100%|█████████████████████████████| 1/1 [00:00<00:00, 14.71it/s]\n"
     ]
    }
   ],
   "source": [
    "# do a full global random shuffle to decorrelate the data\n",
    "ds = ds.random_shuffle()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Split data into train/valid/test </b> \n",
    "\n",
    "We are ready to split the data into train/valid/test.  For now, we will just randomly split the data into 80/20 train/test.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map_Batches: 100%|████████████████████████████████| 1/1 [00:00<00:00, 25.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number rows train, test: 1112269, 278068\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "target = \"trip_duration\"\n",
    "\n",
    "# Split data into train and validation.\n",
    "train_ds, valid_ds = ds.train_test_split(test_size=0.2)\n",
    "\n",
    "# Create a test dataset by dropping the target column.\n",
    "test_ds = valid_ds.drop_columns(cols=[target])\n",
    "\n",
    "assert train_ds.count() + valid_ds.count() == ds.count()\n",
    "print(f\"Number rows train, test: \", end=\"\")\n",
    "print(f\"{train_ds.count()}, {test_ds.count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Tidying up</b>\n",
    "\n",
    "To make our code more modular and easier to read, let's put all those data processing steps into a single function called `dataprep.prepare_data()`.  See [scripts](../scripts/dataprep.py) for the full code.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-08 08:29:09,287\tWARNING read_api.py:291 -- ⚠️  The number of blocks in this dataset (1) limits its parallelism to 1 concurrent tasks. This is much less than the number of available CPU slots in the cluster. Use `.repartition(n)` to increase the number of dataset blocks.\n",
      "Read progress: 100%|██████████████████████████████| 1/1 [00:04<00:00,  4.45s/it]\n",
      "Map_Batches: 100%|████████████████████████████████| 1/1 [00:00<00:00,  2.02it/s]\n",
      "Map_Batches: 100%|████████████████████████████████| 1/1 [00:00<00:00,  4.64it/s]\n",
      "Shuffle Map: 100%|████████████████████████████████| 1/1 [00:00<00:00, 14.50it/s]\n",
      "Shuffle Reduce: 100%|█████████████████████████████| 1/1 [00:00<00:00, 13.57it/s]\n",
      "Map_Batches: 100%|████████████████████████████████| 1/1 [00:00<00:00, 23.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number rows train, test: 1112269, 278068\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[PandasRow({'passenger_count': 1,\n",
       "            'trip_distance': 0.9700000286102295,\n",
       "            'fare_amount': 5.699999809265137})]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Single function call for preparing data using Ray Dataset\n",
    "train_ds, valid_ds, test_ds = dataprep.prepare_data(data_files, target)\n",
    "\n",
    "print(f\"Number rows train, test: \", end=\"\")\n",
    "print(f\"{train_ds.count()}, {test_ds.count()}\")\n",
    "test_ds.take(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PandasRow({'passenger_count': 1,\n",
       "            'trip_distance': 0.9700000286102295,\n",
       "            'fare_amount': 5.699999809265137})]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_ds.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AIR Trainer <a class=\"anchor\" id=\"trainer\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ray AI Runtime (AIR) is a scalable and unified toolkit for ML applications.  AIR builds on Ray’s best-in-class libraries for Preprocessing, Training, Tuning, Scoring, Serving, and Reinforcement Learning to bring together an ecosystem of integrations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of CPUs in this system: 8\n"
     ]
    }
   ],
   "source": [
    "from ray.air.config import ScalingConfig\n",
    "from ray.train.sklearn import SklearnTrainer, SklearnPredictor\n",
    "from ray.data.preprocessors import Chain, OrdinalEncoder, StandardScaler\n",
    "from ray.air.result import Result\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "print(f'Number of CPUs in this system: {num_available_cpus}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Scaling decisions</b>\n",
    "\n",
    "By default, Dataset tasks use all available cluster CPU resources for execution. This can sometimes conflict with Trainer resource requests. For example, if Trainers allocate all CPU resources in the cluster, then no Datasets tasks can run.\n",
    "\n",
    "A good rule of thumb, if you know you need to do other things besides Train, is to reserve a couple CPUs for those other purposes besides training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# decide how many processors to use for training\n",
    "num_training_cpus = num_available_cpus - 2\n",
    "\n",
    "# assign training resources to AIR Trainer\n",
    "trainer_resources = {\"CPU\": num_training_cpus}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a preprocessor to scale some columns.\n",
    "from ray.data.preprocessors import StandardScaler\n",
    "\n",
    "preprocessor = StandardScaler(\n",
    "    columns=[\"passenger_count\", \n",
    "             \"fare_amount\", ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-10-08 08:29:19 (running for 00:00:03.39)<br>Memory usage on this node: 13.4/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/8 CPUs, 0/0 GPUs, 0.0/6.93 GiB heap, 0.0/2.0 GiB objects<br>Result logdir: /Users/christy/ray_results/SklearnTrainer_2022-10-08_08-29-15<br>Number of trials: 1/1 (1 TERMINATED)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status    </th><th>loc           </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  fit_time</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>SklearnTrainer_f7f56_00000</td><td>TERMINATED</td><td>127.0.0.1:8790</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         1.89947</td><td style=\"text-align: right;\">  0.101401</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for SklearnTrainer_f7f56_00000:\n",
      "  cv:\n",
      "    fit_time: [0.09510993957519531, 0.07595109939575195, 0.07802295684814453, 0.07298398017883301,\n",
      "      0.06866836547851562]\n",
      "    fit_time_mean: 0.07814726829528809\n",
      "    fit_time_std: 0.009045219410326244\n",
      "    score_time: [0.006638050079345703, 0.003111124038696289, 0.004126071929931641, 0.0031020641326904297,\n",
      "      0.0026056766510009766]\n",
      "    score_time_mean: 0.003916597366333008\n",
      "    score_time_std: 0.0014478224514137114\n",
      "    test_score: [0.0021971192522125538, 0.0018830469547053141, 0.0021269945373810772,\n",
      "      0.0023612663349836804, 0.0013532884232091424]\n",
      "    test_score_mean: 0.0019843431004983535\n",
      "    test_score_std: 0.0003510513246689167\n",
      "  date: 2022-10-08_08-29-19\n",
      "  done: false\n",
      "  experiment_id: 96cd084697fc46ffa34ab1655d85e11d\n",
      "  fit_time: 0.10140085220336914\n",
      "  hostname: Christys-MacBook-Pro.local\n",
      "  iterations_since_restore: 1\n",
      "  node_ip: 127.0.0.1\n",
      "  pid: 8790\n",
      "  should_checkpoint: true\n",
      "  time_since_restore: 1.8994650840759277\n",
      "  time_this_iter_s: 1.8994650840759277\n",
      "  time_total_s: 1.8994650840759277\n",
      "  timestamp: 1665242959\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 1\n",
      "  trial_id: f7f56_00000\n",
      "  valid:\n",
      "    score_time: 0.005650997161865234\n",
      "    test_score: 0.002360176574100592\n",
      "  warmup_time: 0.0025310516357421875\n",
      "  \n",
      "Result for SklearnTrainer_f7f56_00000:\n",
      "  cv:\n",
      "    fit_time: [0.09510993957519531, 0.07595109939575195, 0.07802295684814453, 0.07298398017883301,\n",
      "      0.06866836547851562]\n",
      "    fit_time_mean: 0.07814726829528809\n",
      "    fit_time_std: 0.009045219410326244\n",
      "    score_time: [0.006638050079345703, 0.003111124038696289, 0.004126071929931641, 0.0031020641326904297,\n",
      "      0.0026056766510009766]\n",
      "    score_time_mean: 0.003916597366333008\n",
      "    score_time_std: 0.0014478224514137114\n",
      "    test_score: [0.0021971192522125538, 0.0018830469547053141, 0.0021269945373810772,\n",
      "      0.0023612663349836804, 0.0013532884232091424]\n",
      "    test_score_mean: 0.0019843431004983535\n",
      "    test_score_std: 0.0003510513246689167\n",
      "  date: 2022-10-08_08-29-19\n",
      "  done: true\n",
      "  experiment_id: 96cd084697fc46ffa34ab1655d85e11d\n",
      "  experiment_tag: '0'\n",
      "  fit_time: 0.10140085220336914\n",
      "  hostname: Christys-MacBook-Pro.local\n",
      "  iterations_since_restore: 1\n",
      "  node_ip: 127.0.0.1\n",
      "  pid: 8790\n",
      "  should_checkpoint: true\n",
      "  time_since_restore: 1.8994650840759277\n",
      "  time_this_iter_s: 1.8994650840759277\n",
      "  time_total_s: 1.8994650840759277\n",
      "  timestamp: 1665242959\n",
      "  timesteps_since_restore: 0\n",
      "  training_iteration: 1\n",
      "  trial_id: f7f56_00000\n",
      "  valid:\n",
      "    score_time: 0.005650997161865234\n",
      "    test_score: 0.002360176574100592\n",
      "  warmup_time: 0.0025310516357421875\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-08 08:29:19,954\tINFO tune.py:758 -- Total run time: 4.15 seconds (3.38 seconds for the tuning loop).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'valid': {'score_time': 0.005650997161865234, 'test_score': 0.002360176574100592}, 'cv': {'fit_time': array([0.09510994, 0.0759511 , 0.07802296, 0.07298398, 0.06866837]), 'score_time': array([0.00663805, 0.00311112, 0.00412607, 0.00310206, 0.00260568]), 'test_score': array([0.00219712, 0.00188305, 0.00212699, 0.00236127, 0.00135329]), 'fit_time_mean': 0.07814726829528809, 'fit_time_std': 0.009045219410326244, 'score_time_mean': 0.003916597366333008, 'score_time_std': 0.0014478224514137114, 'test_score_mean': 0.0019843431004983535, 'test_score_std': 0.0003510513246689167}, 'fit_time': 0.10140085220336914, 'time_this_iter_s': 1.8994650840759277, 'should_checkpoint': True, 'done': True, 'timesteps_total': None, 'episodes_total': None, 'training_iteration': 1, 'trial_id': 'f7f56_00000', 'experiment_id': '96cd084697fc46ffa34ab1655d85e11d', 'date': '2022-10-08_08-29-19', 'timestamp': 1665242959, 'time_total_s': 1.8994650840759277, 'pid': 8790, 'hostname': 'Christys-MacBook-Pro.local', 'node_ip': '127.0.0.1', 'config': {}, 'time_since_restore': 1.8994650840759277, 'timesteps_since_restore': 0, 'iterations_since_restore': 1, 'warmup_time': 0.0025310516357421875, 'experiment_tag': '0'}\n"
     ]
    }
   ],
   "source": [
    "trainer = SklearnTrainer(\n",
    "    estimator=LinearRegression(),\n",
    "    label_column=target,\n",
    "    datasets={\"train\": train_ds, \"valid\": valid_ds},\n",
    "    preprocessor=preprocessor,\n",
    "    cv=5,\n",
    "    scaling_config=ScalingConfig(trainer_resources=trainer_resources),\n",
    ")\n",
    "result = trainer.fit()\n",
    "print(result.metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def train_sklearn(num_cpus: int, use_gpu: bool = False) -> Result:\n",
    "#     if use_gpu and not cuMLRandomForestClassifier:\n",
    "#         raise RuntimeError(\"cuML must be installed for GPU enabled sklearn estimators.\")\n",
    "\n",
    "#     train_dataset, valid_dataset, _ = prepare_data()\n",
    "\n",
    "#     # Scale some random columns\n",
    "#     columns_to_scale = [\"mean radius\", \"mean texture\"]\n",
    "#     preprocessor = Chain(\n",
    "#         OrdinalEncoder([\"categorical_column\"]), StandardScaler(columns=columns_to_scale)\n",
    "#     )\n",
    "\n",
    "#     if use_gpu:\n",
    "#         trainer_resources = {\"CPU\": 1, \"GPU\": 1}\n",
    "#         estimator = cuMLRandomForestClassifier()\n",
    "#     else:\n",
    "#         trainer_resources = {\"CPU\": num_cpus}\n",
    "#         estimator = RandomForestClassifier()\n",
    "\n",
    "#     trainer = SklearnTrainer(\n",
    "#         estimator=estimator,\n",
    "#         label_column=\"target\",\n",
    "#         datasets={\"train\": train_dataset, \"valid\": valid_dataset},\n",
    "#         preprocessor=preprocessor,\n",
    "#         cv=5,\n",
    "#         scaling_config=ScalingConfig(trainer_resources=trainer_resources),\n",
    "#     )\n",
    "#     result = trainer.fit()\n",
    "#     print(result.metrics)\n",
    "\n",
    "#     return result\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
