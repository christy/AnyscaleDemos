{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "55bae90c",
   "metadata": {},
   "source": [
    "# Batch training & tuning on Ray Tune"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "882421fd",
   "metadata": {},
   "source": [
    "Batch training and tuning are common tasks in simple machine learning use-cases such as time series forecasting. They require fitting of simple models on multiple data batches corresponding to locations, products, etc.\n",
    "\n",
    "**'Batch training'** is a workload that trains model(s) on subsets of a dataset. This notebook showcases how to conduct batch training using [Ray Tune](https://docs.ray.io/en/latest/tune/index.html).\n",
    "\n",
    "![Batch training diagram](../../data/examples/images/batch-training.svg)\n",
    "\n",
    "For the data, we will use the [NYC Taxi dataset](https://www1.nyc.gov/site/tlc/about/tlc-trip-record-data.page). This popular tabular dataset contains historical taxi pickups by timestamp and location in NYC.\n",
    "\n",
    "In the notebook, we will split the data by dropoff location and train a separate regression model for each dropoff location predicting `trip_duration`. Specifically, we will use the `dropoff_location_id` column in the dataset to group the dataset into data batches. Then we will fit a separate model for each batch and evaluate it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f886edf7",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Contents\n",
    "\n",
    "In this this tutorial, you will learn about:\n",
    " 1. [Define how to load and prepare Parquet data](#load_data)\n",
    " 2. [Define your Ray Tune Search Space and Search Algorithm](#define_search_space)\n",
    " 3. [Define a Trainable (callable) function](#define_trainable)\n",
    " 4. [Run batch training on Ray Tune](#run_tune_search)\n",
    " 5. [Load a model from checkpoint and perform inference](#load_checkpoint)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e98f36a5",
   "metadata": {},
   "source": [
    "# Walkthrough\n",
    "\n",
    "```{tip}\n",
    "Prerequisite for this notebook: Read the [Key Concepts](https://docs.ray.io/en/latest/tune/key-concepts.html) page for Ray Tune.\n",
    "```\n",
    "\n",
    "Let us start by importing a few required libraries, including open-source [Ray](https://github.com/ray-project/ray) itself!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fc2d666c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of CPUs in this system: 8\n",
      "pyarrow: 10.0.0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(f'Number of CPUs in this system: {os.cpu_count()}')\n",
    "from typing import Tuple, List, Union, Optional, Callable\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pyarrow\n",
    "import pyarrow.parquet as pq\n",
    "import pyarrow.dataset as pds\n",
    "print(f\"pyarrow: {pyarrow.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eeda99c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-13 12:07:53,972\tINFO worker.py:1509 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8265 \u001b[39m\u001b[22m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "    <div style=\"margin-left: 50px;display: flex;flex-direction: row;align-items: center\">\n",
       "        <h3 style=\"color: var(--jp-ui-font-color0)\">Ray</h3>\n",
       "        <svg version=\"1.1\" id=\"ray\" width=\"3em\" viewBox=\"0 0 144.5 144.6\" style=\"margin-left: 3em;margin-right: 3em\">\n",
       "            <g id=\"layer-1\">\n",
       "                <path fill=\"#00a2e9\" class=\"st0\" d=\"M97.3,77.2c-3.8-1.1-6.2,0.9-8.3,5.1c-3.5,6.8-9.9,9.9-17.4,9.6S58,88.1,54.8,81.2c-1.4-3-3-4-6.3-4.1\n",
       "                    c-5.6-0.1-9.9,0.1-13.1,6.4c-3.8,7.6-13.6,10.2-21.8,7.6C5.2,88.4-0.4,80.5,0,71.7c0.1-8.4,5.7-15.8,13.8-18.2\n",
       "                    c8.4-2.6,17.5,0.7,22.3,8c1.3,1.9,1.3,5.2,3.6,5.6c3.9,0.6,8,0.2,12,0.2c1.8,0,1.9-1.6,2.4-2.8c3.5-7.8,9.7-11.8,18-11.9\n",
       "                    c8.2-0.1,14.4,3.9,17.8,11.4c1.3,2.8,2.9,3.6,5.7,3.3c1-0.1,2,0.1,3,0c2.8-0.5,6.4,1.7,8.1-2.7s-2.3-5.5-4.1-7.5\n",
       "                    c-5.1-5.7-10.9-10.8-16.1-16.3C84,38,81.9,37.1,78,38.3C66.7,42,56.2,35.7,53,24.1C50.3,14,57.3,2.8,67.7,0.5\n",
       "                    C78.4-2,89,4.7,91.5,15.3c0.1,0.3,0.1,0.5,0.2,0.8c0.7,3.4,0.7,6.9-0.8,9.8c-1.7,3.2-0.8,5,1.5,7.2c6.7,6.5,13.3,13,19.8,19.7\n",
       "                    c1.8,1.8,3,2.1,5.5,1.2c9.1-3.4,17.9-0.6,23.4,7c4.8,6.9,4.6,16.1-0.4,22.9c-5.4,7.2-14.2,9.9-23.1,6.5c-2.3-0.9-3.5-0.6-5.1,1.1\n",
       "                    c-6.7,6.9-13.6,13.7-20.5,20.4c-1.8,1.8-2.5,3.2-1.4,5.9c3.5,8.7,0.3,18.6-7.7,23.6c-7.9,5-18.2,3.8-24.8-2.9\n",
       "                    c-6.4-6.4-7.4-16.2-2.5-24.3c4.9-7.8,14.5-11,23.1-7.8c3,1.1,4.7,0.5,6.9-1.7C91.7,98.4,98,92.3,104.2,86c1.6-1.6,4.1-2.7,2.6-6.2\n",
       "                    c-1.4-3.3-3.8-2.5-6.2-2.6C99.8,77.2,98.9,77.2,97.3,77.2z M72.1,29.7c5.5,0.1,9.9-4.3,10-9.8c0-0.1,0-0.2,0-0.3\n",
       "                    C81.8,14,77,9.8,71.5,10.2c-5,0.3-9,4.2-9.3,9.2c-0.2,5.5,4,10.1,9.5,10.3C71.8,29.7,72,29.7,72.1,29.7z M72.3,62.3\n",
       "                    c-5.4-0.1-9.9,4.2-10.1,9.7c0,0.2,0,0.3,0,0.5c0.2,5.4,4.5,9.7,9.9,10c5.1,0.1,9.9-4.7,10.1-9.8c0.2-5.5-4-10-9.5-10.3\n",
       "                    C72.6,62.3,72.4,62.3,72.3,62.3z M115,72.5c0.1,5.4,4.5,9.7,9.8,9.9c5.6-0.2,10-4.8,10-10.4c-0.2-5.4-4.6-9.7-10-9.7\n",
       "                    c-5.3-0.1-9.8,4.2-9.9,9.5C115,72.1,115,72.3,115,72.5z M19.5,62.3c-5.4,0.1-9.8,4.4-10,9.8c-0.1,5.1,5.2,10.4,10.2,10.3\n",
       "                    c5.6-0.2,10-4.9,9.8-10.5c-0.1-5.4-4.5-9.7-9.9-9.6C19.6,62.3,19.5,62.3,19.5,62.3z M71.8,134.6c5.9,0.2,10.3-3.9,10.4-9.6\n",
       "                    c0.5-5.5-3.6-10.4-9.1-10.8c-5.5-0.5-10.4,3.6-10.8,9.1c0,0.5,0,0.9,0,1.4c-0.2,5.3,4,9.8,9.3,10\n",
       "                    C71.6,134.6,71.7,134.6,71.8,134.6z\"/>\n",
       "            </g>\n",
       "        </svg>\n",
       "        <table>\n",
       "            <tr>\n",
       "                <td style=\"text-align: left\"><b>Python version:</b></td>\n",
       "                <td style=\"text-align: left\"><b>3.8.13</b></td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                <td style=\"text-align: left\"><b>Ray version:</b></td>\n",
       "                <td style=\"text-align: left\"><b> 2.0.1</b></td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "    <td style=\"text-align: left\"><b>Dashboard:</b></td>\n",
       "    <td style=\"text-align: left\"><b><a href=\"http://127.0.0.1:8265\" target=\"_blank\">http://127.0.0.1:8265</a></b></td>\n",
       "</tr>\n",
       "\n",
       "        </table>\n",
       "    </div>\n",
       "</div>\n"
      ],
      "text/plain": [
       "RayContext(dashboard_url='127.0.0.1:8265', python_version='3.8.13', ray_version='2.0.1', ray_commit='03b6bc7b5a305877501110ec04710a9c57011479', address_info={'node_ip_address': '127.0.0.1', 'raylet_ip_address': '127.0.0.1', 'redis_address': None, 'object_store_address': '/tmp/ray/session_2022-11-13_12-07-51_540487_96694/sockets/plasma_store', 'raylet_socket_name': '/tmp/ray/session_2022-11-13_12-07-51_540487_96694/sockets/raylet', 'webui_url': '127.0.0.1:8265', 'session_dir': '/tmp/ray/session_2022-11-13_12-07-51_540487_96694', 'metrics_export_port': 50232, 'gcs_address': '127.0.0.1:62876', 'address': '127.0.0.1:62876', 'dashboard_agent_listen_port': 52365, 'node_id': '550f625d41ccae393dca8e81150f2e00b6520a7dc92c69fe2255af81'})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ray\n",
    "\n",
    "if ray.is_initialized():\n",
    "    ray.shutdown()\n",
    "ray.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c1d35991",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'memory': 7626799514.0, 'CPU': 8.0, 'object_store_memory': 2147483648.0, 'node:127.0.0.1': 1.0}\n"
     ]
    }
   ],
   "source": [
    "print(ray.cluster_resources())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c9b25453",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sklearn: 1.1.2\n"
     ]
    }
   ],
   "source": [
    "# import standard sklearn libraries\n",
    "import sklearn\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "print(f\"sklearn: {sklearn.__version__}\")\n",
    "\n",
    "# import ray libraries\n",
    "from ray import air, tune\n",
    "from ray.air import session\n",
    "from ray.air.checkpoint import Checkpoint\n",
    "\n",
    "# set global random seed for sklearn models\n",
    "np.random.seed(415)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e5e7a2d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For benchmarking purposes, we can print the times of various operations.\n",
    "# In order to reduce clutter in the output, this is set to False by default.\n",
    "PRINT_TIMES = False\n",
    "\n",
    "def print_time(msg: str):\n",
    "    if PRINT_TIMES:\n",
    "        print(msg)\n",
    "        \n",
    "# To speed things up, weâ€™ll only use a small subset of the full dataset consisting of two last months of 2019.\n",
    "# You can choose to use the full dataset for 2018-2019 by setting the SMOKE_TEST variable to False.\n",
    "SMOKE_TEST = True\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "458fd0bb",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Define how to load and prepare Parquet data <a class=\"anchor\" id=\"load_data\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e46dee70",
   "metadata": {},
   "source": [
    "First, we need to load some data.  Since the NYC Taxi dataset is fairly large, we will filter files first into a PyArrow dataset. And then in the next cell after, we will filter the data on read into a PyArrow table and convert that to a pandas dataframe.\n",
    "\n",
    "```{tip}\n",
    "Use PyArrow dataset and table for reading or writing large parquet files, since its native multithreaded C++ adpater is faster than pandas read_parquet, even using engine=pyarrow.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "11434cc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NYC Taxi using 18 file(s)!\n",
      "s3_files: ['s3://anonymous@air-example-data/ursa-labs-taxi-data/by_year/2018/01/data.parquet/4d6bc4368704460d90c92c22e05a2220_000000.parquet', 's3://anonymous@air-example-data/ursa-labs-taxi-data/by_year/2018/02/data.parquet/e817946252d1409b93964685130e76cb_000000.parquet', 's3://anonymous@air-example-data/ursa-labs-taxi-data/by_year/2018/03/data.parquet/0b7e5121a4904c64be5e91ceec0eee2f_000000.parquet', 's3://anonymous@air-example-data/ursa-labs-taxi-data/by_year/2018/04/data.parquet/f40c2c2806e548bfac8336de9c19a423_000000.parquet', 's3://anonymous@air-example-data/ursa-labs-taxi-data/by_year/2018/05/data.parquet/a5de27164fda47988dec2576685656ae_000000.parquet', 's3://anonymous@air-example-data/ursa-labs-taxi-data/by_year/2018/06/data.parquet/df104576ffed4e308b72941df90f7790_000000.parquet', 's3://anonymous@air-example-data/ursa-labs-taxi-data/by_year/2018/07/data.parquet/ccdef45e50de4678b7e589155f372a3d_000000.parquet', 's3://anonymous@air-example-data/ursa-labs-taxi-data/by_year/2018/08/data.parquet/9d605bf8abf84655997d491bc5a10a4c_000000.parquet', 's3://anonymous@air-example-data/ursa-labs-taxi-data/by_year/2018/09/data.parquet/b200f3d9bf9f485ebd3b20c0c08711e1_000000.parquet', 's3://anonymous@air-example-data/ursa-labs-taxi-data/by_year/2018/10/data.parquet/20624e28db574114b47de3e43065f014_000000.parquet', 's3://anonymous@air-example-data/ursa-labs-taxi-data/by_year/2018/11/data.parquet/9c3fe546f3d746eeb3225b8150fb26e6_000000.parquet', 's3://anonymous@air-example-data/ursa-labs-taxi-data/by_year/2018/12/data.parquet/d9829239c5d34340a7d9ba256917ed98_000000.parquet', 's3://anonymous@air-example-data/ursa-labs-taxi-data/by_year/2019/01/data.parquet/ecce6478ad09480cbc8539e0b6197c2d_000000.parquet', 's3://anonymous@air-example-data/ursa-labs-taxi-data/by_year/2019/02/data.parquet/5bc40cf9bc1145cbb0867d39064daa01_000000.parquet', 's3://anonymous@air-example-data/ursa-labs-taxi-data/by_year/2019/03/data.parquet/8b894872a484458cbd5a6cd0425b77df_000000.parquet', 's3://anonymous@air-example-data/ursa-labs-taxi-data/by_year/2019/04/data.parquet/7e490662e39c4bfe8c64c6a2c45c9e8b_000000.parquet', 's3://anonymous@air-example-data/ursa-labs-taxi-data/by_year/2019/05/data.parquet/359c21b3e28f40328e68cf66f7ba40e2_000000.parquet', 's3://anonymous@air-example-data/ursa-labs-taxi-data/by_year/2019/06/data.parquet/ab5b9d2b8cc94be19346e260b543ec35_000000.parquet']\n",
      "Locations: [24, 140, 141, 257, 239, 143, 238, 170, 229, 113, 224, 79, 100, 189, 236, 162, 61, 75, 233, 125, 146, 234, 263, 237, 166, 249, 151, 231, 116, 145, 82, 164, 68, 161, 48, 90, 13, 148, 88, 87, 213, 74, 25, 158, 246, 12, 142, 66, 186, 232, 4, 262, 202, 107, 50, 42, 205, 137, 80, 114, 43, 181, 255, 7, 126, 89, 220, 133, 209, 152, 160, 41, 144, 211, 203, 65, 20, 167, 168, 129, 139, 223, 33, 227, 26, 36, 217, 226, 193, 3, 112, 163, 62, 117, 95, 21, 261, 17, 127, 174, 198, 243, 256, 241, 70, 244, 108, 173, 197, 260, 67, 259, 208, 72, 52, 40, 230, 73, 216, 37, 228, 10, 14, 225, 179, 83, 45, 11, 49, 97, 200, 250, 69, 196, 178, 188, 138, 106, 18, 22, 77, 169, 132, 182, 1, 201, 28, 159, 222, 54, 32, 78, 51, 149, 218, 60, 183, 55, 157, 212, 175, 147, 119, 235, 128, 136, 16, 92, 247, 195, 57, 121, 39, 9, 35, 56, 185, 248, 251, 58, 190, 177, 86, 171, 155, 258, 153, 192, 47, 91, 124, 191, 130, 240, 94, 122, 98, 206, 102, 165, 71, 76, 254, 219, 215, 118, 134, 156, 63, 131, 135, 245, 180, 242, 252, 93, 64, 85, 29, 15, 176, 31, 34, 210, 81, 194, 120, 172, 19, 150, 38, 111, 214, 123, 46, 101, 204, 27, 115, 53, 44, 8, 5, 109, 23, 59, 221, 6, 253, 96, 84, 207, 154, 187, 184, 30, 105, 2, 199]\n"
     ]
    }
   ],
   "source": [
    "# Define some global variables.\n",
    "target = \"trip_duration\"\n",
    "s3_partitions = pds.dataset(\n",
    "    \"s3://anonymous@air-example-data/ursa-labs-taxi-data/by_year/\",\n",
    "    partitioning=[\"year\", \"month\"],\n",
    ")\n",
    "s3_files = [f\"s3://anonymous@{file}\" for file in s3_partitions.files]\n",
    "\n",
    "# Obtain all location IDs\n",
    "all_location_ids = (\n",
    "    pq.read_table(s3_files[0], columns=[\"dropoff_location_id\"])[\"dropoff_location_id\"]\n",
    "    .unique()\n",
    "    .to_pylist()\n",
    ")\n",
    "# drop [264, 265]\n",
    "all_location_ids.remove(264)\n",
    "all_location_ids.remove(265)\n",
    "\n",
    "# Use smoke testing or not.\n",
    "starting_idx = -1 if SMOKE_TEST else 0\n",
    "sample_locations = [145, 152, 204, 199] if SMOKE_TEST else all_location_ids\n",
    "\n",
    "# Display what data will be used.\n",
    "s3_files = s3_files[starting_idx:]\n",
    "print(f\"NYC Taxi using {len(s3_files)} file(s)!\")\n",
    "print(f\"s3_files: {s3_files}\")\n",
    "print(f\"Locations: {sample_locations}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "552cb39f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to read a pyarrow.Table object using pyarrow parquet \n",
    "def read_data(file: str, sample_id: np.int32) -> pd.DataFrame:\n",
    "    \n",
    "    df = pq.read_table(\n",
    "        file,\n",
    "        filters=[\n",
    "            (\"passenger_count\", \">\", 0),\n",
    "            (\"trip_distance\", \">\", 0),\n",
    "            (\"fare_amount\", \">\", 0),\n",
    "            (\"pickup_location_id\", \"not in\", [264, 265]),\n",
    "            (\"dropoff_location_id\", \"not in\", [264, 265]), \n",
    "            (\"dropoff_location_id\", \"=\", sample_id)\n",
    "        ],\n",
    "        columns=[\n",
    "            \"pickup_at\",\n",
    "            \"dropoff_at\",\n",
    "            \"pickup_location_id\",\n",
    "            \"dropoff_location_id\",\n",
    "            \"passenger_count\",\n",
    "            \"trip_distance\",\n",
    "            \"fare_amount\",\n",
    "        ],\n",
    "    ).to_pandas()\n",
    "\n",
    "    return df\n",
    "\n",
    "# Function to transform a pandas dataframe\n",
    "def transform_df(the_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = the_df.copy()\n",
    "    # calculate trip_duration\n",
    "    df[\"trip_duration\"] = (df[\"dropoff_at\"] - df[\"pickup_at\"]).dt.seconds\n",
    "    # filter trip_durations > 1 minute and less than 24 hours\n",
    "    df = df[df[\"trip_duration\"] > 60]\n",
    "    df = df[df[\"trip_duration\"] < 24 * 60 * 60] \n",
    "    # keep only necessary columns\n",
    "    df.drop([\"dropoff_at\", \"pickup_at\", \"pickup_location_id\", \"fare_amount\"]\n",
    "            , axis=1, inplace=True)\n",
    "    df[\"dropoff_location_id\"] = df[\"dropoff_location_id\"].fillna(-1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d215df4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "# # Test reading data.\n",
    "# import itertools\n",
    "# my_list = itertools.product(s3_files, sample_locations)\n",
    "\n",
    "# # [print(f[0], f[1]) for f in my_list]  \n",
    "# df_list = [read_data(f[0], f[1]) for f in my_list]\n",
    "# df_raw = pd.concat(df_list, ignore_index=True)\n",
    "# # Transform data.\n",
    "# df = transform_batch(df_raw)\n",
    "\n",
    "# # Inspect the pandas dataframe.\n",
    "# df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65b094b8",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Define your Ray Tune Search Space and Search Algorithm <a class=\"anchor\" id=\"define_search_space\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31b4ce8a",
   "metadata": {},
   "source": [
    "In this notebook, we will use Ray Tune to run parallel training jobs per dropoff location.  The training jobs will be defined using a search space and simple grid search.  Depending on your need, fancier search spaces and search algorithms are possible with Tune. \n",
    "\n",
    "**First, define a search space of experiment trials to run.**  \n",
    "> The typical use case for Tune search spaces are for hypterparameter tuning.  In our case, we are defining a Tune search space in a way to allow for training jobs to be conducted automatically.  Each training job will run on a different data partition (taxi dropoff location) and use a different model.\n",
    "\n",
    "Common search algorithms include grid search, random search, and Bayesian optimization.  For more details, see [Working with Tune Search Spaces](https://docs.ray.io/en/master/tune/tutorials/tune-search-spaces.html#tune-search-space-tutorial).  Deciding the best combination of search space and search algorithm is part of the art of being a Data Scientist and depends on the data, algorithm, and problem being solved!  \n",
    "\n",
    "**Next, define a search algorithm.**\n",
    ">Ray Tune will use the search space and the specified search algorithm to generate multiple configurations, each of which will be evaluated in a separate Trial on a Ray Cluster. Ray Tune will take care of orchestrating those Trials automatically.  Specifically, Ray Tune will pass a config dictionary to each partition and make a Trainable function call.\n",
    "\n",
    "**Below, we define our search space consists of:**\n",
    "- 2 different Scikit-learn algorithms \n",
    "- Some or all NYC taxi drop-off locations. \n",
    "\n",
    "**Also below, we define our search algorithm is:**\n",
    "- Grid search.\n",
    "\n",
    "What this means is every algorithm will be applied to every NYC Taxi drop-off location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e7998794",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Define a search space.\n",
    "search_space = {\n",
    "    \"model\": tune.grid_search([LinearRegression(fit_intercept=True), \n",
    "                                DecisionTreeRegressor(max_depth=3)]),\n",
    "    \"location\": tune.grid_search(sample_locations),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ff19e36",
   "metadata": {},
   "source": [
    "## Define a Trainable (callable) function <a class=\"anchor\" id=\"define_trainable\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b00d52da",
   "metadata": {},
   "source": [
    "ðŸ“ˆ Typically when you are running Data Science experiments, you want to be able to keep track of summary metrics for each trial, so you can decide at the end which trials were best.  That way, you can decide which model to deploy.\n",
    "\n",
    "ðŸ‡« Next, we define a trainable function in order to train and evaluate a scikit-learn model on a data partition.  This function will be called in parallel by every Tune trial.  Inside this trainable function, we will:\n",
    "- Add detailed metrics we want to report (each model's loss or error). \n",
    "- Checkpoint each model for easy deployment later.\n",
    "\n",
    "ðŸ“– **The metrics defined inside the trainable function will appear in the Ray Tune experiment summary table.**\n",
    "```{tip}\n",
    "Ray Tune has two ways of defining a trainable, namely the [Function API](https://docs.ray.io/en/latest/tune/api_docs/trainable.html#trainable-docs) and the Class API. Both are valid ways of defining a trainable, but *the Function API is generally recommended*.\n",
    "```\n",
    "\n",
    "**In the cell below, we define a \"Trainable\" function called `train_model()`**.  \n",
    "- The input is a config dictionary argument. \n",
    "- The output can be a simple dictionary of metrics which will be reported back to Tune.  \n",
    "- We will [checkpoint](https://docs.ray.io/en/master/ray-air/key-concepts.html#checkpoints) save each model in addition to reporting each trial's metrics.\n",
    "  > For checkpointing, we use `ray.air.checkpoint.Checkpoint`.  *Ray AIR includes integrations to popular ML libraries, including Scikit-learn*.  This makes it possible to use the convenient AIR API abstractions, without having to specify code details of the Scikit-learn library itself.\n",
    "- Since we are using **grid search**, this means `train_model()` will be run *in parallel for every permutation* in the Tune search space!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4c5c4e2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Define a custom train function\n",
    "def train_model(config: dict):\n",
    "\n",
    "    model = config['model']\n",
    "    the_location = config['location']\n",
    "    \n",
    "    # Load data.\n",
    "    df_list = [read_data(f, the_location) for f in s3_files]   \n",
    "    df_raw = pd.concat(df_list, ignore_index=True)\n",
    "    df = transform_df(df_raw)\n",
    "    \n",
    "    # We need at least 10 rows to create a train / test split.\n",
    "    if len(df) < 10:\n",
    "        print_time(f\"Data batch for LocID {the_location} is empty or smaller than 10 rows\")\n",
    "        return\n",
    "        \n",
    "    # Train/valid split.\n",
    "    train_df, valid_df = train_test_split(df, test_size=0.2, shuffle=True)\n",
    "    train_X = train_df[[\"passenger_count\", \"trip_distance\"]]\n",
    "    train_y = train_df.trip_duration\n",
    "    valid_X = valid_df[[\"passenger_count\", \"trip_distance\"]]\n",
    "    valid_y = valid_df.trip_duration\n",
    "    print(f\"pandas {the_location} size: {df.info(verbose = False)}\")\n",
    "\n",
    "    # Train model.\n",
    "    model = model.fit(train_X, train_y)\n",
    "    pred_y = model.predict(valid_X)\n",
    "\n",
    "    # Evaluate.\n",
    "    error = sklearn.metrics.mean_absolute_error(valid_y, pred_y)\n",
    "\n",
    "    # Define a model checkpoint using AIR API.  \n",
    "    # https://docs.ray.io/en/latest/tune/tutorials/tune-checkpoints.html\n",
    "    checkpoint = ray.air.checkpoint.Checkpoint.from_dict({\n",
    "        \"model\": model, \n",
    "        \"location_id\": the_location})\n",
    "\n",
    "    # Save checkpoint and report back metrics, using ray.air.session.report()\n",
    "    # The metrics you specify here will appear in Tune summary table.\n",
    "    # They will also be recorded in Tune results under `metrics`.\n",
    "    metrics = dict(error = error)\n",
    "    session.report(\n",
    "            metrics, \n",
    "            checkpoint=checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38a2d25f",
   "metadata": {},
   "source": [
    "## Run batch training on Ray Tune <a class=\"anchor\" id=\"run_tune_search\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68f5d7ee",
   "metadata": {},
   "source": [
    "**In the cell below, we configure the resources allocated per trial.** \n",
    "\n",
    "Tune uses this resources allocation to control the parallelism. For example, if each trial was configured to use 4 CPUs, and the cluster had only 32 CPUs, then Tune will limit the number of concurrent trials to 8 to avoid overloading the cluster. For more information, see [A Guide To Parallelism and Resources](https://docs.ray.io/en/master/tune/tutorials/tune-resources.html#tune-parallelism)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "015c314b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Customize resources per trial, here we set 1 CPU each.\n",
    "train_model = tune.with_resources(train_model, {\"cpu\": 1})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac8b4f58",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "**Now we are ready to kick off a Ray Tune experiment!**  \n",
    "\n",
    "Recall what we are doing, high level, is training several different models per dropoff location.  We are using Ray Tune so we can run all these trials in parallel.  At the end, we will inspect the results of the experiment and deploy only the best model per dropoff location.\n",
    "\n",
    "**In the cell below, we use AIR configs and run the experiment using `tuner.fit()`.** \n",
    "\n",
    "Tune will report on experiment status, and after the experiment finishes, you can inspect the results. \n",
    "\n",
    "In the AIR config below, we have specified a local directory `my_Tune_logs` for logging instead of the default `~/ray_results` directory. Giving your logs a project name makes them easier to find.  Also giving a relative path, means you can see your logs inside the Jupyter browser.  Learn more about logging Tune results at [How to configure logging in Tune](https://docs.ray.io/en/master/tune/tutorials/tune-output.html#tune-logging).\n",
    "\n",
    "Tune can [retry failed experiments automatically](https://docs.ray.io/en/master/tune/tutorials/tune-stopping.html#tune-stopping-guide), as well as entire experiments.  This is necessary in case a node on your remote cluster fails (when running on a cloud such as AWS or GCP).\n",
    "\n",
    "ðŸ’¡ Right-click on the cell below and choose \"Enable Scrolling for Outputs\"! This will make it easier to view, since model training output can be very long!\n",
    "\n",
    "**In the output below and in the Ray Dashboard, you can see that 518 models, using 18 NYC Taxi S3 files dating from 2018/01 to 2019/06 (split into partitions approx 1GiB each), were simultaneously trained on a 23-node AWS cluster of [m5.4xlarge](https://aws.amazon.com/ec2/instance-types/m5/)s, within 37 minutes.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d4a9e170",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-11-13 12:20:47 (running for 00:09:08.07)<br>Memory usage on this node: 12.1/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/8 CPUs, 0/0 GPUs, 0.0/7.1 GiB heap, 0.0/2.0 GiB objects<br>Result logdir: /Users/christy/Documents/github_ray_temp/ray/doc/source/ray-air/examples/my_Tune_logs/batch_tuning<br>Number of trials: 8/8 (8 TERMINATED)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name             </th><th>status    </th><th>loc            </th><th style=\"text-align: right;\">  location</th><th>model               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   error</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>train_model_6232c_00000</td><td>TERMINATED</td><td>127.0.0.1:97325</td><td style=\"text-align: right;\">       145</td><td>LinearRegression()  </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         526.048</td><td style=\"text-align: right;\"> 608.163</td></tr>\n",
       "<tr><td>train_model_6232c_00001</td><td>TERMINATED</td><td>127.0.0.1:97331</td><td style=\"text-align: right;\">       152</td><td>LinearRegression()  </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         500.508</td><td style=\"text-align: right;\"> 508.139</td></tr>\n",
       "<tr><td>train_model_6232c_00002</td><td>TERMINATED</td><td>127.0.0.1:97332</td><td style=\"text-align: right;\">       204</td><td>LinearRegression()  </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         525.177</td><td style=\"text-align: right;\">1432.65 </td></tr>\n",
       "<tr><td>train_model_6232c_00003</td><td>TERMINATED</td><td>127.0.0.1:97334</td><td style=\"text-align: right;\">       199</td><td>LinearRegression()  </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         524.266</td><td style=\"text-align: right;\"> 292.392</td></tr>\n",
       "<tr><td>train_model_6232c_00004</td><td>TERMINATED</td><td>127.0.0.1:97335</td><td style=\"text-align: right;\">       145</td><td>DecisionTreeReg_0f40</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         490.44 </td><td style=\"text-align: right;\"> 606.65 </td></tr>\n",
       "<tr><td>train_model_6232c_00005</td><td>TERMINATED</td><td>127.0.0.1:97336</td><td style=\"text-align: right;\">       152</td><td>DecisionTreeReg_0520</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         532.259</td><td style=\"text-align: right;\"> 524.958</td></tr>\n",
       "<tr><td>train_model_6232c_00006</td><td>TERMINATED</td><td>127.0.0.1:97337</td><td style=\"text-align: right;\">       204</td><td>DecisionTreeReg_2df0</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         497.803</td><td style=\"text-align: right;\">1322.66 </td></tr>\n",
       "<tr><td>train_model_6232c_00007</td><td>TERMINATED</td><td>127.0.0.1:97340</td><td style=\"text-align: right;\">       199</td><td>DecisionTreeReg_9160</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         542.74 </td><td style=\"text-align: right;\"> 202.84 </td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-13 12:19:55,436\tINFO tensorboardx.py:267 -- Removed the following hyperparameter values when logging to tensorboard: {'model': DecisionTreeRegressor(max_depth=3)}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial train_model_6232c_00004 reported error=606.6498867711051,should_checkpoint=True with parameters={'model': DecisionTreeRegressor(max_depth=3), 'location': 145}.\n",
      "Trial train_model_6232c_00004 completed. Last result: error=606.6498867711051,should_checkpoint=True\n",
      "\u001b[2m\u001b[36m(train_model pid=97335)\u001b[0m <class 'pandas.core.frame.DataFrame'>\n",
      "\u001b[2m\u001b[36m(train_model pid=97335)\u001b[0m Int64Index: 401142 entries, 0 to 405402\n",
      "\u001b[2m\u001b[36m(train_model pid=97335)\u001b[0m Columns: 4 entries, dropoff_location_id to trip_duration\n",
      "\u001b[2m\u001b[36m(train_model pid=97335)\u001b[0m dtypes: float32(1), int32(1), int64(1), int8(1)\n",
      "\u001b[2m\u001b[36m(train_model pid=97335)\u001b[0m memory usage: 9.6 MB\n",
      "\u001b[2m\u001b[36m(train_model pid=97335)\u001b[0m pandas 145 size: None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-13 12:20:02,831\tINFO tensorboardx.py:267 -- Removed the following hyperparameter values when logging to tensorboard: {'model': DecisionTreeRegressor(max_depth=3)}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial train_model_6232c_00006 reported error=1322.657530899312,should_checkpoint=True with parameters={'model': DecisionTreeRegressor(max_depth=3), 'location': 204}.\n",
      "Trial train_model_6232c_00006 completed. Last result: error=1322.657530899312,should_checkpoint=True\n",
      "\u001b[2m\u001b[36m(train_model pid=97337)\u001b[0m <class 'pandas.core.frame.DataFrame'>\n",
      "\u001b[2m\u001b[36m(train_model pid=97337)\u001b[0m Int64Index: 800 entries, 0 to 804\n",
      "\u001b[2m\u001b[36m(train_model pid=97337)\u001b[0m Columns: 4 entries, dropoff_location_id to trip_duration\n",
      "\u001b[2m\u001b[36m(train_model pid=97337)\u001b[0m dtypes: float32(1), int32(1), int64(1), int8(1)\n",
      "\u001b[2m\u001b[36m(train_model pid=97337)\u001b[0m memory usage: 19.5 KB\n",
      "\u001b[2m\u001b[36m(train_model pid=97337)\u001b[0m pandas 204 size: None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-13 12:20:05,245\tINFO tensorboardx.py:267 -- Removed the following hyperparameter values when logging to tensorboard: {'model': LinearRegression()}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial train_model_6232c_00001 reported error=508.1394279547598,should_checkpoint=True with parameters={'model': LinearRegression(), 'location': 152}.\n",
      "Trial train_model_6232c_00001 completed. Last result: error=508.1394279547598,should_checkpoint=True\n",
      "\u001b[2m\u001b[36m(train_model pid=97331)\u001b[0m <class 'pandas.core.frame.DataFrame'>\n",
      "\u001b[2m\u001b[36m(train_model pid=97331)\u001b[0m Int64Index: 241456 entries, 0 to 242361\n",
      "\u001b[2m\u001b[36m(train_model pid=97331)\u001b[0m Columns: 4 entries, dropoff_location_id to trip_duration\n",
      "\u001b[2m\u001b[36m(train_model pid=97331)\u001b[0m dtypes: float32(1), int32(1), int64(1), int8(1)\n",
      "\u001b[2m\u001b[36m(train_model pid=97331)\u001b[0m memory usage: 5.8 MB\n",
      "\u001b[2m\u001b[36m(train_model pid=97331)\u001b[0m pandas 152 size: None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-13 12:20:28,192\tINFO tensorboardx.py:267 -- Removed the following hyperparameter values when logging to tensorboard: {'model': LinearRegression()}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial train_model_6232c_00000 reported error=608.1634502587052,should_checkpoint=True with parameters={'model': LinearRegression(), 'location': 145}.\n",
      "Trial train_model_6232c_00000 completed. Last result: error=608.1634502587052,should_checkpoint=True\n",
      "\u001b[2m\u001b[36m(train_model pid=97325)\u001b[0m <class 'pandas.core.frame.DataFrame'>\n",
      "\u001b[2m\u001b[36m(train_model pid=97325)\u001b[0m Int64Index: 401142 entries, 0 to 405402\n",
      "\u001b[2m\u001b[36m(train_model pid=97325)\u001b[0m Columns: 4 entries, dropoff_location_id to trip_duration\n",
      "\u001b[2m\u001b[36m(train_model pid=97325)\u001b[0m dtypes: float32(1), int32(1), int64(1), int8(1)\n",
      "\u001b[2m\u001b[36m(train_model pid=97325)\u001b[0m memory usage: 9.6 MB\n",
      "\u001b[2m\u001b[36m(train_model pid=97325)\u001b[0m pandas 145 size: None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-13 12:20:29,224\tINFO tensorboardx.py:267 -- Removed the following hyperparameter values when logging to tensorboard: {'model': LinearRegression()}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial train_model_6232c_00003 reported error=292.3917602539062,should_checkpoint=True with parameters={'model': LinearRegression(), 'location': 199}.\n",
      "Trial train_model_6232c_00003 completed. Last result: error=292.3917602539062,should_checkpoint=True\n",
      "\u001b[2m\u001b[36m(train_model pid=97334)\u001b[0m <class 'pandas.core.frame.DataFrame'>\n",
      "\u001b[2m\u001b[36m(train_model pid=97334)\u001b[0m Int64Index: 23 entries, 0 to 23\n",
      "\u001b[2m\u001b[36m(train_model pid=97334)\u001b[0m Columns: 4 entries, dropoff_location_id to trip_duration\n",
      "\u001b[2m\u001b[36m(train_model pid=97334)\u001b[0m dtypes: float32(1), int32(1), int64(1), int8(1)\n",
      "\u001b[2m\u001b[36m(train_model pid=97334)\u001b[0m memory usage: 575.0 bytes\n",
      "\u001b[2m\u001b[36m(train_model pid=97334)\u001b[0m pandas 199 size: None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-13 12:20:29,938\tINFO tensorboardx.py:267 -- Removed the following hyperparameter values when logging to tensorboard: {'model': LinearRegression()}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial train_model_6232c_00002 reported error=1432.652197265625,should_checkpoint=True with parameters={'model': LinearRegression(), 'location': 204}.\n",
      "Trial train_model_6232c_00002 completed. Last result: error=1432.652197265625,should_checkpoint=True\n",
      "\u001b[2m\u001b[36m(train_model pid=97332)\u001b[0m <class 'pandas.core.frame.DataFrame'>\n",
      "\u001b[2m\u001b[36m(train_model pid=97332)\u001b[0m Int64Index: 800 entries, 0 to 804\n",
      "\u001b[2m\u001b[36m(train_model pid=97332)\u001b[0m Columns: 4 entries, dropoff_location_id to trip_duration\n",
      "\u001b[2m\u001b[36m(train_model pid=97332)\u001b[0m dtypes: float32(1), int32(1), int64(1), int8(1)\n",
      "\u001b[2m\u001b[36m(train_model pid=97332)\u001b[0m memory usage: 19.5 KB\n",
      "\u001b[2m\u001b[36m(train_model pid=97332)\u001b[0m pandas 204 size: None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-13 12:20:37,259\tINFO tensorboardx.py:267 -- Removed the following hyperparameter values when logging to tensorboard: {'model': DecisionTreeRegressor(max_depth=3)}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial train_model_6232c_00005 reported error=524.9583710923414,should_checkpoint=True with parameters={'model': DecisionTreeRegressor(max_depth=3), 'location': 152}.\n",
      "\u001b[2m\u001b[36m(train_model pid=97336)\u001b[0m <class 'pandas.core.frame.DataFrame'>\n",
      "\u001b[2m\u001b[36m(train_model pid=97336)\u001b[0m Int64Index: 241456 entries, 0 to 242361\n",
      "\u001b[2m\u001b[36m(train_model pid=97336)\u001b[0m Columns: 4 entries, dropoff_location_id to trip_duration\n",
      "\u001b[2m\u001b[36m(train_model pid=97336)\u001b[0m dtypes: float32(1), int32(1), int64(1), int8(1)\n",
      "\u001b[2m\u001b[36m(train_model pid=97336)\u001b[0m memory usage: 5.8 MB\n",
      "\u001b[2m\u001b[36m(train_model pid=97336)\u001b[0m pandas 152 size: None\n",
      "Trial train_model_6232c_00005 completed. Last result: error=524.9583710923414,should_checkpoint=True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-13 12:20:47,795\tINFO tensorboardx.py:267 -- Removed the following hyperparameter values when logging to tensorboard: {'model': DecisionTreeRegressor(max_depth=3)}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial train_model_6232c_00007 reported error=202.84,should_checkpoint=True with parameters={'model': DecisionTreeRegressor(max_depth=3), 'location': 199}.\n",
      "Trial train_model_6232c_00007 completed. Last result: error=202.84,should_checkpoint=True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-13 12:20:47,908\tINFO tune.py:758 -- Total run time: 548.18 seconds (548.06 seconds for the tuning loop).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(train_model pid=97340)\u001b[0m <class 'pandas.core.frame.DataFrame'>\n",
      "\u001b[2m\u001b[36m(train_model pid=97340)\u001b[0m Int64Index: 23 entries, 0 to 23\n",
      "\u001b[2m\u001b[36m(train_model pid=97340)\u001b[0m Columns: 4 entries, dropoff_location_id to trip_duration\n",
      "\u001b[2m\u001b[36m(train_model pid=97340)\u001b[0m dtypes: float32(1), int32(1), int64(1), int8(1)\n",
      "\u001b[2m\u001b[36m(train_model pid=97340)\u001b[0m memory usage: 575.0 bytes\n",
      "\u001b[2m\u001b[36m(train_model pid=97340)\u001b[0m pandas 199 size: None\n",
      "Total number of models: 8\n",
      "TOTAL TIME TAKEN: 548.19 seconds\n",
      "Best result: {'model': DecisionTreeRegressor(max_depth=3), 'location': 199}\n"
     ]
    }
   ],
   "source": [
    "# Define a tuner object using Ray AIR Tuner API\n",
    "tuner = tune.Tuner(\n",
    "    train_model, \n",
    "    param_space=search_space,\n",
    "    run_config=air.RunConfig(\n",
    "        \n",
    "        #redirect logs to relative path instead of default ~/ray_results/\n",
    "        local_dir = \"my_Tune_logs\",\n",
    "        name = \"batch_tuning\",\n",
    "\n",
    "        # Set Ray Tune verbosity.  Print summary table only with levels 2 or 3.\n",
    "        verbose=2,\n",
    "        ),\n",
    ")\n",
    "\n",
    "# 4. Run the experiment with Ray Tune\n",
    "start = time.time()\n",
    "results = tuner.fit()\n",
    "total_time_taken = time.time() - start\n",
    "\n",
    "# Print some training stats\n",
    "print(f\"Total number of models: {len(results)}\")\n",
    "print(f\"TOTAL TIME TAKEN: {total_time_taken:.2f} seconds\")\n",
    "best_result = results.get_best_result(metric=\"error\", mode=\"min\").config\n",
    "print(f\"Best result: {best_result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b320511",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "**After the Tune experiment has run, select the best model per dropoff location.**\n",
    "\n",
    "We can assemble the Tune results ([ResultGrid object](https://docs.ray.io/en/master/tune/examples/tune_analyze_results.html)) into a pandas dataframe, then sort by minimum error, to select the best model per dropoff location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "aa190f9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "location_id      int64\n",
      "model           object\n",
      "error          float64\n",
      "checkpoint      object\n",
      "dtype: object\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>location_id</th>\n",
       "      <th>model</th>\n",
       "      <th>error</th>\n",
       "      <th>checkpoint</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>145</td>\n",
       "      <td>LinearRegression()</td>\n",
       "      <td>563.443913</td>\n",
       "      <td>Checkpoint(local_path=/Users/christy/Documents...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>152</td>\n",
       "      <td>LinearRegression()</td>\n",
       "      <td>474.723352</td>\n",
       "      <td>Checkpoint(local_path=/Users/christy/Documents...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>204</td>\n",
       "      <td>LinearRegression()</td>\n",
       "      <td>1561.352322</td>\n",
       "      <td>Checkpoint(local_path=/Users/christy/Documents...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>199</td>\n",
       "      <td>LinearRegression()</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>145</td>\n",
       "      <td>DecisionTreeRegressor(max_depth=3)</td>\n",
       "      <td>559.096856</td>\n",
       "      <td>Checkpoint(local_path=/Users/christy/Documents...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   location_id                               model         error  \\\n",
       "0          145                  LinearRegression()    563.443913   \n",
       "1          152                  LinearRegression()    474.723352   \n",
       "2          204                  LinearRegression()   1561.352322   \n",
       "3          199                  LinearRegression()  10000.000000   \n",
       "4          145  DecisionTreeRegressor(max_depth=3)    559.096856   \n",
       "\n",
       "                                          checkpoint  \n",
       "0  Checkpoint(local_path=/Users/christy/Documents...  \n",
       "1  Checkpoint(local_path=/Users/christy/Documents...  \n",
       "2  Checkpoint(local_path=/Users/christy/Documents...  \n",
       "3                                               None  \n",
       "4  Checkpoint(local_path=/Users/christy/Documents...  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get a list of training loss errors\n",
    "errors = []\n",
    "[errors.append(i.metrics.get('error',10000.0)) for i in results]\n",
    "\n",
    "# get a list of checkpoints\n",
    "checkpoints = []\n",
    "[checkpoints.append(i.checkpoint) for i in results] \n",
    "\n",
    "# get a list of locations\n",
    "locations = []\n",
    "[locations.append(i.config['location']) for i in results]\n",
    "\n",
    "# get a list of models\n",
    "models = []\n",
    "[models.append(i.config['model']) for i in results]\n",
    "\n",
    "# Assemble a pandas dataframe from Tune results\n",
    "results_df = pd.DataFrame(zip(locations, models, errors,checkpoints),\n",
    "                          columns = ['location_id', 'model', 'error', 'checkpoint']\n",
    "                         )\n",
    "print(results_df.dtypes)\n",
    "results_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "50de1600",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model          object\n",
      "error         float64\n",
      "checkpoint     object\n",
      "dtype: object\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>error</th>\n",
       "      <th>checkpoint</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>location_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>152</th>\n",
       "      <td>DecisionTreeRegressor(max_depth=3)</td>\n",
       "      <td>465.135511</td>\n",
       "      <td>Checkpoint(local_path=/Users/christy/Documents...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>DecisionTreeRegressor(max_depth=3)</td>\n",
       "      <td>559.096856</td>\n",
       "      <td>Checkpoint(local_path=/Users/christy/Documents...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>204</th>\n",
       "      <td>DecisionTreeRegressor(max_depth=3)</td>\n",
       "      <td>869.728395</td>\n",
       "      <td>Checkpoint(local_path=/Users/christy/Documents...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>LinearRegression()</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          model         error  \\\n",
       "location_id                                                     \n",
       "152          DecisionTreeRegressor(max_depth=3)    465.135511   \n",
       "145          DecisionTreeRegressor(max_depth=3)    559.096856   \n",
       "204          DecisionTreeRegressor(max_depth=3)    869.728395   \n",
       "199                          LinearRegression()  10000.000000   \n",
       "\n",
       "                                                    checkpoint  \n",
       "location_id                                                     \n",
       "152          Checkpoint(local_path=/Users/christy/Documents...  \n",
       "145          Checkpoint(local_path=/Users/christy/Documents...  \n",
       "204          Checkpoint(local_path=/Users/christy/Documents...  \n",
       "199                                                       None  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Keep only 1 model per location_id with minimum error\n",
    "final_df = results_df.dropna()\n",
    "final_df = final_df.loc[final_df.groupby('location_id')['error'].idxmin()].copy()\n",
    "final_df.sort_values(by=[\"error\"], inplace=True)\n",
    "final_df.set_index('location_id', inplace=True, drop=True)\n",
    "print(final_df.dtypes)\n",
    "final_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24a06818",
   "metadata": {},
   "source": [
    "## Load a model from checkpoint and perform inference  <a class=\"anchor\" id=\"load_checkpoint\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f07b3c31",
   "metadata": {},
   "source": [
    "```{tip}\n",
    "[Ray AIR Predictors](https://docs.ray.io/en/latest/ray-air/predictors.html) make batch inference easy since they have internal logic to parallelize the inference.\n",
    "```\n",
    "  \n",
    "In this notebook, we will restore a single scikit-learn model directly from checkpoint, and demonstrate it can be used for inference.\n",
    "\n",
    "Below, we can easily obtain AIR Checkpoint objects from the Tune results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8e5b2a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a dropoff location\n",
    "the_location = final_df.index[25]\n",
    "the_location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "700d2c56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a checkpoint directly from the pandas dataframe of Tune results\n",
    "checkpoint = final_df.checkpoint[the_location]\n",
    "print(type(checkpoint))\n",
    "\n",
    "# Restore a model from checkpoint\n",
    "model = checkpoint.to_dict()['model']\n",
    "print(type(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5f98f5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create some test data\n",
    "df_list = [read_data(f, the_location) for f in s3_files[:1]]   \n",
    "df_raw = pd.concat(df_list, ignore_index=True)\n",
    "df = transform_df(df_raw)\n",
    "\n",
    "# Train/test split.\n",
    "_, test_df = train_test_split(df, test_size=0.2, shuffle=True)\n",
    "test_X = test_df[[\"passenger_count\", \"trip_distance\"]]\n",
    "test_y = np.array(test_df.trip_duration)  #actual values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c970d831",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform inference using restored model from checkpoint\n",
    "pred_y = model.predict(test_X)\n",
    "\n",
    "# Zip together predictions and actuals to visualize\n",
    "pd.DataFrame(zip(pred_y, test_y), \n",
    "             columns = [\"pred_y\", \"trip_duration\"])[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22cf81de",
   "metadata": {},
   "source": [
    "**Compare validation and test error.**\n",
    "\n",
    "During model training we reported error on \"validation\" data (random sample).  Below, we will report error on a pretend \"test\" data set (a different random sample).\n",
    "\n",
    "Do a quick validation that both errors are reasonably close together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54a222db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate restored model on test data.\n",
    "error = sklearn.metrics.mean_absolute_error(test_y, pred_y)\n",
    "print(f\"Test error: {error}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcd72885",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare test error with training validation error\n",
    "print(f\"Validation error: {final_df.error[the_location]}\")\n",
    "\n",
    "# Validation and test errors should be reasonably close together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73175db5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
