{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5e0b7282",
   "metadata": {},
   "source": [
    "# Batch (parallel) Demand Forecasting using PyTorch Forecasting\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93be96cb",
   "metadata": {},
   "source": [
    "**Batch training** and tuning are common tasks in machine learning use-cases. They require training simple models, on data batches, typcially corresponding to different locations, products, etc. Batch training can take less time to process all the data at once, but only if those batches can run in parallel!\n",
    "\n",
    "This notebook showcases how to conduct batch training using forecast algorithms [Prophet](https://github.com/facebook/prophet) and [ARIMA](https://github.com/Nixtla/statsforecast). **Prophet** is a popular open-source library developed by Facebook and designed for automatic forecasting of univariate time series data. **ARIMA** is an older, well-known algorithm for forecasting univariate time series at less fine-grained detail than Prophet.\n",
    "\n",
    "![Batch training diagram](../../data/examples/images/batch-training.svg)\n",
    "\n",
    "For the data, we will use the [NYC Taxi dataset](https://www1.nyc.gov/site/tlc/about/tlc-trip-record-data.page). This popular tabular dataset contains historical taxi pickups by timestamp and location in NYC.\n",
    "\n",
    "For the training, we will train a separate forecasting model to predict #pickups at each location in NYC at daily level for the next 28 days. Specifically, we will use the `pickup_location_id` column in the dataset to group the dataset into data batches. Then we will conduct an experiment for each location, to find the best either Prophet or ARIMA model, per location."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77b3f018",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Contents\n",
    "\n",
    "In this this tutorial, you will learn about:\n",
    " 1. [Define how to load and prepare Parquet data](#prepare_data)\n",
    " 2. [Define your Ray Tune Search Space and Search Algorithm](#define_search_space2)\n",
    " 3. [Define a Trainable (callable) function](#define_trainable2)\n",
    " 4. [Run batch training with Ray Tune](#run_tune_search2)\n",
    " 5. [Load a model from checkpoint and create a forecast](#load_checkpoint2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "385c5f90",
   "metadata": {},
   "source": [
    "# Walkthrough\n",
    "\n",
    "```{tip}\n",
    "Prerequisite for this notebook: Read the [Key Concepts](https://docs.ray.io/en/latest/tune/key-concepts.html) page for Ray Tune.\n",
    "```\n",
    "\n",
    "Let us start by importing a few required libraries, including open-source [Ray](https://github.com/ray-project/ray) itself!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d1613311",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of CPUs in this system: 8\n",
      "numpy: 1.23.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Matplotlib is building the font cache; this may take a moment.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pyarrow: 10.0.1\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import typing\n",
    "num_cpu = os.cpu_count()\n",
    "\n",
    "print(f\"Number of CPUs in this system: {num_cpu}\")\n",
    "from typing import Tuple, List, Union, Optional, Callable\n",
    "from datetime import datetime, timedelta\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "print(f\"numpy: {np.__version__}\")\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "# import scipy\n",
    "\n",
    "# print(f\"scipy: {scipy.__version__}\")\n",
    "import pyarrow\n",
    "import pyarrow.parquet as pq\n",
    "import pyarrow.dataset as pds\n",
    "\n",
    "print(f\"pyarrow: {pyarrow.__version__}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "595274d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import GPUtil #GPU status from NVIDA GPUs\n",
    "# len(GPUtil.getGPUs())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "59d0333a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-15 16:56:06,258\tINFO worker.py:1230 -- Using address localhost:9031 set in the environment variable RAY_ADDRESS\n",
      "2022-12-15 16:56:06,259\tINFO worker.py:1352 -- Connecting to existing Ray cluster at address: 172.31.230.43:9031...\n",
      "2022-12-15 16:56:06,266\tINFO worker.py:1529 -- Connected to Ray cluster. View the dashboard at \u001b[1m\u001b[32mhttps://console.anyscale-staging.com/api/v2/sessions/ses_bv6cqua3a9m1ir4yiuzkwmrw/services?redirect_to=dashboard \u001b[39m\u001b[22m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "    <div style=\"margin-left: 50px;display: flex;flex-direction: row;align-items: center\">\n",
       "        <h3 style=\"color: var(--jp-ui-font-color0)\">Ray</h3>\n",
       "        <svg version=\"1.1\" id=\"ray\" width=\"3em\" viewBox=\"0 0 144.5 144.6\" style=\"margin-left: 3em;margin-right: 3em\">\n",
       "            <g id=\"layer-1\">\n",
       "                <path fill=\"#00a2e9\" class=\"st0\" d=\"M97.3,77.2c-3.8-1.1-6.2,0.9-8.3,5.1c-3.5,6.8-9.9,9.9-17.4,9.6S58,88.1,54.8,81.2c-1.4-3-3-4-6.3-4.1\n",
       "                    c-5.6-0.1-9.9,0.1-13.1,6.4c-3.8,7.6-13.6,10.2-21.8,7.6C5.2,88.4-0.4,80.5,0,71.7c0.1-8.4,5.7-15.8,13.8-18.2\n",
       "                    c8.4-2.6,17.5,0.7,22.3,8c1.3,1.9,1.3,5.2,3.6,5.6c3.9,0.6,8,0.2,12,0.2c1.8,0,1.9-1.6,2.4-2.8c3.5-7.8,9.7-11.8,18-11.9\n",
       "                    c8.2-0.1,14.4,3.9,17.8,11.4c1.3,2.8,2.9,3.6,5.7,3.3c1-0.1,2,0.1,3,0c2.8-0.5,6.4,1.7,8.1-2.7s-2.3-5.5-4.1-7.5\n",
       "                    c-5.1-5.7-10.9-10.8-16.1-16.3C84,38,81.9,37.1,78,38.3C66.7,42,56.2,35.7,53,24.1C50.3,14,57.3,2.8,67.7,0.5\n",
       "                    C78.4-2,89,4.7,91.5,15.3c0.1,0.3,0.1,0.5,0.2,0.8c0.7,3.4,0.7,6.9-0.8,9.8c-1.7,3.2-0.8,5,1.5,7.2c6.7,6.5,13.3,13,19.8,19.7\n",
       "                    c1.8,1.8,3,2.1,5.5,1.2c9.1-3.4,17.9-0.6,23.4,7c4.8,6.9,4.6,16.1-0.4,22.9c-5.4,7.2-14.2,9.9-23.1,6.5c-2.3-0.9-3.5-0.6-5.1,1.1\n",
       "                    c-6.7,6.9-13.6,13.7-20.5,20.4c-1.8,1.8-2.5,3.2-1.4,5.9c3.5,8.7,0.3,18.6-7.7,23.6c-7.9,5-18.2,3.8-24.8-2.9\n",
       "                    c-6.4-6.4-7.4-16.2-2.5-24.3c4.9-7.8,14.5-11,23.1-7.8c3,1.1,4.7,0.5,6.9-1.7C91.7,98.4,98,92.3,104.2,86c1.6-1.6,4.1-2.7,2.6-6.2\n",
       "                    c-1.4-3.3-3.8-2.5-6.2-2.6C99.8,77.2,98.9,77.2,97.3,77.2z M72.1,29.7c5.5,0.1,9.9-4.3,10-9.8c0-0.1,0-0.2,0-0.3\n",
       "                    C81.8,14,77,9.8,71.5,10.2c-5,0.3-9,4.2-9.3,9.2c-0.2,5.5,4,10.1,9.5,10.3C71.8,29.7,72,29.7,72.1,29.7z M72.3,62.3\n",
       "                    c-5.4-0.1-9.9,4.2-10.1,9.7c0,0.2,0,0.3,0,0.5c0.2,5.4,4.5,9.7,9.9,10c5.1,0.1,9.9-4.7,10.1-9.8c0.2-5.5-4-10-9.5-10.3\n",
       "                    C72.6,62.3,72.4,62.3,72.3,62.3z M115,72.5c0.1,5.4,4.5,9.7,9.8,9.9c5.6-0.2,10-4.8,10-10.4c-0.2-5.4-4.6-9.7-10-9.7\n",
       "                    c-5.3-0.1-9.8,4.2-9.9,9.5C115,72.1,115,72.3,115,72.5z M19.5,62.3c-5.4,0.1-9.8,4.4-10,9.8c-0.1,5.1,5.2,10.4,10.2,10.3\n",
       "                    c5.6-0.2,10-4.9,9.8-10.5c-0.1-5.4-4.5-9.7-9.9-9.6C19.6,62.3,19.5,62.3,19.5,62.3z M71.8,134.6c5.9,0.2,10.3-3.9,10.4-9.6\n",
       "                    c0.5-5.5-3.6-10.4-9.1-10.8c-5.5-0.5-10.4,3.6-10.8,9.1c0,0.5,0,0.9,0,1.4c-0.2,5.3,4,9.8,9.3,10\n",
       "                    C71.6,134.6,71.7,134.6,71.8,134.6z\"/>\n",
       "            </g>\n",
       "        </svg>\n",
       "        <table>\n",
       "            <tr>\n",
       "                <td style=\"text-align: left\"><b>Python version:</b></td>\n",
       "                <td style=\"text-align: left\"><b>3.9.12</b></td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                <td style=\"text-align: left\"><b>Ray version:</b></td>\n",
       "                <td style=\"text-align: left\"><b> 2.2.0</b></td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "    <td style=\"text-align: left\"><b>Dashboard:</b></td>\n",
       "    <td style=\"text-align: left\"><b><a href=\"http://console.anyscale-staging.com/api/v2/sessions/ses_bv6cqua3a9m1ir4yiuzkwmrw/services?redirect_to=dashboard\" target=\"_blank\">http://console.anyscale-staging.com/api/v2/sessions/ses_bv6cqua3a9m1ir4yiuzkwmrw/services?redirect_to=dashboard</a></b></td>\n",
       "</tr>\n",
       "\n",
       "        </table>\n",
       "    </div>\n",
       "</div>\n"
      ],
      "text/plain": [
       "RayContext(dashboard_url='console.anyscale-staging.com/api/v2/sessions/ses_bv6cqua3a9m1ir4yiuzkwmrw/services?redirect_to=dashboard', python_version='3.9.12', ray_version='2.2.0', ray_commit='b6af0887ee5f2e460202133791ad941a41f15beb', address_info={'node_ip_address': '172.31.230.43', 'raylet_ip_address': '172.31.230.43', 'redis_address': None, 'object_store_address': '/tmp/ray/session_2022-12-15_16-52-01_323222_162/sockets/plasma_store', 'raylet_socket_name': '/tmp/ray/session_2022-12-15_16-52-01_323222_162/sockets/raylet', 'webui_url': 'console.anyscale-staging.com/api/v2/sessions/ses_bv6cqua3a9m1ir4yiuzkwmrw/services?redirect_to=dashboard', 'session_dir': '/tmp/ray/session_2022-12-15_16-52-01_323222_162', 'metrics_export_port': 54966, 'gcs_address': '172.31.230.43:9031', 'address': '172.31.230.43:9031', 'dashboard_agent_listen_port': 52365, 'node_id': '9f3d39378ad2bcc86e570135f1e86e3023a0ca42d19d78de2d654519'})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ray\n",
    "\n",
    "if ray.is_initialized():\n",
    "    ray.shutdown()\n",
    "ray.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f61a969d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'CPU': 8.0, 'node:172.31.230.43': 1.0, 'object_store_memory': 9146738688.0, 'memory': 18293477376.0}\n"
     ]
    }
   ],
   "source": [
    "print(ray.cluster_resources())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4f095972",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 415\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch: 1.13.1+cu117\n",
      "PyTorch Lightning: 1.6.5\n",
      "pytorch forecasting: 0.10.3\n",
      "tensorboard: 2.11.0\n"
     ]
    }
   ],
   "source": [
    "# import forecasting libraries\n",
    "# PyTorch, PyTorch Lightning, and PyTorch Forecasting\n",
    "import torch  #Pytorch\n",
    "import pytorch_lightning as pl  #PyTorch Lightning convenience APIs for PyTorch\n",
    "import pytorch_forecasting as ptf #PyTorch Forecasting convenience APIs for PyTorch Lightning\n",
    "pl.seed_everything(415)  # Set global random seed\n",
    "\n",
    "# PyTorch visualization uses Tensorboard\n",
    "# import tensorflow as tf #Tensorflow\n",
    "import tensorboard as tb  #Tensorboard\n",
    "tf.io.gfile = tb.compat.tensorflow_stub.io.gfile  #compatibility for PyTorch\n",
    "\n",
    "print(f\"torch: {torch.__version__}\")\n",
    "print(f\"PyTorch Lightning: {pl.__version__}\")\n",
    "print(f\"pytorch forecasting: {ptf.__version__}\")\n",
    "print(f\"tensorboard: {tb.__version__}\")\n",
    "\n",
    "# import ray libraries\n",
    "import ray_lightning\n",
    "from ray_lightning import RayStrategy\n",
    "from ray_lightning.tune import TuneReportCallback, get_tune_resources\n",
    "# from ray import air, tune\n",
    "# from ray.air import session\n",
    "# from ray.air.checkpoint import Checkpoint\n",
    "\n",
    "# print(f\"ray_lightning: {ray_lightning.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8c81a762",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For benchmarking purposes, we can print the times of various operations.\n",
    "# In order to reduce clutter in the output, this is set to False by default.\n",
    "PRINT_TIMES = False\n",
    "\n",
    "\n",
    "def print_time(msg: str):\n",
    "    if PRINT_TIMES:\n",
    "        print(msg)\n",
    "\n",
    "\n",
    "# To speed things up, weâ€™ll only use a small subset of the full dataset consisting of two last months of 2019.\n",
    "# You can choose to use the full dataset for 2018-2019 by setting the SMOKE_TEST variable to False.\n",
    "SMOKE_TEST = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b99878c3",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Define how to load and prepare Parquet data <a class=\"anchor\" id=\"load_data\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdab0e54",
   "metadata": {},
   "source": [
    "First, we need to load some data. Since the NYC Taxi dataset is fairly large, we will filter files first into a PyArrow dataset. And then in the next cell after, we will filter the data on read into a PyArrow table and convert that to a pandas dataframe.\n",
    "\n",
    "```{tip}\n",
    "Use PyArrow dataset and table for reading or writing large parquet files, since its native multithreaded C++ adapter is faster than pandas read_parquet, even using engine=pyarrow.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d598045a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NYC Taxi using 2 file(s)!\n",
      "s3_files: ['s3://anonymous@air-example-data/ursa-labs-taxi-data/by_year/2019/05/data.parquet/359c21b3e28f40328e68cf66f7ba40e2_000000.parquet', 's3://anonymous@air-example-data/ursa-labs-taxi-data/by_year/2019/06/data.parquet/ab5b9d2b8cc94be19346e260b543ec35_000000.parquet']\n",
      "Locations: [141, 229, 173]\n"
     ]
    }
   ],
   "source": [
    "# Define some global variables.\n",
    "TARGET = \"trip_duration\"\n",
    "FORECAST_LENGTH = 28\n",
    "MAX_DATE = datetime(2019, 6, 30)\n",
    "s3_partitions = pds.dataset(\n",
    "    \"s3://anonymous@air-example-data/ursa-labs-taxi-data/by_year/\",\n",
    "    partitioning=[\"year\", \"month\"],\n",
    ")\n",
    "s3_files = [f\"s3://anonymous@{file}\" for file in s3_partitions.files]\n",
    "\n",
    "# Obtain all location IDs\n",
    "all_location_ids = (\n",
    "    pq.read_table(s3_files[0], columns=[\"pickup_location_id\"])[\n",
    "        \"pickup_location_id\"\n",
    "    ]\n",
    "    .unique()\n",
    "    .to_pylist()\n",
    ")\n",
    "# drop [264, 265]\n",
    "all_location_ids.remove(264)\n",
    "all_location_ids.remove(265)\n",
    "\n",
    "# Use smoke testing or not.\n",
    "starting_idx = -2 if SMOKE_TEST else 0\n",
    "# TODO: drop location 199 to test error-handling before final git checkin\n",
    "sample_locations = [141, 229, 173] if SMOKE_TEST else all_location_ids\n",
    "\n",
    "# Display what data will be used.\n",
    "s3_files = s3_files[starting_idx:]\n",
    "print(f\"NYC Taxi using {len(s3_files)} file(s)!\")\n",
    "print(f\"s3_files: {s3_files}\")\n",
    "print(f\"Locations: {sample_locations}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4c94111f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to read a pyarrow.Table object using pyarrow parquet\n",
    "def read_data(file: str, sample_id: np.int32) -> pd.DataFrame:\n",
    "\n",
    "    # parse out min expected date\n",
    "    part_zero = \"s3://anonymous@air-example-data/ursa-labs-taxi-data/by_year/\"\n",
    "    split_text = file.split(part_zero)[1]\n",
    "    min_year = split_text.split(\"/\")[0]\n",
    "    min_month = split_text.split(\"/\")[1]\n",
    "    string_date = min_year + \"-\" + min_month + \"-\" + \"01\" + \" 00:00:00\"\n",
    "    min_date = datetime.strptime(string_date, \"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "    df = pq.read_table(\n",
    "        file,\n",
    "        filters=[\n",
    "            (\"pickup_at\", \">\", min_date),\n",
    "            (\"pickup_at\", \"<=\", MAX_DATE),\n",
    "            (\"passenger_count\", \">\", 0),\n",
    "            (\"trip_distance\", \">\", 0),\n",
    "            (\"fare_amount\", \">\", 0),\n",
    "            (\"pickup_location_id\", \"not in\", [264, 265]),\n",
    "            (\"dropoff_location_id\", \"not in\", [264, 265]),\n",
    "            (\"pickup_location_id\", \"=\", sample_id),\n",
    "        ],\n",
    "        columns=[\n",
    "            \"pickup_at\",\n",
    "            \"dropoff_at\",\n",
    "            \"pickup_location_id\",\n",
    "            \"dropoff_location_id\",\n",
    "            \"passenger_count\",\n",
    "            \"trip_distance\",\n",
    "            \"fare_amount\",\n",
    "        ],\n",
    "    ).to_pandas()\n",
    "    return df\n",
    "\n",
    "\n",
    "# Function to transform a pandas dataframe\n",
    "def transform_df(input_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = input_df.copy()\n",
    "    \n",
    "    # PyTorch Forecasting expects integer counter 'time_idx' per timeseries\n",
    "    # PyTorch Forecasting expects unique_id to be type string\n",
    "    # PyTorch Forecasting expects target value to be type float\n",
    "\n",
    "    # calculate trip_duration\n",
    "    df[\"trip_duration\"] = (df[\"dropoff_at\"] - df[\"pickup_at\"]).dt.seconds\n",
    "    # filter trip_durations > 1 minute and less than 24 hours\n",
    "    df = df[df[\"trip_duration\"] > 60]\n",
    "    df = df[df[\"trip_duration\"] < 24 * 60 * 60]\n",
    "\n",
    "    # truncate time hourly\n",
    "    df[\"pickup_at\"] = df[\"pickup_at\"].dt.to_period(\"H\").dt.to_timestamp()\n",
    "    # add day_hour as a new feature\n",
    "    df[\"day_hour\"] = (\n",
    "        df[\"pickup_at\"].dt.day.astype(str)\n",
    "        + \"_\"\n",
    "        + df[\"pickup_at\"].dt.hour.astype(str)\n",
    "    )\n",
    "    # create groupby key\n",
    "    df[\"pickup_location_id\"] = df[\"pickup_location_id\"].astype(str)\n",
    "    df[\"loc_year_month_day_hour\"] = (\n",
    "        df[\"pickup_location_id\"]\n",
    "        + \"_\"\n",
    "        + df[\"pickup_at\"].astype(str)\n",
    "    )\n",
    "    # add target_value quantity for groupby count later\n",
    "    df[\"trip_quantity\"] = 1.0\n",
    "    # drop unnecessary columns\n",
    "    df.drop(\n",
    "        [\n",
    "            \"dropoff_at\",\n",
    "            \"pickup_at\",\n",
    "            \"dropoff_location_id\",\n",
    "            \"fare_amount\",\n",
    "            \"passenger_count\",\n",
    "            \"trip_distance\",\n",
    "            \"trip_duration\",\n",
    "        ],\n",
    "        axis=1,\n",
    "        inplace=True,\n",
    "    )\n",
    "\n",
    "    # groupby aggregregate\n",
    "    g = df.groupby(\"loc_year_month_day_hour\").agg(\n",
    "        {\"pickup_location_id\": min, \"day_hour\": min, \"trip_quantity\": sum}\n",
    "    ).reset_index()\n",
    "    # add 'time_idx'\n",
    "    g.reset_index(inplace=True, drop=False)\n",
    "    g.rename(columns={\"index\": \"time_idx\"}, inplace=True)\n",
    "    g['time_idx'] = g['time_idx'].astype('int32')\n",
    "    # having num rows in group > 2\n",
    "    g.dropna(inplace=True)\n",
    "    g = g[g[\"trip_quantity\"] > 2].copy()\n",
    "\n",
    "    # Drop groupby variable since we do not need it anymore\n",
    "    g.drop([\"loc_year_month_day_hour\"], axis=1, inplace=True)\n",
    "\n",
    "    return g\n",
    "\n",
    "\n",
    "def prepare_data(sample_location_id: np.int32) -> pd.DataFrame:\n",
    "\n",
    "    # Load data.\n",
    "    df_list = [read_data(f, sample_location_id) for f in s3_files]\n",
    "    df_raw = pd.concat(df_list, ignore_index=True)\n",
    "    # Abort Tune to avoid Tune Error if df has too few rows\n",
    "    if df_raw.shape[0] < FORECAST_LENGTH:\n",
    "        print_time(\n",
    "            f\"Location {sample_location_id} has only {df_raw.shape[0]} rows\"\n",
    "        )\n",
    "        session.report(dict(error=None))\n",
    "        return None\n",
    "\n",
    "    # Transform data.\n",
    "    df = transform_df(df_raw)\n",
    "    # Abort Tune to avoid Tune Error if df has too few rows\n",
    "    if df.shape[0] < FORECAST_LENGTH:\n",
    "        print_time(f\"Location {sample_location_id} has only {df.shape[0]} rows\")\n",
    "        session.report(dict(error=None))\n",
    "        return None\n",
    "    else:\n",
    "        return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ede46ea",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Define your Ray Tune Search Space and Search Algorithm <a class=\"anchor\" id=\"define_search_space\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b7098e9",
   "metadata": {},
   "source": [
    "In this notebook, we will use Ray Tune to run parallel training jobs per pickup location. The training jobs will be defined using a search space and simple grid search. Depending on your need, fancier search spaces and search algorithms are possible with Tune.\n",
    "\n",
    "**First, define a search space of experiment trials to run.**\n",
    "> The typical use case for Tune search spaces are for hyperparameter tuning. In our case, we are defining a Tune search space in a way to allow for training jobs to be conducted automatically. Each training job will run on a different data partition (taxi pickup location) and use a different algorithm.\n",
    "\n",
    "**Next, define a search algorithm.**  \n",
    "\n",
    "```{tip}\n",
    "Common search algorithms include grid search, random search, and Bayesian optimization. For more details, see [Working with Tune Search Spaces](https://docs.ray.io/en/master/tune/tutorials/tune-search-spaces.html#tune-search-space-tutorial). Deciding the best combination of search space and search algorithm is part of the art of being a Data Scientist and depends on the data, algorithm, and problem being solved.\n",
    "```\n",
    "\n",
    "Ray Tune will use the search space and search algorithm to generate multiple configurations, each of which will be evaluated in a separate Trial on a Ray Cluster. Ray Tune will take care of orchestrating those Trials automatically. Specifically, Ray Tune will pass a config dictionary to each partition and make a Trainable function call.\n",
    "\n",
    "**Below, we define our search space consists of:**\n",
    "- Different algorithms, either:\n",
    "  - Prophet with [multiplicative or additive](https://facebook.github.io/prophet/docs/multiplicative_seasonality.html) seasonal effects \n",
    "  - AutoARIMA.\n",
    "- Some or all NYC taxi pick-up locations.\n",
    "\n",
    "For Tune search algorithm, we want to run *grid search*, meaning we want to run an experiment for every possible combination in the search space. What this means is every algorithm will be applied to every NYC Taxi pick-up location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a11dd257",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Define a search space.\n",
    "search_space = {\n",
    "    \"batch_size\": tune.grid_search([128])  #[128, 64])\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbadbd96",
   "metadata": {},
   "source": [
    "## Define a Trainable (callable) function <a class=\"anchor\" id=\"define_trainable\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a658aab",
   "metadata": {},
   "source": [
    "ðŸ“ˆ Typically when you are running Data Science experiments, you want to be able to keep track of summary metrics for each trial, so you can decide at the end which trials were best. That way, you can decide which model to deploy.\n",
    "\n",
    "ðŸ‡« Next, we define a trainable function in order to train and evaluate a Prophet model on a data partition. This function will be called in parallel by every Tune trial. Inside this trainable function, we will:\n",
    "- Add detailed metrics we want to report (each model's loss or error). \n",
    "- Checkpoint each model for easy deployment later.\n",
    "\n",
    "ðŸ“– **The metrics defined inside the trainable function will appear in the Ray Tune experiment summary table.**\n",
    "```{tip}\n",
    "Ray Tune has two ways of defining a trainable, namely the [Function API](https://docs.ray.io/en/latest/tune/api_docs/trainable.html#trainable-docs) and the Class API. Both are valid ways of defining a trainable, but *the Function API is generally recommended*.\n",
    "```\n",
    "\n",
    "**In the cell below, we define a \"Trainable\" function called `train_model()`**.\n",
    "- The input is a config dictionary argument. \n",
    "- The output can be a simple dictionary of metrics which will be reported back to Tune.\n",
    "- We will [checkpoint](https://docs.ray.io/en/master/ray-air/key-concepts.html#checkpoints) save each model in addition to reporting each trial's metrics.\n",
    "- Since we are using **grid search**, this means `train_model()` will be run *in parallel for every permutation* in the Tune search space!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6a655409",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define functions\n",
    "# Todo: Move functions inside util.py\n",
    "\n",
    "# Convert pandas data to PyTorch tensors.\n",
    "def convert_pandas_pytorch_timeseriesdata(\n",
    "    input_data_pandas_df:pd.DataFrame, \n",
    "    config:dict\n",
    ") -> typing.Union['pytorch_forecasting.data.timeseries.TimeSeriesDataSet',\n",
    "                  'torch.utils.data.dataloader.DataLoader']:\n",
    "\n",
    "    \"\"\"Converts pandas dataframe into TimeSeries folded tensors following \n",
    "       the backtesting technique.  A generator for doing the folding is \n",
    "       per batch also created.  One for the training data.  \n",
    "       Another for the validation data.  \n",
    "\n",
    "    Inputs:\n",
    "        pd.DataFrame: All the input data\n",
    "        dict: config is a configuration file containing hard-coded settings.\n",
    "\n",
    "    Returns:\n",
    "        'pytorch_forecasting.data.timeseries.TimeSeriesDataSet': training data\n",
    "        'torch.utils.data.dataloader.DataLoader': training data loader\n",
    "        'torch.utils.data.dataloader.DataLoader': validation data loader\n",
    "    \"\"\"\n",
    "    \n",
    "    # specify data parameters\n",
    "    FORECAST_HORIZON = config.get(\"forecast_horizon\", 168)\n",
    "    CONTEXT_LENGTH = config.get(\"context_length\", 63)\n",
    "    BATCH_SIZE = config.get(\"batch_size\", 32)\n",
    "    NUM_TRAINING_WORKERS = config.get(\"num_training_workers\", 4)\n",
    "    id_col_name = \"pickup_location_id\"\n",
    "    target_value = \"trip_quantity\"\n",
    "    \n",
    "    the_df = input_data_pandas_df.copy()\n",
    "    \n",
    "    # define forecast horizon and training cutoff\n",
    "    max_prediction_length = FORECAST_HORIZON  #decoder length = 1 week forecast horizon\n",
    "    max_encoder_length = CONTEXT_LENGTH  # window or context length\n",
    "    training_cutoff = the_df[\"time_idx\"].max() - max_prediction_length \n",
    "\n",
    "    # convert pandas to PyTorch tensor\n",
    "    training_data = ptf.data.TimeSeriesDataSet(\n",
    "        the_df[lambda x: x.time_idx <= training_cutoff],\n",
    "        allow_missing_timesteps=True,\n",
    "        time_idx=\"time_idx\",\n",
    "        target=target_value,\n",
    "        group_ids=[id_col_name],\n",
    "        min_encoder_length=5,  # min 5 historical values must exist\n",
    "        max_encoder_length=max_encoder_length,\n",
    "        min_prediction_length=1,\n",
    "        max_prediction_length=max_prediction_length,\n",
    "        static_categoricals=[id_col_name],\n",
    "        # static_reals=[\"avg_population_2017\", \"avg_yearly_household_income_2017\"],\n",
    "        static_reals=[],\n",
    "        time_varying_known_categoricals=[\"day_hour\"],\n",
    "        # group of categorical variables can be treated as one variable\n",
    "        # variable_groups={\"special_days\": special_days},  \n",
    "        time_varying_known_reals=[\"time_idx\", ],\n",
    "                            # \"mean_item_loc_weekday\",\n",
    "                            # \"binned_max_item\"],\n",
    "        time_varying_unknown_categoricals=[],\n",
    "        time_varying_unknown_reals=[target_value,],\n",
    "\n",
    "        # https://pytorch-forecasting.readthedocs.io/en/v0.2.4/_modules/pytorch_forecasting/data.html\n",
    "        target_normalizer=ptf.data.GroupNormalizer(\n",
    "            groups=[\"pickup_location_id\"], \n",
    "            transformation=\"softplus\"  #forces positive values\n",
    "        ), \n",
    "        add_relative_time_idx=True, # add as feature\n",
    "        add_target_scales=True, # add avg target_value as feature\n",
    "        add_encoder_length=True, # add as feature\n",
    "    )\n",
    "    \n",
    "    # create PyTorch dataloader for training\n",
    "    train_loader = training_data\\\n",
    "                        .to_dataloader(\n",
    "                            train=True, \n",
    "                            batch_size=BATCH_SIZE, \n",
    "                            num_workers=NUM_TRAINING_WORKERS)\n",
    "    \n",
    "    # create validation PyTorch data \n",
    "    # (predict=True) means make do inference using the validation data\n",
    "    val_dataset = ptf.data.TimeSeriesDataSet\\\n",
    "                    .from_dataset(\n",
    "                        training_data, \n",
    "                        data=the_df, \n",
    "                        predict=True, \n",
    "                        stop_randomization=True)\n",
    "\n",
    "    # create PyTorch dataloaders for inference on validation data\n",
    "    validation_loader = val_dataset\\\n",
    "                    .to_dataloader(\n",
    "                        train=False, \n",
    "                        batch_size=BATCH_SIZE * 10, \n",
    "                        num_workers=NUM_TRAINING_WORKERS)\n",
    "    \n",
    "    # return original df converted to PyTorch tensors, and pytorch loaders\n",
    "    return training_data, train_loader, validation_loader\n",
    "\n",
    "def evaluate_model(y_actual:'torch.Tensor', \n",
    "             y_quantiles:'torch.Tensor', \n",
    "             quantile_list:list)->'torch.Tensor':\n",
    "    \"\"\"Calculate weighted quantile loss given actuals, quantile predictions,\n",
    "       and list of desired quantiles to average over.\n",
    "    Inputs:\n",
    "        'torch.Tensor': y_actual is a tensor of actual values \n",
    "        'torch.Tensor': y_quantiles is a tensor of quantile predictions\n",
    "        'list': List of quantiles to average over\n",
    "\n",
    "    Returns:\n",
    "        'torch.Tensor': weighted quantile loss over all the desired quantiles\n",
    "    \"\"\"\n",
    "\n",
    "    assert not y_actual.requires_grad\n",
    "    assert y_quantiles.size(0) == y_actual.size(0)\n",
    "    \n",
    "    all_losses = []\n",
    "    for i, q in enumerate(quantile_list):\n",
    "        sum_actuals = torch.sum(torch.abs(y_actual[i]))\n",
    "        errors = torch.abs(y_actual[i] - y_quantiles[i][:, i])\n",
    "        all_losses.append(\n",
    "            torch.where(y_quantiles[i][:, i] > y_actual[i],\n",
    "                        (1-q) * errors, \n",
    "                        q * errors ).unsqueeze(1))\n",
    "        \n",
    "        if torch.is_nonzero(sum_actuals):\n",
    "            all_losses[i] = torch.sum(all_losses[i]).div(sum_actuals)\n",
    "        else:\n",
    "            all_losses[i] = torch.empty_like(all_losses[i])\n",
    "    \n",
    "    WQL = torch.mean(torch.stack(all_losses), dim=0)\n",
    "    return WQL\n",
    "    \n",
    "# Define a calling function to read data, define model, train it\n",
    "def train_ptf(config: dict) -> typing.Union['torch.utils.data.dataloader.DataLoader',\n",
    "        'pytorch_lightning.trainer.trainer.Trainer',\n",
    "        'pytorch_forecasting.models.temporal_fusion_transformer.TemporalFusionTransformer',\n",
    "         str]:\n",
    "    \"\"\"Define a calling function to read data, define a model and train it.\n",
    "\n",
    "    Inputs:\n",
    "        dict: configuration dictionary with hard-coded runtime values\n",
    "\n",
    "    Returns:\n",
    "        'torch.utils.data.dataloader.DataLoader': validation data loader\n",
    "        'pytorch_lightning.trainer.trainer.Trainer': trainer for fitting the model\n",
    "        'pytorch_forecasting.models.temporal_fusion_transformer.TemporalFusionTransformer': trained model\n",
    "        str: path where Pytorch Forecasting model is stored\n",
    "    \"\"\"   \n",
    "    # read data into pandas dataframe\n",
    "    df = prepare_data(sample_location_id)\n",
    "\n",
    "    # convert data from pandas to PyTorch tensors\n",
    "    train_dataset, train_loader, validation_loader = \\\n",
    "        convert_pandas_pytorch_timeseriesdata(df, config)\n",
    "    \n",
    "    # Create your PTF model.\n",
    "    model = ptf.models.TemporalFusionTransformer.from_dataset(\n",
    "        train_dataset,\n",
    "        learning_rate=config.get(\"lr\", 0.01),\n",
    "        hidden_size=config.get(\"hidden_size\", 40), # num neurons in each layer, bigger runs more slowly\n",
    "        # lstm_layers=HIDDEN_LAYERS, #LSTM layers=1 #default=1 for tft architecture\n",
    "        attention_head_size=config.get(\"attention_head_size\", 4),  #default 4 cells in LSTM layer\n",
    "        dropout=config.get(\"droupout\", 0.1),\n",
    "        hidden_continuous_size=config.get(\"hidden_continuous_size\", 1),  #similar to categorical embedding size\n",
    "        # 7 quantiles by default: [0.02, 0.1, 0.25, 0.5, 0.75, 0.9, 0.98]\n",
    "        # output_size=7,  \n",
    "        # optimizer loss metric\n",
    "        loss=ptf.metrics.QuantileLoss(),\n",
    "        # uncomment for learning rate finder and otherwise, e.g. to 10 for logging every 10 batches\n",
    "        log_interval=50,  #50\n",
    "        reduce_on_plateau_patience=4, # reduce learning automatically\n",
    "    )\n",
    "    print(f\"Number of parameters in network: {model.size()/1e3:.1f}k\")\n",
    "    \n",
    "    # Create the Tune Reporting Callback\n",
    "    metrics = {\"loss\": \"val_loss\"}\n",
    "    callbacks = [TuneReportCallback(metrics, on=\"validation_end\")]\n",
    "    \n",
    "    # configure PyTorch trainer with Ray Lightning plugin\n",
    "    trainer = pl.Trainer(\n",
    "        max_epochs=config.get(\"epochs\", 30),\n",
    "        gpus=config.get(\"num_gpus\", 0),\n",
    "        gradient_clip_val=0.1, \n",
    "        limit_train_batches=config.get(\"limit_train_batches\", 30),  \n",
    "        callbacks=callbacks,\n",
    "        # Scaling strategy\n",
    "        strategy=strategy, \n",
    "        # Run \"fast\" mode for quick sanity check\n",
    "        # Note: No trainer checkpoints will be saved in fast mode\n",
    "        fast_dev_run=config.get(\"fast_mode\", False),\n",
    "    )\n",
    "    trainer.fit(\n",
    "        model,\n",
    "        train_dataloaders=train_loader,\n",
    "        val_dataloaders=validation_loader,\n",
    "    )\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2cb14cc",
   "metadata": {},
   "source": [
    "## Create and train a baseline model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3223b4a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1440, 4)\n",
      "      time_idx pickup_location_id day_hour  trip_quantity\n",
      "606        606                141     26_6           38.0\n",
      "1284      1284                141    23_12          250.0\n",
      "1072      1072                141    14_16          224.0\n",
      "1395      1395                141     28_3           41.0\n",
      "55          55                141      3_7          387.0\n",
      "time_idx                int32\n",
      "pickup_location_id     object\n",
      "day_hour               object\n",
      "trip_quantity         float64\n",
      "dtype: object\n",
      "Input data type: <class 'pandas.core.frame.DataFrame'>\n",
      "Converted data type: <class 'pytorch_forecasting.data.timeseries.TimeSeriesDataSet'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ray/anaconda3/lib/python3.9/site-packages/pytorch_lightning/utilities/parsing.py:261: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "/home/ray/anaconda3/lib/python3.9/site-packages/pytorch_lightning/utilities/parsing.py:261: UserWarning: Attribute 'logging_metrics' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['logging_metrics'])`.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "baseline model MAE: 102.18452453613281\n",
      "CPU times: user 4.38 s, sys: 1.22 s, total: 5.6 s\n",
      "Wall time: 4.36 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# specify config parameters for baseline model\n",
    "num_training_workers = min(num_cpu - 2, 32)\n",
    "FORECAST_CONFIG = {\"forecast_horizon\": 168, \"context_length\": 63,\n",
    "          \"num_gpus\":0, \"batch_size\": 128, \n",
    "          \"num_training_workers\": num_training_workers,\n",
    "         }\n",
    "\n",
    "# Test reading data pandas.\n",
    "sample_location_id = sample_locations[0]\n",
    "df = prepare_data(sample_location_id)\n",
    "print(df.shape)\n",
    "print(df.sample(5))\n",
    "print(df.dtypes)\n",
    "\n",
    "# convert data from pandas to PyTorch tensors\n",
    "print(f\"Input data type: {type(df)}\")\n",
    "train_dataset, train_loader, validation_loader = convert_pandas_pytorch_timeseriesdata(\n",
    "    df, FORECAST_CONFIG\n",
    ")\n",
    "print(f\"Converted data type: {type(train_dataset)}\")\n",
    "\n",
    "# calculate baseline mean absolute error, i.e. predict next value as the last available value from the history\n",
    "actuals = torch.cat([y for x, (y, weight) in iter(validation_loader)])\n",
    "baseline_predictions = ptf.models.Baseline().predict(validation_loader)\n",
    "\n",
    "## EVALUATE THE BASELINE MODEL\n",
    "# print MAE\n",
    "print(f\"baseline model MAE: {(actuals - baseline_predictions).abs().mean()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7127d5b",
   "metadata": {},
   "source": [
    "## Run batch training on Ray Tune <a class=\"anchor\" id=\"run_tune_search\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa1f18a2",
   "metadata": {},
   "source": [
    "\n",
    "**Now we are ready to kick off a Ray Tune experiment!**  \n",
    "\n",
    "Recall what we are doing, high level, is training several different models per pickup location. We are using Ray Tune so we can run all these trials in parallel on a Ray cluster. At the end, we will inspect the results of the experiment and deploy only the best model per pickup location.\n",
    "\n",
    "**In the cell below, we use AIR configs and run the experiment using `tuner.fit()`.** \n",
    "\n",
    "Tune will report on experiment status, and after the experiment finishes, you can inspect the results. \n",
    "\n",
    "- In the cell below, we use the default resources config which is 1 CPU core for each task. For more information about configuring resource allocations, see [A Guide To Parallelism and Resources](https://docs.ray.io/en/master/tune/tutorials/tune-resources.html#tune-parallelism). \n",
    "\n",
    "- In the AIR config below, we have specified a local directory `my_Tune_logs` for logging instead of the default `~/ray_results` directory. Giving your logs a project name makes them easier to find. Also giving a relative path, means you can see your logs inside the Jupyter browser. Learn more about logging Tune results at [How to configure logging in Tune](https://docs.ray.io/en/master/tune/tutorials/tune-output.html#tune-logging).\n",
    "\n",
    "- Tune can [retry failed experiments automatically](https://docs.ray.io/en/master/tune/tutorials/tune-stopping.html#tune-stopping-guide), as well as entire experiments. This is necessary in case a node on your remote cluster fails (when running on a cloud such as AWS or GCP).\n",
    "\n",
    "ðŸ’¡ Right-click on the cell below and choose \"Enable Scrolling for Outputs\"! This will make it easier to view, since model training output can be very long!\n",
    "\n",
    "**Setting SMOKE_TEST=False, running on Anyscale: 771 models, using 18 NYC Taxi S3 files dating from 2018/01 to 2019/06 (split into partitions approx 1GiB each), were simultaneously trained on a 7-node AWS cluster of [m5.4xlarges](https://aws.amazon.com/ec2/instance-types/m5/), within 40 minutes.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "02e1ffe9",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"tuneStatus\">\n",
       "  <div style=\"display: flex;flex-direction: row\">\n",
       "    <div style=\"display: flex;flex-direction: column;\">\n",
       "      <h3>Tune Status</h3>\n",
       "      <table>\n",
       "<tbody>\n",
       "<tr><td>Current time:</td><td>2022-12-15 17:14:03</td></tr>\n",
       "<tr><td>Running for: </td><td>00:00:46.70        </td></tr>\n",
       "<tr><td>Memory:      </td><td>6.3/30.6 GiB       </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "    </div>\n",
       "    <div class=\"vDivider\"></div>\n",
       "    <div class=\"systemInfo\">\n",
       "      <h3>System Info</h3>\n",
       "      Using FIFO scheduling algorithm.<br>Resources requested: 0/8 CPUs, 0/0 GPUs, 0.0/17.04 GiB heap, 0.0/8.52 GiB objects\n",
       "    </div>\n",
       "    \n",
       "  </div>\n",
       "  <div class=\"hDivider\"></div>\n",
       "  <div class=\"trialStatus\">\n",
       "    <h3>Trial Status</h3>\n",
       "    <table>\n",
       "<thead>\n",
       "<tr><th>Trial name           </th><th>status    </th><th>loc                </th><th style=\"text-align: right;\">  batch_size</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    loss</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>train_ptf_d1da0_00000</td><td>TERMINATED</td><td>172.31.230.43:20074</td><td style=\"text-align: right;\">         128</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         20.2907</td><td style=\"text-align: right;\"> 91.7215</td></tr>\n",
       "<tr><td>train_ptf_d1da0_00001</td><td>TERMINATED</td><td>172.31.230.43:20074</td><td style=\"text-align: right;\">          64</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         18.828 </td><td style=\"text-align: right;\">117.536 </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "  </div>\n",
       "</div>\n",
       "<style>\n",
       ".tuneStatus {\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".tuneStatus .systemInfo {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       ".tuneStatus .trialStatus {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".tuneStatus .hDivider {\n",
       "  border-bottom-width: var(--jp-border-width);\n",
       "  border-bottom-color: var(--jp-border-color0);\n",
       "  border-bottom-style: solid;\n",
       "}\n",
       ".tuneStatus .vDivider {\n",
       "  border-left-width: var(--jp-border-width);\n",
       "  border-left-color: var(--jp-border-color0);\n",
       "  border-left-style: solid;\n",
       "  margin: 0.5em 1em 0.5em 1em;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(train_ptf pid=20074)\u001b[0m GPU available: False, used: False\n",
      "\u001b[2m\u001b[36m(train_ptf pid=20074)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[2m\u001b[36m(train_ptf pid=20074)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[2m\u001b[36m(train_ptf pid=20074)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[2m\u001b[36m(train_ptf pid=20074)\u001b[0m Running in fast_dev_run mode: will run a full train, val, test and prediction loop using 1 batch(es).\n",
      "\u001b[2m\u001b[36m(train_ptf pid=20074)\u001b[0m `Trainer(limit_train_batches=1)` was configured so 1 batch per epoch will be used.\n",
      "\u001b[2m\u001b[36m(train_ptf pid=20074)\u001b[0m `Trainer(limit_val_batches=1)` was configured so 1 batch will be used.\n",
      "\u001b[2m\u001b[36m(train_ptf pid=20074)\u001b[0m `Trainer(limit_test_batches=1)` was configured so 1 batch will be used.\n",
      "\u001b[2m\u001b[36m(train_ptf pid=20074)\u001b[0m `Trainer(limit_predict_batches=1)` was configured so 1 batch will be used.\n",
      "\u001b[2m\u001b[36m(train_ptf pid=20074)\u001b[0m `Trainer(val_check_interval=1.0)` was configured so validation will run at the end of the training epoch..\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(train_ptf pid=20074)\u001b[0m Number of parameters in network: 39.6k\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(RayExecutor pid=20224)\u001b[0m /home/ray/anaconda3/lib/python3.9/site-packages/pytorch_lightning/utilities/warnings.py:53: LightningDeprecationWarning: pytorch_lightning.utilities.warnings.rank_zero_deprecation has been deprecated in v1.6 and will be removed in v1.8. Use the equivalent function from the pytorch_lightning.utilities.rank_zero module instead.\n",
      "\u001b[2m\u001b[36m(RayExecutor pid=20224)\u001b[0m   new_rank_zero_deprecation(\n",
      "\u001b[2m\u001b[36m(RayExecutor pid=20224)\u001b[0m /home/ray/anaconda3/lib/python3.9/site-packages/pytorch_lightning/utilities/warnings.py:58: LightningDeprecationWarning: ParallelStrategy.torch_distributed_backend was deprecated in v1.6 and will be removed in v1.8.\n",
      "\u001b[2m\u001b[36m(RayExecutor pid=20224)\u001b[0m   return new_rank_zero_deprecation(*args, **kwargs)\n",
      "\u001b[2m\u001b[36m(RayExecutor pid=20224)\u001b[0m Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/6\n",
      "\u001b[2m\u001b[36m(RayExecutor pid=20225)\u001b[0m Initializing distributed: GLOBAL_RANK: 1, MEMBER: 2/6\n",
      "\u001b[2m\u001b[36m(RayExecutor pid=20229)\u001b[0m Initializing distributed: GLOBAL_RANK: 3, MEMBER: 4/6\n",
      "\u001b[2m\u001b[36m(RayExecutor pid=20227)\u001b[0m Initializing distributed: GLOBAL_RANK: 2, MEMBER: 3/6\n",
      "\u001b[2m\u001b[36m(RayExecutor pid=20231)\u001b[0m Initializing distributed: GLOBAL_RANK: 5, MEMBER: 6/6\n",
      "\u001b[2m\u001b[36m(RayExecutor pid=20230)\u001b[0m Initializing distributed: GLOBAL_RANK: 4, MEMBER: 5/6\n",
      "\u001b[2m\u001b[36m(RayExecutor pid=20224)\u001b[0m ----------------------------------------------------------------------------------------------------\n",
      "\u001b[2m\u001b[36m(RayExecutor pid=20224)\u001b[0m distributed_backend=gloo\n",
      "\u001b[2m\u001b[36m(RayExecutor pid=20224)\u001b[0m All distributed processes registered. Starting with 6 processes\n",
      "\u001b[2m\u001b[36m(RayExecutor pid=20224)\u001b[0m ----------------------------------------------------------------------------------------------------\n",
      "\u001b[2m\u001b[36m(RayExecutor pid=20224)\u001b[0m \n",
      "\u001b[2m\u001b[36m(RayExecutor pid=20224)\u001b[0m /home/ray/anaconda3/lib/python3.9/site-packages/pytorch_lightning/trainer/configuration_validator.py:335: LightningDeprecationWarning: The `on_keyboard_interrupt` callback hook was deprecated in v1.5 and will be removed in v1.7. Please use the `on_exception` callback hook instead.\n",
      "\u001b[2m\u001b[36m(RayExecutor pid=20224)\u001b[0m   rank_zero_deprecation(\n",
      "\u001b[2m\u001b[36m(RayExecutor pid=20224)\u001b[0m /home/ray/anaconda3/lib/python3.9/site-packages/pytorch_lightning/trainer/configuration_validator.py:347: LightningDeprecationWarning: The `on_init_start` callback hook was deprecated in v1.6 and will be removed in v1.8.\n",
      "\u001b[2m\u001b[36m(RayExecutor pid=20224)\u001b[0m   rank_zero_deprecation(\n",
      "\u001b[2m\u001b[36m(RayExecutor pid=20224)\u001b[0m /home/ray/anaconda3/lib/python3.9/site-packages/pytorch_lightning/trainer/configuration_validator.py:351: LightningDeprecationWarning: The `on_init_end` callback hook was deprecated in v1.6 and will be removed in v1.8.\n",
      "\u001b[2m\u001b[36m(RayExecutor pid=20224)\u001b[0m   rank_zero_deprecation(\"The `on_init_end` callback hook was deprecated in v1.6 and will be removed in v1.8.\")\n",
      "\u001b[2m\u001b[36m(RayExecutor pid=20224)\u001b[0m /home/ray/anaconda3/lib/python3.9/site-packages/pytorch_lightning/trainer/configuration_validator.py:354: LightningDeprecationWarning: The `on_configure_sharded_model` callback hook was deprecated in v1.6 and will be removed in v1.8. Use `setup()` instead.\n",
      "\u001b[2m\u001b[36m(RayExecutor pid=20224)\u001b[0m   rank_zero_deprecation(\n",
      "\u001b[2m\u001b[36m(RayExecutor pid=20224)\u001b[0m /home/ray/anaconda3/lib/python3.9/site-packages/pytorch_lightning/trainer/configuration_validator.py:359: LightningDeprecationWarning: The `on_before_accelerator_backend_setup` callback hook was deprecated in v1.6 and will be removed in v1.8. Use `setup()` instead.\n",
      "\u001b[2m\u001b[36m(RayExecutor pid=20224)\u001b[0m   rank_zero_deprecation(\n",
      "\u001b[2m\u001b[36m(RayExecutor pid=20224)\u001b[0m /home/ray/anaconda3/lib/python3.9/site-packages/pytorch_lightning/trainer/configuration_validator.py:364: LightningDeprecationWarning: `TuneReportCallback.on_load_checkpoint` will change its signature and behavior in v1.8. If you wish to load the state of the callback, use `load_state_dict` instead. In v1.8 `on_load_checkpoint(..., checkpoint)` will receive the entire loaded checkpoint dictionary instead of callback state.\n",
      "\u001b[2m\u001b[36m(RayExecutor pid=20224)\u001b[0m   rank_zero_deprecation(\n",
      "\u001b[2m\u001b[36m(RayExecutor pid=20224)\u001b[0m /home/ray/anaconda3/lib/python3.9/site-packages/pytorch_lightning/trainer/configuration_validator.py:376: LightningDeprecationWarning: The `Callback.on_batch_start` hook was deprecated in v1.6 and will be removed in v1.8. Please use `Callback.on_train_batch_start` instead.\n",
      "\u001b[2m\u001b[36m(RayExecutor pid=20224)\u001b[0m   rank_zero_deprecation(\n",
      "\u001b[2m\u001b[36m(RayExecutor pid=20224)\u001b[0m /home/ray/anaconda3/lib/python3.9/site-packages/pytorch_lightning/trainer/configuration_validator.py:376: LightningDeprecationWarning: The `Callback.on_batch_end` hook was deprecated in v1.6 and will be removed in v1.8. Please use `Callback.on_train_batch_end` instead.\n",
      "\u001b[2m\u001b[36m(RayExecutor pid=20224)\u001b[0m   rank_zero_deprecation(\n",
      "\u001b[2m\u001b[36m(RayExecutor pid=20224)\u001b[0m /home/ray/anaconda3/lib/python3.9/site-packages/pytorch_lightning/trainer/configuration_validator.py:385: LightningDeprecationWarning: The `Callback.on_epoch_start` hook was deprecated in v1.6 and will be removed in v1.8. Please use `Callback.on_<train/validation/test>_epoch_start` instead.\n",
      "\u001b[2m\u001b[36m(RayExecutor pid=20224)\u001b[0m   rank_zero_deprecation(\n",
      "\u001b[2m\u001b[36m(RayExecutor pid=20224)\u001b[0m /home/ray/anaconda3/lib/python3.9/site-packages/pytorch_lightning/trainer/configuration_validator.py:385: LightningDeprecationWarning: The `Callback.on_epoch_end` hook was deprecated in v1.6 and will be removed in v1.8. Please use `Callback.on_<train/validation/test>_epoch_end` instead.\n",
      "\u001b[2m\u001b[36m(RayExecutor pid=20224)\u001b[0m   rank_zero_deprecation(\n",
      "\u001b[2m\u001b[36m(RayExecutor pid=20224)\u001b[0m /home/ray/anaconda3/lib/python3.9/site-packages/pytorch_lightning/trainer/configuration_validator.py:391: LightningDeprecationWarning: The `Callback.on_pretrain_routine_start` hook has been deprecated in v1.6 and will be removed in v1.8. Please use `Callback.on_fit_start` instead.\n",
      "\u001b[2m\u001b[36m(RayExecutor pid=20224)\u001b[0m   rank_zero_deprecation(\n",
      "\u001b[2m\u001b[36m(RayExecutor pid=20224)\u001b[0m /home/ray/anaconda3/lib/python3.9/site-packages/pytorch_lightning/trainer/configuration_validator.py:391: LightningDeprecationWarning: The `Callback.on_pretrain_routine_end` hook has been deprecated in v1.6 and will be removed in v1.8. Please use `Callback.on_fit_start` instead.\n",
      "\u001b[2m\u001b[36m(RayExecutor pid=20224)\u001b[0m   rank_zero_deprecation(\n",
      "\u001b[2m\u001b[36m(RayExecutor pid=20224)\u001b[0m \n",
      "\u001b[2m\u001b[36m(RayExecutor pid=20224)\u001b[0m    | Name                               | Type                            | Params\n",
      "\u001b[2m\u001b[36m(RayExecutor pid=20224)\u001b[0m ----------------------------------------------------------------------------------------\n",
      "\u001b[2m\u001b[36m(RayExecutor pid=20224)\u001b[0m 0  | loss                               | QuantileLoss                    | 0     \n",
      "\u001b[2m\u001b[36m(RayExecutor pid=20224)\u001b[0m 1  | logging_metrics                    | ModuleList                      | 0     \n",
      "\u001b[2m\u001b[36m(RayExecutor pid=20224)\u001b[0m 2  | input_embeddings                   | MultiEmbedding                  | 14.9 K\n",
      "\u001b[2m\u001b[36m(RayExecutor pid=20224)\u001b[0m 3  | prescalers                         | ModuleDict                      | 48    \n",
      "\u001b[2m\u001b[36m(RayExecutor pid=20224)\u001b[0m 4  | static_variable_selection          | VariableSelectionNetwork        | 1.2 K \n",
      "\u001b[2m\u001b[36m(RayExecutor pid=20224)\u001b[0m 5  | encoder_variable_selection         | VariableSelectionNetwork        | 1.4 K \n",
      "\u001b[2m\u001b[36m(RayExecutor pid=20224)\u001b[0m 6  | decoder_variable_selection         | VariableSelectionNetwork        | 954   \n",
      "\u001b[2m\u001b[36m(RayExecutor pid=20224)\u001b[0m 7  | static_context_variable_selection  | GatedResidualNetwork            | 1.7 K \n",
      "\u001b[2m\u001b[36m(RayExecutor pid=20224)\u001b[0m 8  | static_context_initial_hidden_lstm | GatedResidualNetwork            | 1.7 K \n",
      "\u001b[2m\u001b[36m(RayExecutor pid=20224)\u001b[0m 9  | static_context_initial_cell_lstm   | GatedResidualNetwork            | 1.7 K \n",
      "\u001b[2m\u001b[36m(RayExecutor pid=20224)\u001b[0m 10 | static_context_enrichment          | GatedResidualNetwork            | 1.7 K \n",
      "\u001b[2m\u001b[36m(RayExecutor pid=20224)\u001b[0m 11 | lstm_encoder                       | LSTM                            | 3.4 K \n",
      "\u001b[2m\u001b[36m(RayExecutor pid=20224)\u001b[0m 12 | lstm_decoder                       | LSTM                            | 3.4 K \n",
      "\u001b[2m\u001b[36m(RayExecutor pid=20224)\u001b[0m 13 | post_lstm_gate_encoder             | GatedLinearUnit                 | 840   \n",
      "\u001b[2m\u001b[36m(RayExecutor pid=20224)\u001b[0m 14 | post_lstm_add_norm_encoder         | AddNorm                         | 40    \n",
      "\u001b[2m\u001b[36m(RayExecutor pid=20224)\u001b[0m 15 | static_enrichment                  | GatedResidualNetwork            | 2.1 K \n",
      "\u001b[2m\u001b[36m(RayExecutor pid=20224)\u001b[0m 16 | multihead_attn                     | InterpretableMultiHeadAttention | 879   \n",
      "\u001b[2m\u001b[36m(RayExecutor pid=20224)\u001b[0m 17 | post_attn_gate_norm                | GateAddNorm                     | 880   \n",
      "\u001b[2m\u001b[36m(RayExecutor pid=20224)\u001b[0m 18 | pos_wise_ff                        | GatedResidualNetwork            | 1.7 K \n",
      "\u001b[2m\u001b[36m(RayExecutor pid=20224)\u001b[0m 19 | pre_output_gate_norm               | GateAddNorm                     | 880   \n",
      "\u001b[2m\u001b[36m(RayExecutor pid=20224)\u001b[0m 20 | output_layer                       | Linear                          | 147   \n",
      "\u001b[2m\u001b[36m(RayExecutor pid=20224)\u001b[0m ----------------------------------------------------------------------------------------\n",
      "\u001b[2m\u001b[36m(RayExecutor pid=20224)\u001b[0m 39.6 K    Trainable params\n",
      "\u001b[2m\u001b[36m(RayExecutor pid=20224)\u001b[0m 0         Non-trainable params\n",
      "\u001b[2m\u001b[36m(RayExecutor pid=20224)\u001b[0m 39.6 K    Total params\n",
      "\u001b[2m\u001b[36m(RayExecutor pid=20224)\u001b[0m 0.158     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:   0%|          | 0/2 [00:00<?, ?it/s] \n",
      "Epoch 0:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:03<00:03,  3.51s/it, loss=103, v_num=, train_loss_step=103.0]\n",
      "\u001b[2m\u001b[36m(RayExecutor pid=20224)\u001b[0m \n",
      "Validation: 0it [00:00, ?it/s]\u001b[A[0m \n",
      "\u001b[2m\u001b[36m(RayExecutor pid=20224)\u001b[0m \n",
      "Validation:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "\u001b[2m\u001b[36m(RayExecutor pid=20224)\u001b[0m \n",
      "Validation DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  4.02it/s]\u001b[A\n",
      "Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:04<00:00,  2.16s/it, loss=103, v_num=, train_loss_step=103.0]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div class=\"trialProgress\">\n",
       "  <h3>Trial Progress</h3>\n",
       "  <table>\n",
       "<thead>\n",
       "<tr><th>Trial name           </th><th>date               </th><th>done  </th><th>episodes_total  </th><th>experiment_id                   </th><th>experiment_tag  </th><th>hostname        </th><th style=\"text-align: right;\">  iterations_since_restore</th><th style=\"text-align: right;\">    loss</th><th>node_ip      </th><th style=\"text-align: right;\">  pid</th><th style=\"text-align: right;\">  time_since_restore</th><th style=\"text-align: right;\">  time_this_iter_s</th><th style=\"text-align: right;\">  time_total_s</th><th style=\"text-align: right;\">  timestamp</th><th style=\"text-align: right;\">  timesteps_since_restore</th><th>timesteps_total  </th><th style=\"text-align: right;\">  training_iteration</th><th>trial_id   </th><th style=\"text-align: right;\">  warmup_time</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>train_ptf_d1da0_00000</td><td>2022-12-15_17-13-42</td><td>True  </td><td>                </td><td>e380e2aa36224b6c856f2c5decdeefc6</td><td>0_batch_size=128</td><td>ip-172-31-230-43</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\"> 91.7215</td><td>172.31.230.43</td><td style=\"text-align: right;\">20074</td><td style=\"text-align: right;\">             20.2907</td><td style=\"text-align: right;\">           20.2907</td><td style=\"text-align: right;\">       20.2907</td><td style=\"text-align: right;\"> 1671153222</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">                   1</td><td>d1da0_00000</td><td style=\"text-align: right;\">   0.00292134</td></tr>\n",
       "<tr><td>train_ptf_d1da0_00001</td><td>2022-12-15_17-14-02</td><td>True  </td><td>                </td><td>e380e2aa36224b6c856f2c5decdeefc6</td><td>1_batch_size=64 </td><td>ip-172-31-230-43</td><td style=\"text-align: right;\">                         1</td><td style=\"text-align: right;\">117.536 </td><td>172.31.230.43</td><td style=\"text-align: right;\">20074</td><td style=\"text-align: right;\">             18.828 </td><td style=\"text-align: right;\">           18.828 </td><td style=\"text-align: right;\">       18.828 </td><td style=\"text-align: right;\"> 1671153242</td><td style=\"text-align: right;\">                        0</td><td>                 </td><td style=\"text-align: right;\">                   1</td><td>d1da0_00001</td><td style=\"text-align: right;\">   0.00292134</td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "</div>\n",
       "<style>\n",
       ".trialProgress {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".trialProgress h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".trialProgress td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:05<00:00,  2.57s/it, loss=103, v_num=, train_loss_step=103.0, val_loss=91.70]\n",
      "                                                                      \u001b[A\n",
      "Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:05<00:00,  2.57s/it, loss=103, v_num=, train_loss_step=103.0, val_loss=91.70, train_loss_epoch=102.0]\n",
      "\u001b[2m\u001b[36m(train_ptf pid=20074)\u001b[0m <class 'pytorch_lightning.trainer.trainer.Trainer'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(train_ptf pid=20074)\u001b[0m GPU available: False, used: False\n",
      "\u001b[2m\u001b[36m(train_ptf pid=20074)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[2m\u001b[36m(train_ptf pid=20074)\u001b[0m IPU available: False, using: 0 IPUs\n",
      "\u001b[2m\u001b[36m(train_ptf pid=20074)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[2m\u001b[36m(train_ptf pid=20074)\u001b[0m Running in fast_dev_run mode: will run a full train, val, test and prediction loop using 1 batch(es).\n",
      "\u001b[2m\u001b[36m(train_ptf pid=20074)\u001b[0m `Trainer(limit_train_batches=1)` was configured so 1 batch per epoch will be used.\n",
      "\u001b[2m\u001b[36m(train_ptf pid=20074)\u001b[0m `Trainer(limit_val_batches=1)` was configured so 1 batch will be used.\n",
      "\u001b[2m\u001b[36m(train_ptf pid=20074)\u001b[0m `Trainer(limit_test_batches=1)` was configured so 1 batch will be used.\n",
      "\u001b[2m\u001b[36m(train_ptf pid=20074)\u001b[0m `Trainer(limit_predict_batches=1)` was configured so 1 batch will be used.\n",
      "\u001b[2m\u001b[36m(train_ptf pid=20074)\u001b[0m `Trainer(val_check_interval=1.0)` was configured so validation will run at the end of the training epoch..\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(train_ptf pid=20074)\u001b[0m Number of parameters in network: 39.6k\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(RayExecutor pid=22974)\u001b[0m /home/ray/anaconda3/lib/python3.9/site-packages/pytorch_lightning/utilities/warnings.py:53: LightningDeprecationWarning: pytorch_lightning.utilities.warnings.rank_zero_deprecation has been deprecated in v1.6 and will be removed in v1.8. Use the equivalent function from the pytorch_lightning.utilities.rank_zero module instead.\n",
      "\u001b[2m\u001b[36m(RayExecutor pid=22974)\u001b[0m   new_rank_zero_deprecation(\n",
      "\u001b[2m\u001b[36m(RayExecutor pid=22974)\u001b[0m /home/ray/anaconda3/lib/python3.9/site-packages/pytorch_lightning/utilities/warnings.py:58: LightningDeprecationWarning: ParallelStrategy.torch_distributed_backend was deprecated in v1.6 and will be removed in v1.8.\n",
      "\u001b[2m\u001b[36m(RayExecutor pid=22974)\u001b[0m   return new_rank_zero_deprecation(*args, **kwargs)\n",
      "\u001b[2m\u001b[36m(RayExecutor pid=22974)\u001b[0m Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/6\n",
      "\u001b[2m\u001b[36m(RayExecutor pid=22976)\u001b[0m Initializing distributed: GLOBAL_RANK: 2, MEMBER: 3/6\n",
      "\u001b[2m\u001b[36m(RayExecutor pid=22975)\u001b[0m Initializing distributed: GLOBAL_RANK: 1, MEMBER: 2/6\n",
      "\u001b[2m\u001b[36m(RayExecutor pid=22978)\u001b[0m Initializing distributed: GLOBAL_RANK: 4, MEMBER: 5/6\n",
      "\u001b[2m\u001b[36m(RayExecutor pid=22979)\u001b[0m Initializing distributed: GLOBAL_RANK: 5, MEMBER: 6/6\n",
      "\u001b[2m\u001b[36m(RayExecutor pid=22977)\u001b[0m Initializing distributed: GLOBAL_RANK: 3, MEMBER: 4/6\n",
      "\u001b[2m\u001b[36m(RayExecutor pid=22974)\u001b[0m ----------------------------------------------------------------------------------------------------\n",
      "\u001b[2m\u001b[36m(RayExecutor pid=22974)\u001b[0m distributed_backend=gloo\n",
      "\u001b[2m\u001b[36m(RayExecutor pid=22974)\u001b[0m All distributed processes registered. Starting with 6 processes\n",
      "\u001b[2m\u001b[36m(RayExecutor pid=22974)\u001b[0m ----------------------------------------------------------------------------------------------------\n",
      "\u001b[2m\u001b[36m(RayExecutor pid=22974)\u001b[0m \n",
      "\u001b[2m\u001b[36m(RayExecutor pid=22974)\u001b[0m /home/ray/anaconda3/lib/python3.9/site-packages/pytorch_lightning/trainer/configuration_validator.py:335: LightningDeprecationWarning: The `on_keyboard_interrupt` callback hook was deprecated in v1.5 and will be removed in v1.7. Please use the `on_exception` callback hook instead.\n",
      "\u001b[2m\u001b[36m(RayExecutor pid=22974)\u001b[0m   rank_zero_deprecation(\n",
      "\u001b[2m\u001b[36m(RayExecutor pid=22974)\u001b[0m /home/ray/anaconda3/lib/python3.9/site-packages/pytorch_lightning/trainer/configuration_validator.py:347: LightningDeprecationWarning: The `on_init_start` callback hook was deprecated in v1.6 and will be removed in v1.8.\n",
      "\u001b[2m\u001b[36m(RayExecutor pid=22974)\u001b[0m   rank_zero_deprecation(\n",
      "\u001b[2m\u001b[36m(RayExecutor pid=22974)\u001b[0m /home/ray/anaconda3/lib/python3.9/site-packages/pytorch_lightning/trainer/configuration_validator.py:351: LightningDeprecationWarning: The `on_init_end` callback hook was deprecated in v1.6 and will be removed in v1.8.\n",
      "\u001b[2m\u001b[36m(RayExecutor pid=22974)\u001b[0m   rank_zero_deprecation(\"The `on_init_end` callback hook was deprecated in v1.6 and will be removed in v1.8.\")\n",
      "\u001b[2m\u001b[36m(RayExecutor pid=22974)\u001b[0m /home/ray/anaconda3/lib/python3.9/site-packages/pytorch_lightning/trainer/configuration_validator.py:354: LightningDeprecationWarning: The `on_configure_sharded_model` callback hook was deprecated in v1.6 and will be removed in v1.8. Use `setup()` instead.\n",
      "\u001b[2m\u001b[36m(RayExecutor pid=22974)\u001b[0m   rank_zero_deprecation(\n",
      "\u001b[2m\u001b[36m(RayExecutor pid=22974)\u001b[0m /home/ray/anaconda3/lib/python3.9/site-packages/pytorch_lightning/trainer/configuration_validator.py:359: LightningDeprecationWarning: The `on_before_accelerator_backend_setup` callback hook was deprecated in v1.6 and will be removed in v1.8. Use `setup()` instead.\n",
      "\u001b[2m\u001b[36m(RayExecutor pid=22974)\u001b[0m   rank_zero_deprecation(\n",
      "\u001b[2m\u001b[36m(RayExecutor pid=22974)\u001b[0m /home/ray/anaconda3/lib/python3.9/site-packages/pytorch_lightning/trainer/configuration_validator.py:364: LightningDeprecationWarning: `TuneReportCallback.on_load_checkpoint` will change its signature and behavior in v1.8. If you wish to load the state of the callback, use `load_state_dict` instead. In v1.8 `on_load_checkpoint(..., checkpoint)` will receive the entire loaded checkpoint dictionary instead of callback state.\n",
      "\u001b[2m\u001b[36m(RayExecutor pid=22974)\u001b[0m   rank_zero_deprecation(\n",
      "\u001b[2m\u001b[36m(RayExecutor pid=22974)\u001b[0m /home/ray/anaconda3/lib/python3.9/site-packages/pytorch_lightning/trainer/configuration_validator.py:376: LightningDeprecationWarning: The `Callback.on_batch_start` hook was deprecated in v1.6 and will be removed in v1.8. Please use `Callback.on_train_batch_start` instead.\n",
      "\u001b[2m\u001b[36m(RayExecutor pid=22974)\u001b[0m   rank_zero_deprecation(\n",
      "\u001b[2m\u001b[36m(RayExecutor pid=22974)\u001b[0m /home/ray/anaconda3/lib/python3.9/site-packages/pytorch_lightning/trainer/configuration_validator.py:376: LightningDeprecationWarning: The `Callback.on_batch_end` hook was deprecated in v1.6 and will be removed in v1.8. Please use `Callback.on_train_batch_end` instead.\n",
      "\u001b[2m\u001b[36m(RayExecutor pid=22974)\u001b[0m   rank_zero_deprecation(\n",
      "\u001b[2m\u001b[36m(RayExecutor pid=22974)\u001b[0m /home/ray/anaconda3/lib/python3.9/site-packages/pytorch_lightning/trainer/configuration_validator.py:385: LightningDeprecationWarning: The `Callback.on_epoch_start` hook was deprecated in v1.6 and will be removed in v1.8. Please use `Callback.on_<train/validation/test>_epoch_start` instead.\n",
      "\u001b[2m\u001b[36m(RayExecutor pid=22974)\u001b[0m   rank_zero_deprecation(\n",
      "\u001b[2m\u001b[36m(RayExecutor pid=22974)\u001b[0m /home/ray/anaconda3/lib/python3.9/site-packages/pytorch_lightning/trainer/configuration_validator.py:385: LightningDeprecationWarning: The `Callback.on_epoch_end` hook was deprecated in v1.6 and will be removed in v1.8. Please use `Callback.on_<train/validation/test>_epoch_end` instead.\n",
      "\u001b[2m\u001b[36m(RayExecutor pid=22974)\u001b[0m   rank_zero_deprecation(\n",
      "\u001b[2m\u001b[36m(RayExecutor pid=22974)\u001b[0m /home/ray/anaconda3/lib/python3.9/site-packages/pytorch_lightning/trainer/configuration_validator.py:391: LightningDeprecationWarning: The `Callback.on_pretrain_routine_start` hook has been deprecated in v1.6 and will be removed in v1.8. Please use `Callback.on_fit_start` instead.\n",
      "\u001b[2m\u001b[36m(RayExecutor pid=22974)\u001b[0m   rank_zero_deprecation(\n",
      "\u001b[2m\u001b[36m(RayExecutor pid=22974)\u001b[0m /home/ray/anaconda3/lib/python3.9/site-packages/pytorch_lightning/trainer/configuration_validator.py:391: LightningDeprecationWarning: The `Callback.on_pretrain_routine_end` hook has been deprecated in v1.6 and will be removed in v1.8. Please use `Callback.on_fit_start` instead.\n",
      "\u001b[2m\u001b[36m(RayExecutor pid=22974)\u001b[0m   rank_zero_deprecation(\n",
      "\u001b[2m\u001b[36m(RayExecutor pid=22974)\u001b[0m \n",
      "\u001b[2m\u001b[36m(RayExecutor pid=22974)\u001b[0m    | Name                               | Type                            | Params\n",
      "\u001b[2m\u001b[36m(RayExecutor pid=22974)\u001b[0m ----------------------------------------------------------------------------------------\n",
      "\u001b[2m\u001b[36m(RayExecutor pid=22974)\u001b[0m 0  | loss                               | QuantileLoss                    | 0     \n",
      "\u001b[2m\u001b[36m(RayExecutor pid=22974)\u001b[0m 1  | logging_metrics                    | ModuleList                      | 0     \n",
      "\u001b[2m\u001b[36m(RayExecutor pid=22974)\u001b[0m 2  | input_embeddings                   | MultiEmbedding                  | 14.9 K\n",
      "\u001b[2m\u001b[36m(RayExecutor pid=22974)\u001b[0m 3  | prescalers                         | ModuleDict                      | 48    \n",
      "\u001b[2m\u001b[36m(RayExecutor pid=22974)\u001b[0m 4  | static_variable_selection          | VariableSelectionNetwork        | 1.2 K \n",
      "\u001b[2m\u001b[36m(RayExecutor pid=22974)\u001b[0m 5  | encoder_variable_selection         | VariableSelectionNetwork        | 1.4 K \n",
      "\u001b[2m\u001b[36m(RayExecutor pid=22974)\u001b[0m 6  | decoder_variable_selection         | VariableSelectionNetwork        | 954   \n",
      "\u001b[2m\u001b[36m(RayExecutor pid=22974)\u001b[0m 7  | static_context_variable_selection  | GatedResidualNetwork            | 1.7 K \n",
      "\u001b[2m\u001b[36m(RayExecutor pid=22974)\u001b[0m 8  | static_context_initial_hidden_lstm | GatedResidualNetwork            | 1.7 K \n",
      "\u001b[2m\u001b[36m(RayExecutor pid=22974)\u001b[0m 9  | static_context_initial_cell_lstm   | GatedResidualNetwork            | 1.7 K \n",
      "\u001b[2m\u001b[36m(RayExecutor pid=22974)\u001b[0m 10 | static_context_enrichment          | GatedResidualNetwork            | 1.7 K \n",
      "\u001b[2m\u001b[36m(RayExecutor pid=22974)\u001b[0m 11 | lstm_encoder                       | LSTM                            | 3.4 K \n",
      "\u001b[2m\u001b[36m(RayExecutor pid=22974)\u001b[0m 12 | lstm_decoder                       | LSTM                            | 3.4 K \n",
      "\u001b[2m\u001b[36m(RayExecutor pid=22974)\u001b[0m 13 | post_lstm_gate_encoder             | GatedLinearUnit                 | 840   \n",
      "\u001b[2m\u001b[36m(RayExecutor pid=22974)\u001b[0m 14 | post_lstm_add_norm_encoder         | AddNorm                         | 40    \n",
      "\u001b[2m\u001b[36m(RayExecutor pid=22974)\u001b[0m 15 | static_enrichment                  | GatedResidualNetwork            | 2.1 K \n",
      "\u001b[2m\u001b[36m(RayExecutor pid=22974)\u001b[0m 16 | multihead_attn                     | InterpretableMultiHeadAttention | 879   \n",
      "\u001b[2m\u001b[36m(RayExecutor pid=22974)\u001b[0m 17 | post_attn_gate_norm                | GateAddNorm                     | 880   \n",
      "\u001b[2m\u001b[36m(RayExecutor pid=22974)\u001b[0m 18 | pos_wise_ff                        | GatedResidualNetwork            | 1.7 K \n",
      "\u001b[2m\u001b[36m(RayExecutor pid=22974)\u001b[0m 19 | pre_output_gate_norm               | GateAddNorm                     | 880   \n",
      "\u001b[2m\u001b[36m(RayExecutor pid=22974)\u001b[0m 20 | output_layer                       | Linear                          | 147   \n",
      "\u001b[2m\u001b[36m(RayExecutor pid=22974)\u001b[0m ----------------------------------------------------------------------------------------\n",
      "\u001b[2m\u001b[36m(RayExecutor pid=22974)\u001b[0m 39.6 K    Trainable params\n",
      "\u001b[2m\u001b[36m(RayExecutor pid=22974)\u001b[0m 0         Non-trainable params\n",
      "\u001b[2m\u001b[36m(RayExecutor pid=22974)\u001b[0m 39.6 K    Total params\n",
      "\u001b[2m\u001b[36m(RayExecutor pid=22974)\u001b[0m 0.158     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:   0%|          | 0/2 [00:00<?, ?it/s] \n",
      "Epoch 0:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:02<00:02,  2.10s/it, loss=119, v_num=, train_loss_step=119.0]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A[0m \n",
      "\u001b[2m\u001b[36m(RayExecutor pid=22974)\u001b[0m \n",
      "Validation:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "\u001b[2m\u001b[36m(RayExecutor pid=22974)\u001b[0m \n",
      "Validation DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  4.00it/s]\u001b[A\n",
      "Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:02<00:00,  1.42s/it, loss=119, v_num=, train_loss_step=119.0]\n",
      "Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:03<00:00,  1.80s/it, loss=119, v_num=, train_loss_step=119.0, val_loss=118.0]\n",
      "                                                                      \u001b[A\n",
      "Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:03<00:00,  1.81s/it, loss=119, v_num=, train_loss_step=119.0, val_loss=118.0, train_loss_epoch=119.0]\n",
      "Epoch 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:03<00:00,  1.81s/it, loss=119, v_num=, train_loss_step=119.0, val_loss=118.0, train_loss_epoch=119.0]\n",
      "\u001b[2m\u001b[36m(train_ptf pid=20074)\u001b[0m <class 'pytorch_lightning.trainer.trainer.Trainer'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-15 17:14:03,180\tINFO tune.py:762 -- Total run time: 46.82 seconds (46.70 seconds for the tuning loop).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished in: 0.7805266578992208 minutes\n"
     ]
    }
   ],
   "source": [
    "# By default, Tune reserves 1 CPU core per task.\n",
    "# 3. Customize scaling \n",
    "num_training_workers = min(num_cpu - 2, 32)\n",
    "strategy = RayStrategy(num_workers=num_training_workers, \n",
    "                       num_cpus_per_worker=1, \n",
    "                       use_gpu=False,\n",
    "                        # add this to skip warnings\n",
    "                        # https://github.com/PyTorchLightning/pytorch-lightning/discussions/6761\n",
    "                        find_unused_parameters=False,)\n",
    "\n",
    "# specify all the other config parameters\n",
    "FORECAST_CONFIG = {\n",
    "    \"forecast_horizon\": 168,\n",
    "    \"context_length\": 63,\n",
    "    \"num_gpus\": 0,\n",
    "    \"num_training_workers\": num_training_workers,\n",
    "#     \"batch_size\": 128,     \n",
    "    \"batch_size\": tune.grid_search([128, 64]), \n",
    "    \"epochs\": 2,\n",
    "    \"lr\": 0.05,\n",
    "    \"hidden_size\": 20,\n",
    "    \"dropout\": 0.1,\n",
    "    # \"dropout\": tune.choice([0, 0.05, 0.1]),\n",
    "    \"hidden_continuous_size\": 4,\n",
    "    \"attention_head_size\": num_training_workers,\n",
    "    \"limit_train_batches\": 1, #0.25,\n",
    "    \"fast_mode\": True,\n",
    "    \"tuning_run\": False,\n",
    "}\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "# 4. Run the experiment with Ray Tune.\n",
    "results = tune.run(\n",
    "        train_ptf,\n",
    "        metric=\"loss\",\n",
    "        mode=\"min\",\n",
    "        config=FORECAST_CONFIG,\n",
    "        resources_per_trial=get_tune_resources(num_workers=num_training_workers),\n",
    "        name=\"ptf_nyc\")\n",
    "\n",
    "total_time_taken = time.time() - start\n",
    "print(f\"Finished in: {(time.time()-start)/60} minutes\")\n",
    "\n",
    "# Total run time: 46.82 seconds (46.70 seconds for the tuning loop)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "509c6673",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'ray.tune.analysis.experiment_analysis.ExperimentAnalysis'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'forecast_horizon': 168,\n",
       " 'context_length': 63,\n",
       " 'num_gpus': 0,\n",
       " 'num_training_workers': 6,\n",
       " 'batch_size': 128,\n",
       " 'epochs': 2,\n",
       " 'lr': 0.05,\n",
       " 'hidden_size': 20,\n",
       " 'dropout': 0.1,\n",
       " 'hidden_continuous_size': 4,\n",
       " 'attention_head_size': 6,\n",
       " 'limit_train_batches': 1,\n",
       " 'fast_mode': True,\n",
       " 'tuning_run': False}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(type(results))\n",
    "results.best_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1cea2c0d",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'best_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m## EVALUATE THE DL MODEL\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# get actuals, predictions on validation data, as tensors\u001b[39;00m\n\u001b[1;32m      4\u001b[0m actuals \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([y[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m x, y \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28miter\u001b[39m(validation_loader)])\n\u001b[0;32m----> 5\u001b[0m predictions \u001b[38;5;241m=\u001b[39m \u001b[43mbest_model\u001b[49m\u001b[38;5;241m.\u001b[39mpredict(validation_loader)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# calculate MAE\u001b[39;00m\n\u001b[1;32m      8\u001b[0m MAE \u001b[38;5;241m=\u001b[39m (actuals \u001b[38;5;241m-\u001b[39m predictions)\u001b[38;5;241m.\u001b[39mabs()\u001b[38;5;241m.\u001b[39mmean()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'best_model' is not defined"
     ]
    }
   ],
   "source": [
    "# ## EVALUATE THE DL MODEL\n",
    "\n",
    "# # get actuals, predictions on validation data, as tensors\n",
    "# actuals = torch.cat([y[0] for x, y in iter(validation_loader)])\n",
    "# predictions = best_model.predict(validation_loader)\n",
    "\n",
    "# # calculate MAE\n",
    "# MAE = (actuals - predictions).abs().mean()\n",
    "# print(f\"MAE: {MAE}\")\n",
    "\n",
    "# # calculate RMSE\n",
    "# criterion = torch.nn.MSELoss()\n",
    "# RMSE = torch.sqrt(criterion(actuals, predictions))\n",
    "# print(f\"RMSE: {RMSE}\")\n",
    "\n",
    "# # calculate WQL\n",
    "# print(f\"available quantiles: {best_model.loss.quantiles}\")\n",
    "# # note: to get a single item quantile prediction:\n",
    "# # example: quantile p50 for itemid=\"140\"\n",
    "# # y_quantiles[1].detach().cpu()[43, : x[\"decoder_lengths\"][43]] \n",
    "# # raw predictions are a dictionary from which quantiles can be extracted\n",
    "# raw_predictions, x = best_model.predict(validation_loader, mode=\"raw\", return_x=True)\n",
    "# desired_quantiles = [0.25, 0.5, 0.75]\n",
    "# y_quantiles = best_model.to_quantiles(raw_predictions, desired_quantiles)\n",
    "# WQL = calc_wql(actuals, y_quantiles, desired_quantiles)\n",
    "# print(f\"Mean WQL over quantiles {desired_quantiles}: {WQL}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d7ff0ff",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "**After the Tune experiment has run, select the best model per pickup location.**\n",
    "\n",
    "We can assemble the Tune results ([ResultGrid object](https://docs.ray.io/en/master/tune/examples/tune_analyze_results.html)) into a pandas dataframe, then sort by minimum error, to select the best model per pickup location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c2e1c9ab",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'ExperimentAnalysis' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[33], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# get a list of training loss errors\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m errors \u001b[38;5;241m=\u001b[39m [i\u001b[38;5;241m.\u001b[39mmetrics\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124merror\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m10000.0\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m results]\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# get a list of checkpoints\u001b[39;00m\n\u001b[1;32m      5\u001b[0m checkpoints \u001b[38;5;241m=\u001b[39m [i\u001b[38;5;241m.\u001b[39mcheckpoint \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m results]\n",
      "\u001b[0;31mTypeError\u001b[0m: 'ExperimentAnalysis' object is not iterable"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[1m\u001b[36m(scheduler +20m26s)\u001b[0m Resized to 24 CPUs.\n",
      "\u001b[2m\u001b[1m\u001b[36m(scheduler +25m32s)\u001b[0m Removing 1 nodes of type worker-node-type-0 (idle).\n",
      "\u001b[2m\u001b[1m\u001b[36m(scheduler +25m43s)\u001b[0m Resized to 8 CPUs.\n"
     ]
    }
   ],
   "source": [
    "# get a list of training loss errors\n",
    "errors = [i.metrics.get(\"error\", 10000.0) for i in results]\n",
    "\n",
    "# get a list of checkpoints\n",
    "checkpoints = [i.checkpoint for i in results]\n",
    "\n",
    "# get a list of locations\n",
    "locations = [i.config[\"location\"] for i in results]\n",
    "\n",
    "# get a list of model params\n",
    "algorithm = [i.config[\"algorithm\"] for i in results]\n",
    "\n",
    "# Assemble a pandas dataframe from Tune results\n",
    "results_df = pd.DataFrame(\n",
    "    zip(locations, errors, algorithm, checkpoints),\n",
    "    columns=[\"location_id\", \"error\", \"algorithm\", \"checkpoint\"],\n",
    ")\n",
    "print(results_df.dtypes)\n",
    "results_df.head(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "174e829e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep only 1 model per location_id with minimum error\n",
    "final_df = results_df.copy()\n",
    "final_df = final_df.loc[(final_df.error > 0), :]\n",
    "final_df = final_df.loc[final_df.groupby(\"location_id\")[\"error\"].idxmin()]\n",
    "final_df.sort_values(by=[\"error\"], inplace=True)\n",
    "final_df.set_index(\"location_id\", inplace=True, drop=True)\n",
    "print(final_df.dtypes)\n",
    "final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91cf84b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.describe(include=\"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e85fac64",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df[[\"algorithm\"]].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe533b30",
   "metadata": {},
   "source": [
    "## Load a model from checkpoint and create a forecast  <a class=\"anchor\" id=\"load_checkpoint\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a282de8c",
   "metadata": {},
   "source": [
    "```{tip}\n",
    "[Ray AIR Predictors](https://docs.ray.io/en/latest/ray-air/predictors.html) make batch inference easy since they have internal logic to parallelize the inference.\n",
    "```\n",
    "  \n",
    "Finally, we will restore the best and worst models from checkpoint and inspect the forecasts. Prophet includes a convenient plot library which displays actual data along with backtest predictions and confidence intervals and future forecasts. With ARIMA, you have to create a prediciton manually.\n",
    "\n",
    "- We will easily obtain AIR Checkpoint objects from the Tune results. \n",
    "- We will restore a Prophet or ARIMA model directly from checkpoint, and demonstrate it can be used for prediction.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d72f8bb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the pickup location for the best model\n",
    "sample_location_id = final_df.index[0]\n",
    "\n",
    "# Get the algorithm used\n",
    "sample_algorithm = final_df.loc[[sample_location_id]].algorithm.values[0]\n",
    "\n",
    "# Get a checkpoint directly from the pandas dataframe of Tune results\n",
    "checkpoint = final_df.checkpoint[sample_location_id]\n",
    "print(f\"checkpoint type:: {type(checkpoint)}\")\n",
    "\n",
    "# Restore a model from checkpoint\n",
    "sample_model = checkpoint.to_dict()[\"model\"]\n",
    "\n",
    "# Restore already-created predictions from model training and eval\n",
    "forecast_df = checkpoint.to_dict()[\"forecast_df\"]\n",
    "\n",
    "# Print location and error.\n",
    "sample_error = final_df.loc[[sample_location_id]].error.values[0]\n",
    "print(\n",
    "    f\"location {sample_location_id}, algorithm {sample_algorithm}, best error {sample_error}\"\n",
    ")\n",
    "\n",
    "# If prophet model, use prophet built-in plot\n",
    "if sample_algorithm == \"arima\":\n",
    "    forecast_df[[\"trend\", \"yhat\"]].plot()\n",
    "else:\n",
    "    plot1 = sample_model.plot(forecast_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b013cd7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the pickup location for the worst model\n",
    "sample_location_id = final_df.index[len(final_df) - 2]\n",
    "\n",
    "# Get the algorithm used\n",
    "sample_algorithm = final_df.loc[[sample_location_id]].algorithm.values[0]\n",
    "\n",
    "# Get a checkpoint directly from the pandas dataframe of Tune results\n",
    "checkpoint = final_df.checkpoint[sample_location_id]\n",
    "print(f\"checkpoint type:: {type(checkpoint)}\")\n",
    "\n",
    "# Restore a model from checkpoint\n",
    "sample_model = checkpoint.to_dict()[\"model\"]\n",
    "\n",
    "# Make a prediction using the restored model.\n",
    "prediction = (\n",
    "    sample_model.forecast(2 * (FORECAST_LENGTH + 1))\n",
    "    .reset_index()\n",
    "    .set_index(\"ds\")\n",
    ")\n",
    "prediction[\"trend\"] = None\n",
    "prediction.rename(columns={\"AutoARIMA\": \"yhat\"}, inplace=True)\n",
    "prediction = prediction.tail(FORECAST_LENGTH + 1)\n",
    "\n",
    "# Restore already-created inferences from model training and eval\n",
    "forecast_df = checkpoint.to_dict()[\"forecast_df\"]\n",
    "\n",
    "# Append the prediction to the inferences\n",
    "forecast_df = pd.concat([forecast_df, prediction])\n",
    "\n",
    "# Print location and error.\n",
    "sample_error = final_df.loc[[sample_location_id]].error.values[0]\n",
    "print(\n",
    "    f\"location {sample_location_id}, algorithm {sample_algorithm}, best error {sample_error}\"\n",
    ")\n",
    "\n",
    "# If prophet model, use prophet built-in plot\n",
    "if sample_algorithm == \"arima\":\n",
    "    forecast_df[[\"trend\", \"yhat\"]].plot()\n",
    "else:\n",
    "    plot1 = sample_model.plot(forecast_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "290b40ce",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
