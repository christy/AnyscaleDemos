{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e3b747f1-cc67-4e31-936d-478c2d03dbb2",
   "metadata": {},
   "source": [
    "# Demand forecasting using RNN with LSTM on PyTorch\n",
    "In this tutorial, we will use the <a href=\"https://github.com/ray-project/ray_lightning\">Ray Lightning plugin</a> (which runs on top <a href=\"https://docs.ray.io\">Ray</a>) to speed up training and inference of Google's <a href=\"https://github.com/google-research/google-research/tree/master/tft\">TemporalFusionTransformer</a> algorithm for RNN with LSTM, which has been adapted by <a href=\"https://pytorch-forecasting.readthedocs.io\">PyTorch Forecasting</a>, which in turn is built on <a href=\"https://pytorch-lightning.readthedocs.io\">PyTorch Lightning</a>. PyTorch Lightning is a set of APIs to simplify PyTorch, similar to the relationship of Keras to TensorFlow.\n",
    "\n",
    "Ray can take any Python code and enable it to run distributed across multiple compute nodes.  The compute node cluster could be your own laptop cores or a cluster in any cloud.  Together with <a href=\"https://www.anyscale.com/\">Anyscale cluster management</a>  for cloud, this is how Ray can speed up AI training and inferencing.\n",
    "\n",
    "Demo data is NYC yellow taxi from: https://www1.nyc.gov/site/tlc/about/tlc-trip-record-data.page \n",
    "\n",
    "Forecast goal:  Given 8 months historical taxi trips data for NYC, predict #pickups at each location, at an hourly granularity, for the next week.\n",
    "\n",
    "Suggestion: Make a copy of this notebook. Make edits and run in the copied notebook. This way you will retain the original, executed notebook outputs.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18e549a6-181f-4279-844c-434378f7d9ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "###########\n",
    "# Install libraries\n",
    "###########\n",
    "\n",
    "# !pip install ray     #install ray for the first time\n",
    "# !pip install -U ray  #update ray to latest v1.9\n",
    "# !pip install ray_lightning  #PyTorch Lightning plugin for Ray\n",
    "\n",
    "# !pip install pytorch_lightning==1.4\n",
    "# !pip install pytorch_forecasting\n",
    "\n",
    "# Extra installs for Tensorboard to work with PyTorch on M1 apple\n",
    "# conda uninstall -y tensorflow\n",
    "# conda install -y tensorflow\n",
    "# conda install pyparsing=2.4.2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8f214ce0-9774-4d5b-96a9-773d32c69986",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.8.12\n",
      "pytorch: 1.10.0\n",
      "pytorch_lightning: 1.4.0\n",
      "pytorch_forecasting: 0.9.2\n",
      "ray: 1.9.0\n"
     ]
    }
   ],
   "source": [
    "###########\n",
    "# Import libraries\n",
    "###########\n",
    "\n",
    "# Basic Python\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")  # avoid printing out absolute paths\n",
    "import fastparquet        # Engine for parquet support\n",
    "\n",
    "# Open-source libraries:\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import ray                # Run distributed code\n",
    "from ray.train import Trainer\n",
    "from ray_lightning import RayPlugin\n",
    "\n",
    "# PyTorch, PyTorch Lightning, and PyTorch Forecasting\n",
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "import pytorch_forecasting as ptf\n",
    "\n",
    "# PyTorch visualization uses Tensorboard\n",
    "import tensorflow as tf \n",
    "import tensorboard as tb \n",
    "tf.io.gfile = tb.compat.tensorflow_stub.io.gfile\n",
    "\n",
    "!python --version\n",
    "print(f\"pytorch: {torch.__version__}\")\n",
    "print(f\"pytorch_lightning: {pl.__version__}\")\n",
    "print(f\"pytorch_forecasting: {ptf.__version__}\")\n",
    "print(f\"ray: {ray.__version__}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "00454763-0a5a-4d2d-a17b-7009be375f53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define functions\n",
    "# Todo: Move functions inside util.py\n",
    "\n",
    "# Convert data from pandas to PyTorch tensors.\n",
    "def convert_pandas_pytorch_timeseriesdata(\n",
    "    input_data_pandas_df, config):\n",
    "    \n",
    "    # specify data parameters\n",
    "    FORECAST_HORIZON = config.get(\"forecast_horizon\", 168)\n",
    "    CONTEXT_LENGTH = config.get(\"context_length\", 63)\n",
    "    BATCH_SIZE = config.get(\"batch_size\", 32)\n",
    "    id_col_name = \"pulocationid\"\n",
    "    target_value = \"trip_quantity\"\n",
    "    covariates_numerical = [\"time_idx\", ]\n",
    "                            # \"mean_item_loc_weekday\",\n",
    "                            # \"binned_max_item\"]\n",
    "    covariates_categorical=[\"day_hour\"]\n",
    "    \n",
    "    the_df = input_data_pandas_df.copy()\n",
    "    \n",
    "    # define forecast horizon and training cutoff\n",
    "    max_prediction_length = FORECAST_HORIZON  #decoder length = 1 week forecast horizon\n",
    "    max_encoder_length = CONTEXT_LENGTH  # window or context length\n",
    "    training_cutoff = the_df[\"time_idx\"].max() - max_prediction_length\n",
    "\n",
    "    # convert pandas to PyTorch tensor\n",
    "    training_data = ptf.data.TimeSeriesDataSet(\n",
    "        the_df[lambda x: x.time_idx <= training_cutoff],\n",
    "        allow_missing_timesteps=True,\n",
    "        time_idx=\"time_idx\",\n",
    "        target=target_value,\n",
    "        group_ids=[id_col_name],\n",
    "        min_encoder_length=5,  # allowing predictions without history\n",
    "        max_encoder_length=max_encoder_length,\n",
    "        min_prediction_length=1,\n",
    "        max_prediction_length=max_prediction_length,\n",
    "        static_categoricals=[id_col_name],\n",
    "        # static_reals=[\"avg_population_2017\", \"avg_yearly_household_income_2017\"],\n",
    "        static_reals=[],\n",
    "        time_varying_known_categoricals=covariates_categorical,\n",
    "        # group of categorical variables can be treated as one variable\n",
    "        # variable_groups={\"special_days\": special_days},  \n",
    "        time_varying_known_reals=covariates_numerical,\n",
    "        time_varying_unknown_categoricals=[],\n",
    "        time_varying_unknown_reals=[target_value,],\n",
    "\n",
    "        # https://pytorch-forecasting.readthedocs.io/en/v0.2.4/_modules/pytorch_forecasting/data.html\n",
    "        target_normalizer=ptf.data.GroupNormalizer(\n",
    "            groups=[\"pulocationid\"], \n",
    "            transformation=\"softplus\"  #forces positive values\n",
    "        ), \n",
    "        add_relative_time_idx=True, # add as feature\n",
    "        add_target_scales=True, # add as feature\n",
    "        add_encoder_length=True, # add as feature\n",
    "    )\n",
    "    \n",
    "    # create PyTorch dataloader for training\n",
    "    train_loader = training_data\\\n",
    "                        .to_dataloader(\n",
    "                            train=True, \n",
    "                            batch_size=BATCH_SIZE, \n",
    "                            num_workers=0)\n",
    "    \n",
    "    # create validation PyTorch data \n",
    "    # (predict=True) means make do inference using the validation data\n",
    "    val_dataset = ptf.data.TimeSeriesDataSet\\\n",
    "                    .from_dataset(\n",
    "                        training_data, \n",
    "                        df, predict=True, \n",
    "                        stop_randomization=True)\n",
    "\n",
    "    # create PyTorch dataloaders for inference on validation data\n",
    "    validation_loader = val_dataset\\\n",
    "                    .to_dataloader(\n",
    "                        train=False, \n",
    "                        batch_size=BATCH_SIZE * 10, \n",
    "                        num_workers=0)\n",
    "    \n",
    "    # return original df converted to PyTorch tensors, and pytorch loaders\n",
    "    return training_data, train_loader, validation_loader\n",
    "\n",
    "\n",
    "# Define a PyTorch Lightning TemporalFusionTransformer model\n",
    "def define_pytorch_model(train_dataset, config, ray_plugin):\n",
    "    \n",
    "    # get the parameters from config\n",
    "    NUM_GPU = config.get(\"num_gpus\", 0)\n",
    "    EPOCHS = config.get(\"epochs\", 30)\n",
    "    LR = config.get(\"lr\", 0.01)\n",
    "    HIDDEN_SIZE = config.get(\"hidden_size\", 40)\n",
    "    HIDDEN_LAYERS = config.get(\"hidden_layers\", 2)\n",
    "    ATTENTION_HEAD_SIZE = config.get(\"attention_head_size\", 4)\n",
    "    HIDDEN_CONTINUOUS_SIZE = config.get(\"hidden_continuous_size\", 1)\n",
    "    \n",
    "    print(f\"learning_rate = {LR}\")\n",
    "    print(f\"hidden_size = {HIDDEN_SIZE}\")\n",
    "    print(f\"lstm_layers = {HIDDEN_LAYERS}\")\n",
    "    print(f\"attention_head_size = {ATTENTION_HEAD_SIZE}\")\n",
    "    print(f\"hidden_continuous_size = {HIDDEN_CONTINUOUS_SIZE}\")\n",
    "\n",
    "    # configure early stopping when validation loss does not improve \n",
    "    early_stop_callback = \\\n",
    "        pl.callbacks.EarlyStopping(\n",
    "            monitor=\"val_loss\", \n",
    "            min_delta=1e-4, \n",
    "            patience=10,   #1\n",
    "            verbose=False, \n",
    "            mode=\"min\")\n",
    "    \n",
    "    # configure logging\n",
    "    lr_logger = pl.callbacks.LearningRateMonitor(logging_interval='epoch')\n",
    "    logger = pl.loggers.TensorBoardLogger(\"lightning_logs\")  # log results to a tensorboard\n",
    "\n",
    "    # configure PyTorch trainer with Ray Lightning plugin\n",
    "    torch_trainer = pl.Trainer(\n",
    "        max_epochs=EPOCHS,\n",
    "        gpus=NUM_GPU,\n",
    "        # weights_summary=\"top\",\n",
    "        gradient_clip_val=0.1,\n",
    "        limit_train_batches=30,  # running validation for every 30 batches\n",
    "        # comment in to check that trainer dataset has no serious bugs\n",
    "        # Note: No trainer checkpoints will be saved in fast mode\n",
    "        # fast_dev_run=True,  \n",
    "        callbacks=[lr_logger, early_stop_callback],\n",
    "        logger=logger,\n",
    "        \n",
    "        # regular python - just comment out below line - runs fine!\n",
    "        plugins=[ray_plugin]\n",
    "    )\n",
    "    print(f\"checkpoints location: {torch_trainer.logger.log_dir}\")\n",
    "\n",
    "    # initialize the model\n",
    "    tft = ptf.models.TemporalFusionTransformer.from_dataset(\n",
    "        train_dataset,\n",
    "        learning_rate=LR,\n",
    "        hidden_size=HIDDEN_SIZE, #network size, bigger runs more slowly\n",
    "        lstm_layers=HIDDEN_LAYERS, #hidden layers\n",
    "        attention_head_size=ATTENTION_HEAD_SIZE,  #default 4 cells in LSTM layer\n",
    "        # dropout=0.1,\n",
    "        hidden_continuous_size=HIDDEN_CONTINUOUS_SIZE,  #similar to categorical embedding size\n",
    "        output_size=7,  # 7 quantiles by default\n",
    "        loss=ptf.metrics.QuantileLoss(),\n",
    "        # # uncomment for learning rate finder and otherwise, e.g. to 10 for logging every 10 batches\n",
    "        log_interval=10,  \n",
    "        reduce_on_plateau_patience=4, # reduce learning automatically\n",
    "    )\n",
    "    print(f\"Number of parameters in network: {tft.size()/1e3:.1f}k\")\n",
    "    \n",
    "    # return the model and trainer\n",
    "    return tft, torch_trainer\n",
    "\n",
    "\n",
    "def train_func(config, ray_plugin):\n",
    "    \n",
    "    # read data into pandas dataframe\n",
    "    filename = \"../data/clean_taxi_hourly.parquet\"\n",
    "    df = pd.read_parquet(filename)\n",
    "    df = df[[\"time_idx\", \"pulocationid\", \"day_hour\",\n",
    "                 \"trip_quantity\", \"mean_item_loc_weekday\",\n",
    "                 \"binned_max_item\"]].copy()\n",
    "\n",
    "    # convert data from pandas to PyTorch tensors\n",
    "    train_dataset, train_loader, validation_loader = \\\n",
    "        convert_pandas_pytorch_timeseriesdata(df, config)\n",
    "\n",
    "    # define a PyTorch deep learning forecasting model\n",
    "    model, trainer  = define_pytorch_model(train_dataset, \n",
    "                                           config,\n",
    "                                           ray_plugin)\n",
    "    print(type(model))\n",
    "    print(type(trainer))\n",
    "\n",
    "    # now train the model\n",
    "    trainer.fit(\n",
    "        model,\n",
    "        train_dataloaders=train_loader,\n",
    "        val_dataloaders=validation_loader,\n",
    "    )\n",
    "\n",
    "    # return PyTorch DataLoader and Lightning Trainer\n",
    "    return validation_loader, trainer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80fef3fd-9995-4ff9-9c00-ea4061dd59dc",
   "metadata": {},
   "source": [
    "# Create and train a baseline model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "34189646-564b-4df5-a4e6-9fdf7252240d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data type: <class 'pandas.core.frame.DataFrame'>\n",
      "Converted data type: <class 'pytorch_forecasting.data.timeseries.TimeSeriesDataSet'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(29.2463)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# specify all the config parameters \n",
    "config = {\"forecast_horizon\": 168, \"context_length\": 63,\n",
    "          \"num_gpus\":0, \"batch_size\": 128, \"epochs\": 2,\n",
    "          \"lr\": 0.05, \"hidden_size\": 16, \"hidden_layers\": 2,\n",
    "          \"attention_head_size\": 4, \"hidden_continuous_size\": 2}\n",
    "\n",
    "# read data into pandas dataframe\n",
    "filename = \"~/Documents/AnyscaleDemosPrivate/demos/forecasting_demo/data/clean_taxi_hourly.parquet\"\n",
    "df = pd.read_parquet(filename)\n",
    "df = df[[\"time_idx\", \"pulocationid\", \"day_hour\",\n",
    "             \"trip_quantity\", \"mean_item_loc_weekday\",\n",
    "             \"binned_max_item\"]].copy()\n",
    "\n",
    "# convert data from pandas to PyTorch tensors\n",
    "print(f\"Input data type: {type(df)}\")\n",
    "train_dataset, train_loader, validation_loader = \\\n",
    "    convert_pandas_pytorch_timeseriesdata(df, config)\n",
    "print(f\"Converted data type: {type(train_dataset)}\")\n",
    "\n",
    "# calculate baseline mean absolute error, i.e. predict next value as the last available value from the history\n",
    "actuals = torch.cat(\n",
    "            [\n",
    "                y for x, (y, weight) in iter(validation_loader)\n",
    "            ]\n",
    "          )\n",
    "baseline_predictions = ptf.models.Baseline().predict(validation_loader)\n",
    "\n",
    "\n",
    "## EVALUATE THE BASELINE MODEL\n",
    "# print MAE\n",
    "(actuals - baseline_predictions).abs().mean()\n",
    "\n",
    "#29.2463\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfa434b6-ac42-4c67-8526-289fda1f8d06",
   "metadata": {},
   "source": [
    "# Train a PyTorch Lightning DL Forecast Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "13957d6b-e02a-4ca1-8761-06b7f697dc6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-13 18:25:05,260\tINFO services.py:1338 -- View the Ray dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8266\u001b[39m\u001b[22m\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learning_rate = 0.05\n",
      "hidden_size = 16\n",
      "lstm_layers = 2\n",
      "attention_head_size = 2\n",
      "hidden_continuous_size = 1\n",
      "checkpoints location: lightning_logs/default/version_1\n",
      "Number of parameters in network: 27.1k\n",
      "<class 'pytorch_forecasting.models.temporal_fusion_transformer.TemporalFusionTransformer'>\n",
      "<class 'pytorch_lightning.trainer.trainer.Trainer'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(RayExecutor pid=41273)\u001b[0m initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/4\n",
      "\u001b[2m\u001b[36m(RayExecutor pid=41274)\u001b[0m initializing ddp: GLOBAL_RANK: 1, MEMBER: 2/4\n",
      "\u001b[2m\u001b[36m(RayExecutor pid=41278)\u001b[0m initializing ddp: GLOBAL_RANK: 2, MEMBER: 3/4\n",
      "\u001b[2m\u001b[36m(RayExecutor pid=41271)\u001b[0m initializing ddp: GLOBAL_RANK: 3, MEMBER: 4/4\n",
      "\u001b[2m\u001b[36m(RayExecutor pid=41273)\u001b[0m \n",
      "\u001b[2m\u001b[36m(RayExecutor pid=41273)\u001b[0m    | Name                               | Type                            | Params\n",
      "\u001b[2m\u001b[36m(RayExecutor pid=41273)\u001b[0m ----------------------------------------------------------------------------------------\n",
      "\u001b[2m\u001b[36m(RayExecutor pid=41273)\u001b[0m 0  | loss                               | QuantileLoss                    | 0     \n",
      "\u001b[2m\u001b[36m(RayExecutor pid=41273)\u001b[0m 1  | logging_metrics                    | ModuleList                      | 0     \n",
      "\u001b[2m\u001b[36m(RayExecutor pid=41273)\u001b[0m 2  | input_embeddings                   | MultiEmbedding                  | 6.8 K \n",
      "\u001b[2m\u001b[36m(RayExecutor pid=41273)\u001b[0m 3  | prescalers                         | ModuleDict                      | 12    \n",
      "\u001b[2m\u001b[36m(RayExecutor pid=41273)\u001b[0m 4  | static_variable_selection          | VariableSelectionNetwork        | 658   \n",
      "\u001b[2m\u001b[36m(RayExecutor pid=41273)\u001b[0m 5  | encoder_variable_selection         | VariableSelectionNetwork        | 722   \n",
      "\u001b[2m\u001b[36m(RayExecutor pid=41273)\u001b[0m 6  | decoder_variable_selection         | VariableSelectionNetwork        | 504   \n",
      "\u001b[2m\u001b[36m(RayExecutor pid=41273)\u001b[0m 7  | static_context_variable_selection  | GatedResidualNetwork            | 1.1 K \n",
      "\u001b[2m\u001b[36m(RayExecutor pid=41273)\u001b[0m 8  | static_context_initial_hidden_lstm | GatedResidualNetwork            | 1.1 K \n",
      "\u001b[2m\u001b[36m(RayExecutor pid=41273)\u001b[0m 9  | static_context_initial_cell_lstm   | GatedResidualNetwork            | 1.1 K \n",
      "\u001b[2m\u001b[36m(RayExecutor pid=41273)\u001b[0m 10 | static_context_enrichment          | GatedResidualNetwork            | 1.1 K \n",
      "\u001b[2m\u001b[36m(RayExecutor pid=41273)\u001b[0m 11 | lstm_encoder                       | LSTM                            | 4.4 K \n",
      "\u001b[2m\u001b[36m(RayExecutor pid=41273)\u001b[0m 12 | lstm_decoder                       | LSTM                            | 4.4 K \n",
      "\u001b[2m\u001b[36m(RayExecutor pid=41273)\u001b[0m 13 | post_lstm_gate_encoder             | GatedLinearUnit                 | 544   \n",
      "\u001b[2m\u001b[36m(RayExecutor pid=41273)\u001b[0m 14 | post_lstm_add_norm_encoder         | AddNorm                         | 32    \n",
      "\u001b[2m\u001b[36m(RayExecutor pid=41273)\u001b[0m 15 | static_enrichment                  | GatedResidualNetwork            | 1.4 K \n",
      "\u001b[2m\u001b[36m(RayExecutor pid=41273)\u001b[0m 16 | multihead_attn                     | InterpretableMultiHeadAttention | 808   \n",
      "\u001b[2m\u001b[36m(RayExecutor pid=41273)\u001b[0m 17 | post_attn_gate_norm                | GateAddNorm                     | 576   \n",
      "\u001b[2m\u001b[36m(RayExecutor pid=41273)\u001b[0m 18 | pos_wise_ff                        | GatedResidualNetwork            | 1.1 K \n",
      "\u001b[2m\u001b[36m(RayExecutor pid=41273)\u001b[0m 19 | pre_output_gate_norm               | GateAddNorm                     | 576   \n",
      "\u001b[2m\u001b[36m(RayExecutor pid=41273)\u001b[0m 20 | output_layer                       | Linear                          | 119   \n",
      "\u001b[2m\u001b[36m(RayExecutor pid=41273)\u001b[0m ----------------------------------------------------------------------------------------\n",
      "\u001b[2m\u001b[36m(RayExecutor pid=41273)\u001b[0m 27.1 K    Trainable params\n",
      "\u001b[2m\u001b[36m(RayExecutor pid=41273)\u001b[0m 0         Non-trainable params\n",
      "\u001b[2m\u001b[36m(RayExecutor pid=41273)\u001b[0m 27.1 K    Total params\n",
      "\u001b[2m\u001b[36m(RayExecutor pid=41273)\u001b[0m 0.108     Total estimated model params size (MB)\n",
      "\u001b[2m\u001b[36m(RayExecutor pid=41273)\u001b[0m /Users/christy/mambaforge/envs/ray/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:105: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001b[2m\u001b[36m(RayExecutor pid=41273)\u001b[0m   rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation sanity check:   0%|          | 0/1 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(RayExecutor pid=41274)\u001b[0m /Users/christy/mambaforge/envs/ray/lib/python3.8/site-packages/pytorch_forecasting/data/timeseries.py:1657: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  ../torch/csrc/utils/tensor_new.cpp:201.)\n",
      "\u001b[2m\u001b[36m(RayExecutor pid=41274)\u001b[0m   target_scale = torch.tensor([batch[0][\"target_scale\"] for batch in batches], dtype=torch.float)\n",
      "\u001b[2m\u001b[36m(RayExecutor pid=41278)\u001b[0m /Users/christy/mambaforge/envs/ray/lib/python3.8/site-packages/pytorch_forecasting/data/timeseries.py:1657: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  ../torch/csrc/utils/tensor_new.cpp:201.)\n",
      "\u001b[2m\u001b[36m(RayExecutor pid=41278)\u001b[0m   target_scale = torch.tensor([batch[0][\"target_scale\"] for batch in batches], dtype=torch.float)\n",
      "\u001b[2m\u001b[36m(RayExecutor pid=41271)\u001b[0m /Users/christy/mambaforge/envs/ray/lib/python3.8/site-packages/pytorch_forecasting/data/timeseries.py:1657: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  ../torch/csrc/utils/tensor_new.cpp:201.)\n",
      "\u001b[2m\u001b[36m(RayExecutor pid=41271)\u001b[0m   target_scale = torch.tensor([batch[0][\"target_scale\"] for batch in batches], dtype=torch.float)\n",
      "\u001b[2m\u001b[36m(RayExecutor pid=41273)\u001b[0m /Users/christy/mambaforge/envs/ray/lib/python3.8/site-packages/pytorch_forecasting/data/timeseries.py:1657: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  ../torch/csrc/utils/tensor_new.cpp:201.)\n",
      "\u001b[2m\u001b[36m(RayExecutor pid=41273)\u001b[0m   target_scale = torch.tensor([batch[0][\"target_scale\"] for batch in batches], dtype=torch.float)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation sanity check: 100%|██████████| 1/1 [00:01<00:00,  1.66s/it]\n",
      "Epoch 0:   0%|          | 0/31 [00:00<00:00, 6626.07it/s]             \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(RayExecutor pid=41273)\u001b[0m /Users/christy/mambaforge/envs/ray/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:105: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\u001b[2m\u001b[36m(RayExecutor pid=41273)\u001b[0m   rank_zero_warn(\n",
      "\u001b[2m\u001b[36m(RayExecutor pid=41273)\u001b[0m /Users/christy/mambaforge/envs/ray/lib/python3.8/site-packages/pytorch_lightning/trainer/data_loading.py:322: UserWarning: The number of training samples (30) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "\u001b[2m\u001b[36m(RayExecutor pid=41273)\u001b[0m   rank_zero_warn(\n",
      "\u001b[2m\u001b[36m(RayExecutor pid=41278)\u001b[0m [W reducer.cpp:1303] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\n",
      "\u001b[2m\u001b[36m(RayExecutor pid=41274)\u001b[0m [W reducer.cpp:1303] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\n",
      "\u001b[2m\u001b[36m(RayExecutor pid=41271)\u001b[0m [W reducer.cpp:1303] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\n",
      "\u001b[2m\u001b[36m(RayExecutor pid=41273)\u001b[0m [W reducer.cpp:1303] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\n"
     ]
    },
    {
     "ename": "RayTaskError(AttributeError)",
     "evalue": "\u001b[36mray::RayExecutor.execute()\u001b[39m (pid=41278, ip=127.0.0.1, repr=<ray_lightning.ray_ddp.RayExecutor object at 0x13546a3a0>)\n  File \"/Users/christy/mambaforge/envs/ray/lib/python3.8/site-packages/ray_lightning/ray_ddp.py\", line 54, in execute\n    return fn(*args, **kwargs)\n  File \"/Users/christy/mambaforge/envs/ray/lib/python3.8/site-packages/ray_lightning/ray_ddp.py\", line 297, in execute_remote\n    super(RayPlugin, self).new_process(\n  File \"/Users/christy/mambaforge/envs/ray/lib/python3.8/site-packages/pytorch_lightning/plugins/training_type/ddp_spawn.py\", line 201, in new_process\n    results = trainer.run_stage()\n  File \"/Users/christy/mambaforge/envs/ray/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 996, in run_stage\n    return self._run_train()\n  File \"/Users/christy/mambaforge/envs/ray/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 1045, in _run_train\n    self.fit_loop.run()\n  File \"/Users/christy/mambaforge/envs/ray/lib/python3.8/site-packages/pytorch_lightning/loops/base.py\", line 111, in run\n    self.advance(*args, **kwargs)\n  File \"/Users/christy/mambaforge/envs/ray/lib/python3.8/site-packages/pytorch_lightning/loops/fit_loop.py\", line 200, in advance\n    epoch_output = self.epoch_loop.run(train_dataloader)\n  File \"/Users/christy/mambaforge/envs/ray/lib/python3.8/site-packages/pytorch_lightning/loops/base.py\", line 111, in run\n    self.advance(*args, **kwargs)\n  File \"/Users/christy/mambaforge/envs/ray/lib/python3.8/site-packages/pytorch_lightning/loops/epoch/training_epoch_loop.py\", line 131, in advance\n    batch_output = self.batch_loop.run(batch, self.iteration_count, self._dataloader_idx)\n  File \"/Users/christy/mambaforge/envs/ray/lib/python3.8/site-packages/pytorch_lightning/loops/batch/training_batch_loop.py\", line 100, in run\n    super().run(batch, batch_idx, dataloader_idx)\n  File \"/Users/christy/mambaforge/envs/ray/lib/python3.8/site-packages/pytorch_lightning/loops/base.py\", line 111, in run\n    self.advance(*args, **kwargs)\n  File \"/Users/christy/mambaforge/envs/ray/lib/python3.8/site-packages/pytorch_lightning/loops/batch/training_batch_loop.py\", line 147, in advance\n    result = self._run_optimization(batch_idx, split_batch, opt_idx, optimizer)\n  File \"/Users/christy/mambaforge/envs/ray/lib/python3.8/site-packages/pytorch_lightning/loops/batch/training_batch_loop.py\", line 201, in _run_optimization\n    self._optimizer_step(optimizer, opt_idx, batch_idx, closure)\n  File \"/Users/christy/mambaforge/envs/ray/lib/python3.8/site-packages/pytorch_lightning/loops/batch/training_batch_loop.py\", line 394, in _optimizer_step\n    model_ref.optimizer_step(\n  File \"/Users/christy/mambaforge/envs/ray/lib/python3.8/site-packages/pytorch_lightning/core/lightning.py\", line 1593, in optimizer_step\n    optimizer.step(closure=optimizer_closure)\n  File \"/Users/christy/mambaforge/envs/ray/lib/python3.8/site-packages/pytorch_lightning/core/optimizer.py\", line 209, in step\n    self.__optimizer_step(*args, closure=closure, profiler_name=profiler_name, **kwargs)\n  File \"/Users/christy/mambaforge/envs/ray/lib/python3.8/site-packages/pytorch_lightning/core/optimizer.py\", line 129, in __optimizer_step\n    trainer.accelerator.optimizer_step(optimizer, self._optimizer_idx, lambda_closure=closure, **kwargs)\n  File \"/Users/christy/mambaforge/envs/ray/lib/python3.8/site-packages/pytorch_lightning/accelerators/accelerator.py\", line 296, in optimizer_step\n    self.run_optimizer_step(optimizer, opt_idx, lambda_closure, **kwargs)\n  File \"/Users/christy/mambaforge/envs/ray/lib/python3.8/site-packages/pytorch_lightning/accelerators/accelerator.py\", line 303, in run_optimizer_step\n    self.training_type_plugin.optimizer_step(optimizer, lambda_closure=lambda_closure, **kwargs)\n  File \"/Users/christy/mambaforge/envs/ray/lib/python3.8/site-packages/pytorch_lightning/plugins/training_type/training_type_plugin.py\", line 226, in optimizer_step\n    optimizer.step(closure=lambda_closure, **kwargs)\n  File \"/Users/christy/mambaforge/envs/ray/lib/python3.8/site-packages/torch/optim/optimizer.py\", line 88, in wrapper\n    return func(*args, **kwargs)\n  File \"/Users/christy/mambaforge/envs/ray/lib/python3.8/site-packages/pytorch_forecasting/optim.py\", line 195, in step\n    buffered = self.radam_buffer[int(state[\"step\"] % 10)]\nAttributeError: 'Ranger' object has no attribute 'radam_buffer'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRayTaskError(AttributeError)\u001b[0m              Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;32m/var/folders/0g/jfs_l_113_356_c0rfp4jd8c0000gn/T/ipykernel_41181/774039944.py\u001b[0m in \u001b[0;36mtrain_func\u001b[0;34m(config, ray_plugin)\u001b[0m\n\u001b[1;32m    172\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m     \u001b[0;31m# now train the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 174\u001b[0;31m     trainer.fit(\n\u001b[0m\u001b[1;32m    175\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m         \u001b[0mtrain_dataloaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/mambaforge/envs/ray/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, train_dataloader)\u001b[0m\n\u001b[1;32m    551\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheckpoint_connector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresume_start\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 553\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    554\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    555\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstopped\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/mambaforge/envs/ray/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, model)\u001b[0m\n\u001b[1;32m    916\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    917\u001b[0m         \u001b[0;31m# dispatch `start_training` or `start_evaluating` or `start_predicting`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 918\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    919\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    920\u001b[0m         \u001b[0;31m# plugin will finalized fitting (e.g. ddp_spawn will load trained model)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/mambaforge/envs/ray/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36m_dispatch\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    984\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccelerator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart_predicting\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    985\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 986\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccelerator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart_training\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    987\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    988\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrun_stage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/mambaforge/envs/ray/lib/python3.8/site-packages/pytorch_lightning/accelerators/accelerator.py\u001b[0m in \u001b[0;36mstart_training\u001b[0;34m(self, trainer)\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstart_training\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"pl.Trainer\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_type_plugin\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart_training\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstart_evaluating\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"pl.Trainer\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/mambaforge/envs/ray/lib/python3.8/site-packages/ray_lightning/ray_ddp.py\u001b[0m in \u001b[0;36mstart_training\u001b[0;34m(self, trainer)\u001b[0m\n\u001b[1;32m    243\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstart_training\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 245\u001b[0;31m         \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecution_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtune_enabled\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    246\u001b[0m         \u001b[0;31m# reset optimizers, since main process is never used for training and\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m         \u001b[0;31m# thus does not have a valid optim state.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/mambaforge/envs/ray/lib/python3.8/site-packages/ray_lightning/ray_ddp.py\u001b[0m in \u001b[0;36mexecution_loop\u001b[0;34m(self, trainer, tune_enabled)\u001b[0m\n\u001b[1;32m    206\u001b[0m         ]\n\u001b[1;32m    207\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 208\u001b[0;31m         \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocess_results\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfutures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mqueue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    209\u001b[0m         \u001b[0;31m# Get the results, checkpoint path, and model weights from worker 0.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m         \u001b[0mresults\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbest_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate_stream\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/mambaforge/envs/ray/lib/python3.8/site-packages/ray_lightning/util.py\u001b[0m in \u001b[0;36mprocess_results\u001b[0;34m(training_result_futures, queue)\u001b[0m\n\u001b[1;32m     47\u001b[0m             \u001b[0m_handle_queue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mqueue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0mready\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnot_ready\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnot_ready\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m         \u001b[0mray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mready\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m     \u001b[0mray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mready\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/mambaforge/envs/ray/lib/python3.8/site-packages/ray/_private/client_mode_hook.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    103\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m\"init\"\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mis_client_mode_enabled_by_default\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 105\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/mambaforge/envs/ray/lib/python3.8/site-packages/ray/worker.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(object_refs, timeout)\u001b[0m\n\u001b[1;32m   1711\u001b[0m                     \u001b[0mworker\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore_worker\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump_object_store_memory_usage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1712\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRayTaskError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1713\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_instanceof_cause\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1714\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1715\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRayTaskError(AttributeError)\u001b[0m: \u001b[36mray::RayExecutor.execute()\u001b[39m (pid=41278, ip=127.0.0.1, repr=<ray_lightning.ray_ddp.RayExecutor object at 0x13546a3a0>)\n  File \"/Users/christy/mambaforge/envs/ray/lib/python3.8/site-packages/ray_lightning/ray_ddp.py\", line 54, in execute\n    return fn(*args, **kwargs)\n  File \"/Users/christy/mambaforge/envs/ray/lib/python3.8/site-packages/ray_lightning/ray_ddp.py\", line 297, in execute_remote\n    super(RayPlugin, self).new_process(\n  File \"/Users/christy/mambaforge/envs/ray/lib/python3.8/site-packages/pytorch_lightning/plugins/training_type/ddp_spawn.py\", line 201, in new_process\n    results = trainer.run_stage()\n  File \"/Users/christy/mambaforge/envs/ray/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 996, in run_stage\n    return self._run_train()\n  File \"/Users/christy/mambaforge/envs/ray/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 1045, in _run_train\n    self.fit_loop.run()\n  File \"/Users/christy/mambaforge/envs/ray/lib/python3.8/site-packages/pytorch_lightning/loops/base.py\", line 111, in run\n    self.advance(*args, **kwargs)\n  File \"/Users/christy/mambaforge/envs/ray/lib/python3.8/site-packages/pytorch_lightning/loops/fit_loop.py\", line 200, in advance\n    epoch_output = self.epoch_loop.run(train_dataloader)\n  File \"/Users/christy/mambaforge/envs/ray/lib/python3.8/site-packages/pytorch_lightning/loops/base.py\", line 111, in run\n    self.advance(*args, **kwargs)\n  File \"/Users/christy/mambaforge/envs/ray/lib/python3.8/site-packages/pytorch_lightning/loops/epoch/training_epoch_loop.py\", line 131, in advance\n    batch_output = self.batch_loop.run(batch, self.iteration_count, self._dataloader_idx)\n  File \"/Users/christy/mambaforge/envs/ray/lib/python3.8/site-packages/pytorch_lightning/loops/batch/training_batch_loop.py\", line 100, in run\n    super().run(batch, batch_idx, dataloader_idx)\n  File \"/Users/christy/mambaforge/envs/ray/lib/python3.8/site-packages/pytorch_lightning/loops/base.py\", line 111, in run\n    self.advance(*args, **kwargs)\n  File \"/Users/christy/mambaforge/envs/ray/lib/python3.8/site-packages/pytorch_lightning/loops/batch/training_batch_loop.py\", line 147, in advance\n    result = self._run_optimization(batch_idx, split_batch, opt_idx, optimizer)\n  File \"/Users/christy/mambaforge/envs/ray/lib/python3.8/site-packages/pytorch_lightning/loops/batch/training_batch_loop.py\", line 201, in _run_optimization\n    self._optimizer_step(optimizer, opt_idx, batch_idx, closure)\n  File \"/Users/christy/mambaforge/envs/ray/lib/python3.8/site-packages/pytorch_lightning/loops/batch/training_batch_loop.py\", line 394, in _optimizer_step\n    model_ref.optimizer_step(\n  File \"/Users/christy/mambaforge/envs/ray/lib/python3.8/site-packages/pytorch_lightning/core/lightning.py\", line 1593, in optimizer_step\n    optimizer.step(closure=optimizer_closure)\n  File \"/Users/christy/mambaforge/envs/ray/lib/python3.8/site-packages/pytorch_lightning/core/optimizer.py\", line 209, in step\n    self.__optimizer_step(*args, closure=closure, profiler_name=profiler_name, **kwargs)\n  File \"/Users/christy/mambaforge/envs/ray/lib/python3.8/site-packages/pytorch_lightning/core/optimizer.py\", line 129, in __optimizer_step\n    trainer.accelerator.optimizer_step(optimizer, self._optimizer_idx, lambda_closure=closure, **kwargs)\n  File \"/Users/christy/mambaforge/envs/ray/lib/python3.8/site-packages/pytorch_lightning/accelerators/accelerator.py\", line 296, in optimizer_step\n    self.run_optimizer_step(optimizer, opt_idx, lambda_closure, **kwargs)\n  File \"/Users/christy/mambaforge/envs/ray/lib/python3.8/site-packages/pytorch_lightning/accelerators/accelerator.py\", line 303, in run_optimizer_step\n    self.training_type_plugin.optimizer_step(optimizer, lambda_closure=lambda_closure, **kwargs)\n  File \"/Users/christy/mambaforge/envs/ray/lib/python3.8/site-packages/pytorch_lightning/plugins/training_type/training_type_plugin.py\", line 226, in optimizer_step\n    optimizer.step(closure=lambda_closure, **kwargs)\n  File \"/Users/christy/mambaforge/envs/ray/lib/python3.8/site-packages/torch/optim/optimizer.py\", line 88, in wrapper\n    return func(*args, **kwargs)\n  File \"/Users/christy/mambaforge/envs/ray/lib/python3.8/site-packages/pytorch_forecasting/optim.py\", line 195, in step\n    buffered = self.radam_buffer[int(state[\"step\"] % 10)]\nAttributeError: 'Ranger' object has no attribute 'radam_buffer'"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-13 18:25:23,371\tERROR worker.py:84 -- Unhandled error (suppress with RAY_IGNORE_UNHANDLED_ERRORS=1): \u001b[36mray::RayExecutor.execute()\u001b[39m (pid=41274, ip=127.0.0.1, repr=<ray_lightning.ray_ddp.RayExecutor object at 0x11654a3a0>)\n",
      "  File \"/Users/christy/mambaforge/envs/ray/lib/python3.8/site-packages/ray_lightning/ray_ddp.py\", line 54, in execute\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/Users/christy/mambaforge/envs/ray/lib/python3.8/site-packages/ray_lightning/ray_ddp.py\", line 297, in execute_remote\n",
      "    super(RayPlugin, self).new_process(\n",
      "  File \"/Users/christy/mambaforge/envs/ray/lib/python3.8/site-packages/pytorch_lightning/plugins/training_type/ddp_spawn.py\", line 201, in new_process\n",
      "    results = trainer.run_stage()\n",
      "  File \"/Users/christy/mambaforge/envs/ray/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 996, in run_stage\n",
      "    return self._run_train()\n",
      "  File \"/Users/christy/mambaforge/envs/ray/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 1045, in _run_train\n",
      "    self.fit_loop.run()\n",
      "  File \"/Users/christy/mambaforge/envs/ray/lib/python3.8/site-packages/pytorch_lightning/loops/base.py\", line 111, in run\n",
      "    self.advance(*args, **kwargs)\n",
      "  File \"/Users/christy/mambaforge/envs/ray/lib/python3.8/site-packages/pytorch_lightning/loops/fit_loop.py\", line 200, in advance\n",
      "    epoch_output = self.epoch_loop.run(train_dataloader)\n",
      "  File \"/Users/christy/mambaforge/envs/ray/lib/python3.8/site-packages/pytorch_lightning/loops/base.py\", line 111, in run\n",
      "    self.advance(*args, **kwargs)\n",
      "  File \"/Users/christy/mambaforge/envs/ray/lib/python3.8/site-packages/pytorch_lightning/loops/epoch/training_epoch_loop.py\", line 131, in advance\n",
      "    batch_output = self.batch_loop.run(batch, self.iteration_count, self._dataloader_idx)\n",
      "  File \"/Users/christy/mambaforge/envs/ray/lib/python3.8/site-packages/pytorch_lightning/loops/batch/training_batch_loop.py\", line 100, in run\n",
      "    super().run(batch, batch_idx, dataloader_idx)\n",
      "  File \"/Users/christy/mambaforge/envs/ray/lib/python3.8/site-packages/pytorch_lightning/loops/base.py\", line 111, in run\n",
      "    self.advance(*args, **kwargs)\n",
      "  File \"/Users/christy/mambaforge/envs/ray/lib/python3.8/site-packages/pytorch_lightning/loops/batch/training_batch_loop.py\", line 147, in advance\n",
      "    result = self._run_optimization(batch_idx, split_batch, opt_idx, optimizer)\n",
      "  File \"/Users/christy/mambaforge/envs/ray/lib/python3.8/site-packages/pytorch_lightning/loops/batch/training_batch_loop.py\", line 201, in _run_optimization\n",
      "    self._optimizer_step(optimizer, opt_idx, batch_idx, closure)\n",
      "  File \"/Users/christy/mambaforge/envs/ray/lib/python3.8/site-packages/pytorch_lightning/loops/batch/training_batch_loop.py\", line 394, in _optimizer_step\n",
      "    model_ref.optimizer_step(\n",
      "  File \"/Users/christy/mambaforge/envs/ray/lib/python3.8/site-packages/pytorch_lightning/core/lightning.py\", line 1593, in optimizer_step\n",
      "    optimizer.step(closure=optimizer_closure)\n",
      "  File \"/Users/christy/mambaforge/envs/ray/lib/python3.8/site-packages/pytorch_lightning/core/optimizer.py\", line 209, in step\n",
      "    self.__optimizer_step(*args, closure=closure, profiler_name=profiler_name, **kwargs)\n",
      "  File \"/Users/christy/mambaforge/envs/ray/lib/python3.8/site-packages/pytorch_lightning/core/optimizer.py\", line 129, in __optimizer_step\n",
      "    trainer.accelerator.optimizer_step(optimizer, self._optimizer_idx, lambda_closure=closure, **kwargs)\n",
      "  File \"/Users/christy/mambaforge/envs/ray/lib/python3.8/site-packages/pytorch_lightning/accelerators/accelerator.py\", line 296, in optimizer_step\n",
      "    self.run_optimizer_step(optimizer, opt_idx, lambda_closure, **kwargs)\n",
      "  File \"/Users/christy/mambaforge/envs/ray/lib/python3.8/site-packages/pytorch_lightning/accelerators/accelerator.py\", line 303, in run_optimizer_step\n",
      "    self.training_type_plugin.optimizer_step(optimizer, lambda_closure=lambda_closure, **kwargs)\n",
      "  File \"/Users/christy/mambaforge/envs/ray/lib/python3.8/site-packages/pytorch_lightning/plugins/training_type/training_type_plugin.py\", line 226, in optimizer_step\n",
      "    optimizer.step(closure=lambda_closure, **kwargs)\n",
      "  File \"/Users/christy/mambaforge/envs/ray/lib/python3.8/site-packages/torch/optim/optimizer.py\", line 88, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/Users/christy/mambaforge/envs/ray/lib/python3.8/site-packages/pytorch_forecasting/optim.py\", line 195, in step\n",
      "    buffered = self.radam_buffer[int(state[\"step\"] % 10)]\n",
      "AttributeError: 'Ranger' object has no attribute 'radam_buffer'\n",
      "2021-12-13 18:25:23,373\tERROR worker.py:84 -- Unhandled error (suppress with RAY_IGNORE_UNHANDLED_ERRORS=1): \u001b[36mray::RayExecutor.execute()\u001b[39m (pid=41273, ip=127.0.0.1, repr=<ray_lightning.ray_ddp.RayExecutor object at 0x133e2a3a0>)\n",
      "  File \"/Users/christy/mambaforge/envs/ray/lib/python3.8/site-packages/ray_lightning/ray_ddp.py\", line 54, in execute\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/Users/christy/mambaforge/envs/ray/lib/python3.8/site-packages/ray_lightning/ray_ddp.py\", line 297, in execute_remote\n",
      "    super(RayPlugin, self).new_process(\n",
      "  File \"/Users/christy/mambaforge/envs/ray/lib/python3.8/site-packages/pytorch_lightning/plugins/training_type/ddp_spawn.py\", line 201, in new_process\n",
      "    results = trainer.run_stage()\n",
      "  File \"/Users/christy/mambaforge/envs/ray/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 996, in run_stage\n",
      "    return self._run_train()\n",
      "  File \"/Users/christy/mambaforge/envs/ray/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 1045, in _run_train\n",
      "    self.fit_loop.run()\n",
      "  File \"/Users/christy/mambaforge/envs/ray/lib/python3.8/site-packages/pytorch_lightning/loops/base.py\", line 111, in run\n",
      "    self.advance(*args, **kwargs)\n",
      "  File \"/Users/christy/mambaforge/envs/ray/lib/python3.8/site-packages/pytorch_lightning/loops/fit_loop.py\", line 200, in advance\n",
      "    epoch_output = self.epoch_loop.run(train_dataloader)\n",
      "  File \"/Users/christy/mambaforge/envs/ray/lib/python3.8/site-packages/pytorch_lightning/loops/base.py\", line 111, in run\n",
      "    self.advance(*args, **kwargs)\n",
      "  File \"/Users/christy/mambaforge/envs/ray/lib/python3.8/site-packages/pytorch_lightning/loops/epoch/training_epoch_loop.py\", line 131, in advance\n",
      "    batch_output = self.batch_loop.run(batch, self.iteration_count, self._dataloader_idx)\n",
      "  File \"/Users/christy/mambaforge/envs/ray/lib/python3.8/site-packages/pytorch_lightning/loops/batch/training_batch_loop.py\", line 100, in run\n",
      "    super().run(batch, batch_idx, dataloader_idx)\n",
      "  File \"/Users/christy/mambaforge/envs/ray/lib/python3.8/site-packages/pytorch_lightning/loops/base.py\", line 111, in run\n",
      "    self.advance(*args, **kwargs)\n",
      "  File \"/Users/christy/mambaforge/envs/ray/lib/python3.8/site-packages/pytorch_lightning/loops/batch/training_batch_loop.py\", line 147, in advance\n",
      "    result = self._run_optimization(batch_idx, split_batch, opt_idx, optimizer)\n",
      "  File \"/Users/christy/mambaforge/envs/ray/lib/python3.8/site-packages/pytorch_lightning/loops/batch/training_batch_loop.py\", line 201, in _run_optimization\n",
      "    self._optimizer_step(optimizer, opt_idx, batch_idx, closure)\n",
      "  File \"/Users/christy/mambaforge/envs/ray/lib/python3.8/site-packages/pytorch_lightning/loops/batch/training_batch_loop.py\", line 394, in _optimizer_step\n",
      "    model_ref.optimizer_step(\n",
      "  File \"/Users/christy/mambaforge/envs/ray/lib/python3.8/site-packages/pytorch_lightning/core/lightning.py\", line 1593, in optimizer_step\n",
      "    optimizer.step(closure=optimizer_closure)\n",
      "  File \"/Users/christy/mambaforge/envs/ray/lib/python3.8/site-packages/pytorch_lightning/core/optimizer.py\", line 209, in step\n",
      "    self.__optimizer_step(*args, closure=closure, profiler_name=profiler_name, **kwargs)\n",
      "  File \"/Users/christy/mambaforge/envs/ray/lib/python3.8/site-packages/pytorch_lightning/core/optimizer.py\", line 129, in __optimizer_step\n",
      "    trainer.accelerator.optimizer_step(optimizer, self._optimizer_idx, lambda_closure=closure, **kwargs)\n",
      "  File \"/Users/christy/mambaforge/envs/ray/lib/python3.8/site-packages/pytorch_lightning/accelerators/accelerator.py\", line 296, in optimizer_step\n",
      "    self.run_optimizer_step(optimizer, opt_idx, lambda_closure, **kwargs)\n",
      "  File \"/Users/christy/mambaforge/envs/ray/lib/python3.8/site-packages/pytorch_lightning/accelerators/accelerator.py\", line 303, in run_optimizer_step\n",
      "    self.training_type_plugin.optimizer_step(optimizer, lambda_closure=lambda_closure, **kwargs)\n",
      "  File \"/Users/christy/mambaforge/envs/ray/lib/python3.8/site-packages/pytorch_lightning/plugins/training_type/training_type_plugin.py\", line 226, in optimizer_step\n",
      "    optimizer.step(closure=lambda_closure, **kwargs)\n",
      "  File \"/Users/christy/mambaforge/envs/ray/lib/python3.8/site-packages/torch/optim/optimizer.py\", line 88, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/Users/christy/mambaforge/envs/ray/lib/python3.8/site-packages/pytorch_forecasting/optim.py\", line 195, in step\n",
      "    buffered = self.radam_buffer[int(state[\"step\"] % 10)]\n",
      "AttributeError: 'Ranger' object has no attribute 'radam_buffer'\n",
      "2021-12-13 18:25:23,374\tERROR worker.py:84 -- Unhandled error (suppress with RAY_IGNORE_UNHANDLED_ERRORS=1): \u001b[36mray::RayExecutor.execute()\u001b[39m (pid=41271, ip=127.0.0.1, repr=<ray_lightning.ray_ddp.RayExecutor object at 0x11760a3a0>)\n",
      "  File \"/Users/christy/mambaforge/envs/ray/lib/python3.8/site-packages/ray_lightning/ray_ddp.py\", line 54, in execute\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/Users/christy/mambaforge/envs/ray/lib/python3.8/site-packages/ray_lightning/ray_ddp.py\", line 297, in execute_remote\n",
      "    super(RayPlugin, self).new_process(\n",
      "  File \"/Users/christy/mambaforge/envs/ray/lib/python3.8/site-packages/pytorch_lightning/plugins/training_type/ddp_spawn.py\", line 201, in new_process\n",
      "    results = trainer.run_stage()\n",
      "  File \"/Users/christy/mambaforge/envs/ray/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 996, in run_stage\n",
      "    return self._run_train()\n",
      "  File \"/Users/christy/mambaforge/envs/ray/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py\", line 1045, in _run_train\n",
      "    self.fit_loop.run()\n",
      "  File \"/Users/christy/mambaforge/envs/ray/lib/python3.8/site-packages/pytorch_lightning/loops/base.py\", line 111, in run\n",
      "    self.advance(*args, **kwargs)\n",
      "  File \"/Users/christy/mambaforge/envs/ray/lib/python3.8/site-packages/pytorch_lightning/loops/fit_loop.py\", line 200, in advance\n",
      "    epoch_output = self.epoch_loop.run(train_dataloader)\n",
      "  File \"/Users/christy/mambaforge/envs/ray/lib/python3.8/site-packages/pytorch_lightning/loops/base.py\", line 111, in run\n",
      "    self.advance(*args, **kwargs)\n",
      "  File \"/Users/christy/mambaforge/envs/ray/lib/python3.8/site-packages/pytorch_lightning/loops/epoch/training_epoch_loop.py\", line 131, in advance\n",
      "    batch_output = self.batch_loop.run(batch, self.iteration_count, self._dataloader_idx)\n",
      "  File \"/Users/christy/mambaforge/envs/ray/lib/python3.8/site-packages/pytorch_lightning/loops/batch/training_batch_loop.py\", line 100, in run\n",
      "    super().run(batch, batch_idx, dataloader_idx)\n",
      "  File \"/Users/christy/mambaforge/envs/ray/lib/python3.8/site-packages/pytorch_lightning/loops/base.py\", line 111, in run\n",
      "    self.advance(*args, **kwargs)\n",
      "  File \"/Users/christy/mambaforge/envs/ray/lib/python3.8/site-packages/pytorch_lightning/loops/batch/training_batch_loop.py\", line 147, in advance\n",
      "    result = self._run_optimization(batch_idx, split_batch, opt_idx, optimizer)\n",
      "  File \"/Users/christy/mambaforge/envs/ray/lib/python3.8/site-packages/pytorch_lightning/loops/batch/training_batch_loop.py\", line 201, in _run_optimization\n",
      "    self._optimizer_step(optimizer, opt_idx, batch_idx, closure)\n",
      "  File \"/Users/christy/mambaforge/envs/ray/lib/python3.8/site-packages/pytorch_lightning/loops/batch/training_batch_loop.py\", line 394, in _optimizer_step\n",
      "    model_ref.optimizer_step(\n",
      "  File \"/Users/christy/mambaforge/envs/ray/lib/python3.8/site-packages/pytorch_lightning/core/lightning.py\", line 1593, in optimizer_step\n",
      "    optimizer.step(closure=optimizer_closure)\n",
      "  File \"/Users/christy/mambaforge/envs/ray/lib/python3.8/site-packages/pytorch_lightning/core/optimizer.py\", line 209, in step\n",
      "    self.__optimizer_step(*args, closure=closure, profiler_name=profiler_name, **kwargs)\n",
      "  File \"/Users/christy/mambaforge/envs/ray/lib/python3.8/site-packages/pytorch_lightning/core/optimizer.py\", line 129, in __optimizer_step\n",
      "    trainer.accelerator.optimizer_step(optimizer, self._optimizer_idx, lambda_closure=closure, **kwargs)\n",
      "  File \"/Users/christy/mambaforge/envs/ray/lib/python3.8/site-packages/pytorch_lightning/accelerators/accelerator.py\", line 296, in optimizer_step\n",
      "    self.run_optimizer_step(optimizer, opt_idx, lambda_closure, **kwargs)\n",
      "  File \"/Users/christy/mambaforge/envs/ray/lib/python3.8/site-packages/pytorch_lightning/accelerators/accelerator.py\", line 303, in run_optimizer_step\n",
      "    self.training_type_plugin.optimizer_step(optimizer, lambda_closure=lambda_closure, **kwargs)\n",
      "  File \"/Users/christy/mambaforge/envs/ray/lib/python3.8/site-packages/pytorch_lightning/plugins/training_type/training_type_plugin.py\", line 226, in optimizer_step\n",
      "    optimizer.step(closure=lambda_closure, **kwargs)\n",
      "  File \"/Users/christy/mambaforge/envs/ray/lib/python3.8/site-packages/torch/optim/optimizer.py\", line 88, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/Users/christy/mambaforge/envs/ray/lib/python3.8/site-packages/pytorch_forecasting/optim.py\", line 195, in step\n",
      "    buffered = self.radam_buffer[int(state[\"step\"] % 10)]\n",
      "AttributeError: 'Ranger' object has no attribute 'radam_buffer'\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# specify all the config parameters \n",
    "config = {\"forecast_horizon\": 168, \"context_length\": 63,\n",
    "          \"batch_size\": 128, \"epochs\": 2,\n",
    "          \"lr\": 0.05, \"hidden_size\": 16, \"hidden_layers\": 2,\n",
    "          \"attention_head_size\": 2, \"hidden_continuous_size\": 1} \n",
    "\n",
    "# Don't set ``gpus`` in the ``Trainer``.\n",
    "# The actual number of GPUs is determined by ``num_workers``.\n",
    "plugin = RayPlugin(num_workers=4, \n",
    "                   num_cpus_per_worker=1, \n",
    "                   use_gpu=False)\n",
    "validation_loader, trainer = train_func(config, plugin)\n",
    "\n",
    "\n",
    "print(type(validation_loader))\n",
    "print(type(trainer))\n",
    "print(type(plugin))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acde8f44-c621-495c-a235-1eb7caa35807",
   "metadata": {},
   "source": [
    "# Evaluate Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d312b2f1-30f5-4a18-8e16-3b470835cb3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## EVALUATE THE DL MODEL\n",
    "\n",
    "# load the best model according to the validation loss (given that\n",
    "# we use early stopping, this is not necessarily the last epoch)\n",
    "best_model_path = trainer.checkpoint_callback.best_model_path\n",
    "print(best_model_path)\n",
    "best_tft = ptf.models.TemporalFusionTransformer.load_from_checkpoint(best_model_path)\n",
    "\n",
    "# calcualte mean absolute error on validation set\n",
    "actuals = torch.cat([y[0] for x, y in iter(validation_loader)])\n",
    "predictions = best_tft.predict(validation_loader)\n",
    "\n",
    "# print MAE\n",
    "(actuals - predictions).abs().mean()\n",
    "\n",
    "#18.9355 with  \"attention_head_size\": 2, \"hidden_continuous_size\": 1} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f89819a-3f66-4122-a27a-258c894743e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize in tensorboard, hit ctrl-c when done\n",
    "!tensorboard --logdir=lightning_logs --load_fast=false"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "172ded39-8f0a-4392-b63e-55a1819fb051",
   "metadata": {},
   "source": [
    "# Plot actuals vs predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fd98f04-a2f7-43be-9a13-2421499194cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# raw predictions are a dictionary from which all kind of information including quantiles can be extracted\n",
    "raw_predictions, x = best_tft.predict(validation_loader, mode=\"raw\", return_x=True)\n",
    "\n",
    "for idx in range(5):  # plot 5 examples\n",
    "    best_tft.plot_prediction(x, raw_predictions, idx=idx, add_loss_to_title=True);"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
