{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8d27058a",
   "metadata": {},
   "source": [
    "# Demand forecasting using RNN with LSTM on PyTorch\n",
    "\n",
    "This notebook goes along with the tutorial <a href=\"https://towardsdatascience.com/how-to-train-time-series-forecasting-faster-using-ray-part-2-of-2-aacba89ca49a\">How to Train Time Series Forecasting Faster Using Ray, part 2 of 2</a>.\n",
    "\n",
    "<b>Suggestion: Make a copy of this notebook.  This way you will retain the original, executed notebook outputs.  Make edits in the copied notebook. </b>\n",
    "\n",
    "In this notebook, we will use the <a href=\"https://github.com/ray-project/ray_lightning\">Ray Lightning plugin</a> (which runs on top <a href=\"https://docs.ray.io\">Ray</a>) to speed up training and inference of Google's <a href=\"https://github.com/google-research/google-research/tree/master/tft\">TemporalFusionTransformer</a> algorithm for RNN with LSTM, which has been adapted by <a href=\"https://pytorch-forecasting.readthedocs.io\">PyTorch Forecasting</a>, which in turn is built on <a href=\"https://pytorch-lightning.readthedocs.io\">PyTorch Lightning</a>. PyTorch Lightning is a set of APIs to simplify PyTorch, similar to the relationship of Keras to TensorFlow.\n",
    "\n",
    "Ray can take any Python code and enable it to run distributed across multiple compute nodes.  The compute node cluster could be your own laptop cores or a cluster in any cloud.  Together with <a href=\"https://www.anyscale.com/\">Anyscale cluster management</a>  for cloud, this is how Ray can speed up AI training and inferencing.\n",
    "\n",
    "Demo data is NYC yellow taxi from: https://www1.nyc.gov/site/tlc/about/tlc-trip-record-data.page. Cleaned, aggregated hourly <b>sample data is provided with this demo in this repo: data/clean_taxi_hourly.parquet</b>\n",
    "\n",
    "Forecast goal:  Given 8 months historical taxi trips data for NYC, predict #pickups at each location, at an hourly granularity, for the next week.\n",
    "\n",
    "Suggestion: Make a copy of this notebook. Make edits and run in the copied notebook. This way you will retain the original, executed notebook outputs.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5c63b5d4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "###########\n",
    "# Install libraries\n",
    "###########\n",
    "\n",
    "# !pip install fastparquet #read parquet files\n",
    "# !pip install tblib  #Serialization library for Exceptions and Tracebacks\n",
    "# !conda install -y grpcio #only if on Apple M1 chip, since pip can't find otherwise\n",
    "# !pip install ray     #install ray for the first time\n",
    "# !pip install -U ray  #update ray to latest v1.9\n",
    "# !pip install pytorch_lightning==1.4 #PyTorch Lightning restricted to older version for Ray\n",
    "# !conda install -y pytorch torchvision -c pytorch  #PyTorch\n",
    "# !pip install ray_lightning  #PyTorch Lightning plugin for Ray\n",
    "# !pip install anyscale  #run Ray code on any cloud\n",
    "# !conda install -y gputil  #GPU status from NVIDA GPUs\n",
    "\n",
    "# Special instructions to install pytorch_forecasting\n",
    "# !pip install poetry-core==1.0.7\n",
    "# !pip install poetry-dynamic-versioning==0.13.1\n",
    "# Experimental use my copy to get rid of warning messages about numpy\n",
    "# !pip install git+https://github.com/christy/my-copy-pytorch-forecasting@main\n",
    "# Official version works but gives extra warning messages\n",
    "# !pip install git+https://github.com/jdb78/pytorch-forecasting@maintenance/pip-install\n",
    "\n",
    "# Extra install for Tensorboard to work with PyTorch\n",
    "# conda install -y tensorflow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4f2447c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 415\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.8.12\n",
      "pytorch: 1.10.1\n",
      "pytorch_lightning: 1.4.0\n",
      "pytorch_forecasting: 0.0.0\n",
      "ray: 1.9.2\n",
      "gputil: 1.4.0\n"
     ]
    }
   ],
   "source": [
    "###########\n",
    "# Import libraries\n",
    "###########\n",
    "\n",
    "# Basic Python\n",
    "import sys # Python sys functions\n",
    "import os  # Python os functions\n",
    "import time # Python time functions\n",
    "import typing #Python types\n",
    "import logging # Python logging functions\n",
    "import warnings # Python warnings\n",
    "warnings.filterwarnings(\"ignore\")  # don't show warnings\n",
    "import fastparquet  # Engine for parquet support\n",
    "import GPUtil #GPU status from NVIDA GPUs\n",
    "\n",
    "# Open-source libraries:\n",
    "import numpy as np # Numerical processing\n",
    "import pandas as pd  # Dataframe (tabular data) processing\n",
    "# import matplotlib # Graph plotting\n",
    "from matplotlib import pyplot as plt # Graph plotting\n",
    "# stop warnings from pyotorch_forecasting too many open plots\n",
    "plt.rcParams.update({'figure.max_open_warning': 0})\n",
    "import ray                # Run distributed code\n",
    "from ray_lightning import RayPlugin #Ray plugin to parallelize Pytorch Lightning\n",
    "# from ray.train import Trainer # Ray library for other AI libraries\n",
    "\n",
    "# PyTorch, PyTorch Lightning, and PyTorch Forecasting\n",
    "import torch  #Pytorch\n",
    "import pytorch_lightning as pl  #PyTorch Lightning convenience APIs for PyTorch\n",
    "import pytorch_forecasting as ptf #PyTorch Forecasting convenience APIs for PyTorch Lightning\n",
    "pl.seed_everything(415)  # Set global random seed\n",
    "\n",
    "# PyTorch visualization uses Tensorboard\n",
    "import tensorflow as tf #Tensorflow\n",
    "import tensorboard as tb  #Tensorboard\n",
    "tf.io.gfile = tb.compat.tensorflow_stub.io.gfile  #compatibility for PyTorch\n",
    "\n",
    "# TODO remove this\n",
    "# Override pytorch_forecasting with my copy\n",
    "sys.path.insert( 0, os.path.abspath(\"../githubPublicPytorchForecasting/my-copy-pytorch-forecasting\") )\n",
    "import pytorch_forecasting as ptf \n",
    "\n",
    "\n",
    "!python --version\n",
    "print(f\"pytorch: {torch.__version__}\")\n",
    "print(f\"pytorch_lightning: {pl.__version__}\")\n",
    "print(f\"pytorch_forecasting: {ptf.__version__}\")\n",
    "print(f\"ray: {ray.__version__}\")\n",
    "print(f\"gputil: {GPUtil.__version__}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6a1e0cb-6890-476a-88a2-98b6b43fee74",
   "metadata": {},
   "source": [
    "# Instructions\n",
    "\n",
    "In the cell below, set the variables to choose between:\n",
    "\n",
    "<ul>\n",
    "    <li><b>REGULAR_PYTHON = True:</b> Do not do any Ray parallelization of your Python code.  RAY_SMOKE_TEST will be ignored.</li>\n",
    "    <li><b>REGULAR_PYTHON = False:</b> Use Ray engine to parallelize your code.  The Ray engine will run locally or in a cloud based on RAY_SMOKE_TEST values.</li>\n",
    "    <ul>\n",
    "        <li><b>RAY_SMOKE_TEST = True:</b>  Ray engine will run locally.</li>\n",
    "        <li><b>RAY_SMOKE_TEST = False:</b>  Ray engine will run in a cloud using Anyscale.</li>\n",
    "        <ul>\n",
    "            <li>CLUSTER_NAME = <b>\"your_name\"</b>. Give your cloud cluster a name.</li>\n",
    "            <li>CLUSTER_CONFIG = <b>\"your_config\"</b>.  Your pre-configured cluster config name, if you have one defined on console.anyscale.com</li>\n",
    "            <li>my_env={\"working_dir\": \".\"}.  Leave this command as-is.  This means upload data from current directory. </li>\n",
    "        </ul>\n",
    "    </ul>\n",
    "        </ul>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "109a0b3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are running Ray Local\n"
     ]
    }
   ],
   "source": [
    "# Set these variable to True or False\n",
    "\n",
    "# Regular python = True means no parallelization using Ray\n",
    "REGULAR_PYTHON = False\n",
    "\n",
    "# Smoke test = True means run Ray on your laptop\n",
    "# Smoke test = False means run Ray on a cloud\n",
    "RAY_SMOKE_TEST = True\n",
    "if RAY_SMOKE_TEST:\n",
    "    if REGULAR_PYTHON:\n",
    "        # Running regular Python\n",
    "        print(\"You are running regular Python, not running Ray.\")\n",
    "    else: \n",
    "        # Run Ray locally on your laptop\n",
    "        print(\"You are running Ray Local\")\n",
    "else:\n",
    "    # Run Ray on any cloud using Anyscale\n",
    "    print(\"You are running Ray on a Cloud using Anyscale\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fb1da60e-e78e-41bb-9b66-33539cc4e777",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are running Ray Local\n",
      "Found available CPU: 8\n",
      "Found available GPU: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-18 07:13:24,730\tINFO services.py:1338 -- View the Ray dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8265\u001b[39m\u001b[22m\n"
     ]
    }
   ],
   "source": [
    "# Anyscale cluster_name and cluster_config\n",
    "CLUSTER_NAME = \"christy-pytorch95\"\n",
    "CLUSTER_CONFIG = \"christy-forecast-pytorch:14\"\n",
    "my_env={\"working_dir\": \".\"}\n",
    "\n",
    "###########\n",
    "# Initialize Ray using above variable choices\n",
    "# Default location for ray dashboard: http://127.0.0.1:8265\n",
    "# More details:  https://docs.ray.io/en/latest/ray-dashboard.html\n",
    "###########\n",
    "if ray.is_initialized():\n",
    "    ray.shutdown()\n",
    "\n",
    "if not REGULAR_PYTHON:\n",
    "\n",
    "    if RAY_SMOKE_TEST:\n",
    "        # Run Ray locally on your laptop\n",
    "        print(\"You are running Ray Local\")\n",
    "        \n",
    "        # Detect number of local CPU, used by default in ray.init()\n",
    "        AVAILABLE_LOCAL_CPU = os.cpu_count()\n",
    "        print(f\"Found available CPU: {AVAILABLE_LOCAL_CPU}\")\n",
    "        \n",
    "        # Set num_workers for the plugin to be the same\n",
    "        NUM_TRAINING_WORKERS = AVAILABLE_LOCAL_CPU\n",
    "        \n",
    "        # Detect number of local GPU, used by default in ray.init()\n",
    "        # These GPU will be used when ray schedules a remote function\n",
    "        AVAILABLE_GPU = len(GPUtil.getGPUs())\n",
    "        print(f\"Found available GPU: {AVAILABLE_GPU}\")\n",
    "\n",
    "        # start up ray local\n",
    "        ray.init()\n",
    "\n",
    "    else:\n",
    "        # Run Ray on any cloud using Anyscale\n",
    "        print(\"You are running Ray on a Cloud with Anyscale\")\n",
    "\n",
    "        import anyscale\n",
    "\n",
    "        # ray.init(Anyscale_cluster, cluster_config)\n",
    "        ray.init(\n",
    "            f\"anyscale://{CLUSTER_NAME}\",\n",
    "            cluster_env=CLUSTER_CONFIG,\n",
    "            # runtime_env=my_env,  #data pre-loaded on cluster env from github\n",
    "            \n",
    "            # quiet logging settings especially needed for Prophet\n",
    "            # log_to_driver=False,\n",
    "            # configure_logging=True,\n",
    "            # logging_level=logging.ERROR,\n",
    "        )\n",
    "        \n",
    "else:\n",
    "    AVAILABLE_LOCAL_CPU = 1\n",
    "    print(f\"You are running regular Python, not running Ray.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "85a4bb5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define functions\n",
    "# Todo: Move functions inside util.py\n",
    "\n",
    "# Convert data from pandas to PyTorch tensors.\n",
    "def convert_pandas_pytorch_timeseriesdata(\n",
    "    input_data_pandas_df:pd.DataFrame, \n",
    "    config:dict\n",
    ") -> typing.Union['pytorch_forecasting.data.timeseries.TimeSeriesDataSet',\n",
    "                  'torch.utils.data.dataloader.DataLoader']:\n",
    "\n",
    "    \"\"\"Converts pandas dataframe into TimeSeries folded tensors following \n",
    "       the backtesting technique.  A generator for doing the folding is \n",
    "       per batch also created.  One for the training data.  \n",
    "       Another for the validation data.  \n",
    "\n",
    "    Inputs:\n",
    "        pd.DataFrame: All the input data\n",
    "        dict: config is a configuration file containing hard-coded settings.\n",
    "\n",
    "    Returns:\n",
    "        'pytorch_forecasting.data.timeseries.TimeSeriesDataSet': training data\n",
    "        'torch.utils.data.dataloader.DataLoader': training data loader\n",
    "        'torch.utils.data.dataloader.DataLoader': validation data loader\n",
    "    \"\"\"\n",
    "    \n",
    "    # specify data parameters\n",
    "    FORECAST_HORIZON = config.get(\"forecast_horizon\", 168)\n",
    "    CONTEXT_LENGTH = config.get(\"context_length\", 63)\n",
    "    BATCH_SIZE = config.get(\"batch_size\", 32)\n",
    "    NUM_TRAINING_WORKERS = config.get(\"num_training_workers\", 4)\n",
    "    id_col_name = \"pulocationid\"\n",
    "    target_value = \"trip_quantity\"\n",
    "    \n",
    "    the_df = input_data_pandas_df.copy()\n",
    "    \n",
    "    # define forecast horizon and training cutoff\n",
    "    max_prediction_length = FORECAST_HORIZON  #decoder length = 1 week forecast horizon\n",
    "    max_encoder_length = CONTEXT_LENGTH  # window or context length\n",
    "    training_cutoff = the_df[\"time_idx\"].max() - max_prediction_length \n",
    "\n",
    "    # convert pandas to PyTorch tensor\n",
    "    training_data = ptf.data.TimeSeriesDataSet(\n",
    "        the_df[lambda x: x.time_idx <= training_cutoff],\n",
    "        allow_missing_timesteps=True,\n",
    "        time_idx=\"time_idx\",\n",
    "        target=target_value,\n",
    "        group_ids=[id_col_name],\n",
    "        min_encoder_length=5,  # min 5 historical values must exist\n",
    "        max_encoder_length=max_encoder_length,\n",
    "        min_prediction_length=1,\n",
    "        max_prediction_length=max_prediction_length,\n",
    "        static_categoricals=[id_col_name],\n",
    "        # static_reals=[\"avg_population_2017\", \"avg_yearly_household_income_2017\"],\n",
    "        static_reals=[],\n",
    "        time_varying_known_categoricals=[\"day_hour\"],\n",
    "        # group of categorical variables can be treated as one variable\n",
    "        # variable_groups={\"special_days\": special_days},  \n",
    "        time_varying_known_reals=[\"time_idx\", ],\n",
    "                            # \"mean_item_loc_weekday\",\n",
    "                            # \"binned_max_item\"],\n",
    "        time_varying_unknown_categoricals=[],\n",
    "        time_varying_unknown_reals=[target_value,],\n",
    "\n",
    "        # https://pytorch-forecasting.readthedocs.io/en/v0.2.4/_modules/pytorch_forecasting/data.html\n",
    "        target_normalizer=ptf.data.GroupNormalizer(\n",
    "            groups=[\"pulocationid\"], \n",
    "            transformation=\"softplus\"  #forces positive values\n",
    "        ), \n",
    "        add_relative_time_idx=True, # add as feature\n",
    "        add_target_scales=True, # add avg target_value as feature\n",
    "        add_encoder_length=True, # add as feature\n",
    "    )\n",
    "    \n",
    "    # create PyTorch dataloader for training\n",
    "    train_loader = training_data\\\n",
    "                        .to_dataloader(\n",
    "                            train=True, \n",
    "                            batch_size=BATCH_SIZE, \n",
    "                            num_workers=NUM_TRAINING_WORKERS)\n",
    "    \n",
    "    # create validation PyTorch data \n",
    "    # (predict=True) means make do inference using the validation data\n",
    "    val_dataset = ptf.data.TimeSeriesDataSet\\\n",
    "                    .from_dataset(\n",
    "                        training_data, \n",
    "                        data=the_df, \n",
    "                        predict=True, \n",
    "                        stop_randomization=True)\n",
    "\n",
    "    # create PyTorch dataloaders for inference on validation data\n",
    "    validation_loader = val_dataset\\\n",
    "                    .to_dataloader(\n",
    "                        train=False, \n",
    "                        batch_size=BATCH_SIZE * 10, \n",
    "                        num_workers=NUM_TRAINING_WORKERS)\n",
    "    \n",
    "    # return original df converted to PyTorch tensors, and pytorch loaders\n",
    "    return training_data, train_loader, validation_loader\n",
    "\n",
    "# Define a PyTorch Lightning TemporalFusionTransformer model\n",
    "def define_pytorch_model(\n",
    "    train_dataset: 'pytorch_forecasting.data.timeseries.TimeSeriesDataSet', \n",
    "    config: dict, \n",
    "    ray_plugin: 'ray_lightning.ray_ddp.RayPlugin'\n",
    ") -> typing.Union['pytorch_forecasting.models.temporal_fusion_transformer.TemporalFusionTransformer',\n",
    "                  'pytorch_lightning.trainer.trainer.Trainer']:\n",
    "\n",
    "    \"\"\"Define a PyTorch Lightning TemporalFusionTransformer model and a \n",
    "       PyTorch Lightning trainer.  Initial values for the model are hard-\n",
    "       coded in the config dictionary.\n",
    "\n",
    "    Returns:\n",
    "        'pytorch_forecasting.models.temporal_fusion_transformer.TemporalFusionTransformer': model\n",
    "        'pytorch_lightning.trainer.trainer.Trainer': trainer for fitting the model\n",
    "    \"\"\"  \n",
    "    # get the parameters from config\n",
    "    NUM_GPU = config.get(\"num_gpus\", 0)\n",
    "    EPOCHS = config.get(\"epochs\", 30)\n",
    "    LR = config.get(\"lr\", 0.01)\n",
    "    HIDDEN_SIZE = config.get(\"hidden_size\", 40)\n",
    "    ATTENTION_HEAD_SIZE = config.get(\"attention_head_size\", 4)\n",
    "    HIDDEN_CONTINUOUS_SIZE = config.get(\"hidden_continuous_size\", 1)\n",
    "    DROPOUT = config.get(\"droupout\", 0.1)\n",
    "    LIMIT_TRAIN_BATCHES = config.get(\"limit_train_batches\", 30)\n",
    "    FAST_MODE = config.get(\"fast_mode\", False)\n",
    "    TUNING_RUN = config.get(\"tuning_run\", False)\n",
    "    \n",
    "    print(f\"learning_rate = {LR}\")\n",
    "    print(f\"hidden_size = {HIDDEN_SIZE}\")\n",
    "    print(f\"attention_head_size = {ATTENTION_HEAD_SIZE}\")\n",
    "    print(f\"hidden_continuous_size = {HIDDEN_CONTINUOUS_SIZE}\")\n",
    "    print(f\"limit_train_batches = {LIMIT_TRAIN_BATCHES}\")\n",
    "    \n",
    "    if ray_plugin is None:\n",
    "        PLUGINS = []\n",
    "    else:\n",
    "        PLUGINS=[ray_plugin]\n",
    "\n",
    "    # configure early stopping when validation loss does not improve \n",
    "    early_stop_callback = \\\n",
    "        pl.callbacks.EarlyStopping(\n",
    "            monitor=\"val_loss\", \n",
    "            min_delta=1e-4, \n",
    "            patience=10,   #1\n",
    "            verbose=False, \n",
    "            mode=\"min\")\n",
    "    \n",
    "    # Create the Tune Reporting Callback\n",
    "    metrics = {\"loss\": \"ptf.metrics.QuantileLoss()\"}\n",
    "    tune_callback = \\\n",
    "        ray.tune.integration.pytorch_lightning.TuneReportCallback(\n",
    "            metrics, \n",
    "            on=\"validation_end\")\n",
    "    \n",
    "    # configure logging\n",
    "    lr_logger = pl.callbacks.LearningRateMonitor(logging_interval='epoch')\n",
    "    logger = pl.loggers.TensorBoardLogger(\"lightning_logs\")  # log results to a tensorboard\n",
    "        \n",
    "    # Define callbacks based on passed parameter to run tuning or not\n",
    "    if TUNING_RUN:\n",
    "        CALLBACKS = [lr_logger, early_stop_callback, tune_callback]\n",
    "    else:\n",
    "        CALLBACKS = [lr_logger, early_stop_callback]\n",
    "        \n",
    "    # configure PyTorch trainer with Ray Lightning plugin\n",
    "    torch_trainer = pl.Trainer(\n",
    "        max_epochs=EPOCHS,\n",
    "        gpus=NUM_GPU,\n",
    "        # weights_summary=\"top\",\n",
    "        \n",
    "        # The value at which to clip gradients. \n",
    "        # Passing gradient_clip_val=None disables gradient clipping. \n",
    "        gradient_clip_val=0.1, \n",
    "        \n",
    "        # Number of batches or percent size of training data each epoch\n",
    "        # limit_train_batches=30,  #use 30 batches of training data each epoch \n",
    "        limit_train_batches=LIMIT_TRAIN_BATCHES,  \n",
    "        \n",
    "        # how often to log, default=50\n",
    "        logger=logger,\n",
    "        # log_every_n_steps=500,  #default 50\n",
    "        \n",
    "        # sanity check runs n batches of val before starting the training, default=2\n",
    "        # num_sanity_val_steps=1,\n",
    "        \n",
    "        # Callbacks run sequentially in the order defined here.  Except ModelCheckpoint callback\n",
    "        # runs after all others to ensure all states are saved to the checkpoints.\n",
    "        callbacks=CALLBACKS,\n",
    "        \n",
    "        # Run \"fast\" mode for quick sanity check\n",
    "        # Note: No trainer checkpoints will be saved in fast mode\n",
    "        fast_dev_run=FAST_MODE,\n",
    "        \n",
    "        # This is the Ray parallelizing distributed part\n",
    "        # regular python - just comment out below line\n",
    "        plugins = PLUGINS\n",
    "    )\n",
    "    if not FAST_MODE:\n",
    "        print(f\"checkpoints location: {torch_trainer.logger.log_dir}\")\n",
    "    \n",
    "\n",
    "    # initialize the model\n",
    "    tft = ptf.models.TemporalFusionTransformer.from_dataset(\n",
    "        train_dataset,\n",
    "        learning_rate=LR,\n",
    "        hidden_size=HIDDEN_SIZE, # num neurons in each layer, bigger runs more slowly\n",
    "        # lstm_layers=HIDDEN_LAYERS, #LSTM layers=1 #default=1 for tft architecture\n",
    "        attention_head_size=ATTENTION_HEAD_SIZE,  #default 4 cells in LSTM layer\n",
    "        dropout=DROPOUT,\n",
    "        hidden_continuous_size=HIDDEN_CONTINUOUS_SIZE,  #similar to categorical embedding size\n",
    "        # 7 quantiles by default: [0.02, 0.1, 0.25, 0.5, 0.75, 0.9, 0.98]\n",
    "        # output_size=7,  \n",
    "        # optimizer loss metric\n",
    "        loss=ptf.metrics.QuantileLoss(),\n",
    "        # uncomment for learning rate finder and otherwise, e.g. to 10 for logging every 10 batches\n",
    "        log_interval=50,  #50\n",
    "        reduce_on_plateau_patience=4, # reduce learning automatically\n",
    "    )\n",
    "    print(f\"Number of parameters in network: {tft.size()/1e3:.1f}k\")\n",
    "    \n",
    "    # return the model and trainer\n",
    "    return tft, torch_trainer\n",
    "\n",
    "# Define a calling function to read data, define model, train it\n",
    "def train_func(config: dict, \n",
    "               ray_plugin: 'ray_lightning.ray_ddp.RayPlugin'\n",
    ") -> typing.Union['torch.utils.data.dataloader.DataLoader',\n",
    "        'pytorch_lightning.trainer.trainer.Trainer',\n",
    "        'pytorch_forecasting.models.temporal_fusion_transformer.TemporalFusionTransformer',\n",
    "         str]:\n",
    "    \"\"\"Define a calling function to read data, define a model and train it.\n",
    "\n",
    "    Inputs:\n",
    "        dict: configuration dictionary with hard-coded runtime values\n",
    "        'ray_lightning.ray_ddp.RayPlugin': plugin for PyTorch Lightning trainer\n",
    "\n",
    "    Returns:\n",
    "        'torch.utils.data.dataloader.DataLoader': validation data loader\n",
    "        'pytorch_lightning.trainer.trainer.Trainer': trainer for fitting the model\n",
    "        'pytorch_forecasting.models.temporal_fusion_transformer.TemporalFusionTransformer': trained model\n",
    "        str: path where Pytorch Forecasting model is stored\n",
    "    \"\"\"\n",
    "    # # stop warnings from pyotorch_forecasting too many open plots\n",
    "    # matplotlib.rcParams.update({'figure.max_open_warning': 0})\n",
    "    \n",
    "    # read data into pandas dataframe\n",
    "    filename = \"data/clean_taxi_hourly.parquet\"\n",
    "    df = pd.read_parquet(filename)\n",
    "    df = df[[\"time_idx\", \"pulocationid\", \"day_hour\",\n",
    "                 \"trip_quantity\", \"mean_item_loc_weekday\",\n",
    "                 \"binned_max_item\"]].copy()\n",
    "\n",
    "    # convert data from pandas to PyTorch tensors\n",
    "    train_dataset, train_loader, validation_loader = \\\n",
    "        convert_pandas_pytorch_timeseriesdata(df, config)\n",
    "\n",
    "    # define a PyTorch deep learning forecasting model\n",
    "    model, trainer = define_pytorch_model(\n",
    "                                           train_dataset, \n",
    "                                           config,\n",
    "                                           ray_plugin)\n",
    "    \n",
    "    # With Ray Core API, here you would issue ray.put(model) and ray.put(data).\n",
    "    # Since we are using Ray Train Library, the model and data parallelism are \n",
    "    # managed automatically behind the scenes for us.\n",
    "    # model_ref = ray.put(model)\n",
    "    # train_loader_ref = ray.put(train_loader)\n",
    "    # validation_loader_ref = ray.put(validation_loader)\n",
    "\n",
    "    # now train the model\n",
    "    trainer.fit(\n",
    "        model,\n",
    "        train_dataloaders=train_loader,\n",
    "        val_dataloaders=validation_loader,\n",
    "    )\n",
    "    \n",
    "    # # close all plots\n",
    "    # plt.close('all')\n",
    "    \n",
    "    # now use the model and check metrics on validation data\n",
    "    # load the best model according to the validation loss (given that\n",
    "    # we use early stopping, this is not necessarily the last epoch)\n",
    "    best_model_path = trainer.checkpoint_callback.best_model_path\n",
    "    print(best_model_path)\n",
    "    \n",
    "    # handle \"fast mode\"\n",
    "    FAST_MODE = config.get(\"fast_mode\", False)\n",
    "    if FAST_MODE:\n",
    "        return validation_loader, trainer, best_model_path, best_model_path\n",
    "    else:\n",
    "        best_tft = ptf.models.TemporalFusionTransformer.load_from_checkpoint(best_model_path)\n",
    "    \n",
    "    # return PyTorch DataLoader, Lightning Trainer, best model and path\n",
    "    return validation_loader, trainer, best_tft, best_model_path\n",
    "\n",
    "\n",
    "def calc_wql(y_actual:'torch.Tensor', \n",
    "             y_quantiles:'torch.Tensor', \n",
    "             quantile_list:list)->'torch.Tensor':\n",
    "    \"\"\"Calculate weighted quantile loss given actuals, quantile predictions,\n",
    "       and list of desired quantiles to average over.\n",
    "    Inputs:\n",
    "        'torch.Tensor': y_actual is a tensor of actual values \n",
    "        'torch.Tensor': y_quantiles is a tensor of quantile predictions\n",
    "        'list': List of quantiles to average over\n",
    "\n",
    "    Returns:\n",
    "        'torch.Tensor': weighted quantile loss over all the desired quantiles\n",
    "    \"\"\"\n",
    "\n",
    "    assert not y_actual.requires_grad\n",
    "    assert y_quantiles.size(0) == y_actual.size(0)\n",
    "    \n",
    "    all_losses = []\n",
    "    for i, q in enumerate(quantile_list):\n",
    "        sum_actuals = torch.sum(torch.abs(y_actual[i]))\n",
    "        errors = torch.abs(y_actual[i] - y_quantiles[i][:, i])\n",
    "        all_losses.append(\n",
    "            torch.where(y_quantiles[i][:, i] > y_actual[i],\n",
    "                        (1-q) * errors, \n",
    "                        q * errors ).unsqueeze(1))\n",
    "        \n",
    "        if torch.is_nonzero(sum_actuals):\n",
    "            all_losses[i] = torch.sum(all_losses[i]).div(sum_actuals)\n",
    "        else:\n",
    "            all_losses[i] = torch.empty_like(all_losses[i])\n",
    "    \n",
    "    WQL = torch.mean(torch.stack(all_losses), dim=0)\n",
    "    return WQL\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b8808a1",
   "metadata": {},
   "source": [
    "# Create and train a baseline model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4dce987e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data type: <class 'pandas.core.frame.DataFrame'>\n",
      "Converted data type: <class 'pytorch_forecasting.data.timeseries.TimeSeriesDataSet'>\n",
      "baseline model MAE: 29.246337890625\n",
      "CPU times: user 8.91 s, sys: 819 ms, total: 9.73 s\n",
      "Wall time: 13.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# specify config parameters for baseline model\n",
    "config = {\"forecast_horizon\": 168, \"context_length\": 63,\n",
    "          \"num_gpus\":0, \"batch_size\": 128, \n",
    "          \"num_training_workers\":4,\n",
    "         }\n",
    "\n",
    "# read data into pandas dataframe\n",
    "filename = \"data/clean_taxi_hourly.parquet\"\n",
    "df = pd.read_parquet(filename)\n",
    "df = df[\n",
    "    [\n",
    "        \"time_idx\",\n",
    "        \"pulocationid\",\n",
    "        \"day_hour\",\n",
    "        \"trip_quantity\",\n",
    "        \"mean_item_loc_weekday\",\n",
    "        \"binned_max_item\",\n",
    "    ]\n",
    "].copy()\n",
    "\n",
    "# convert data from pandas to PyTorch tensors\n",
    "print(f\"Input data type: {type(df)}\")\n",
    "train_dataset, train_loader, validation_loader = convert_pandas_pytorch_timeseriesdata(\n",
    "    df, config\n",
    ")\n",
    "print(f\"Converted data type: {type(train_dataset)}\")\n",
    "\n",
    "# calculate baseline mean absolute error, i.e. predict next value as the last available value from the history\n",
    "actuals = torch.cat([y for x, (y, weight) in iter(validation_loader)])\n",
    "baseline_predictions = ptf.models.Baseline().predict(validation_loader)\n",
    "\n",
    "\n",
    "## EVALUATE THE BASELINE MODEL\n",
    "# print MAE\n",
    "print(f\"baseline model MAE: {(actuals - baseline_predictions).abs().mean()}\")\n",
    "\n",
    "# 29.2463\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84598ad7",
   "metadata": {},
   "source": [
    "# Train a PyTorch Lightning DL Forecast Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8bceac0d-1fe3-479c-b9e4-a764daed4084",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are running Ray Local\n"
     ]
    }
   ],
   "source": [
    "# To set up RayTrain PyTorch Lightning Plugin, you need to know how many \n",
    "# CPU and GPU you have.\n",
    "# If running on a cloud using Anyscale,\n",
    "#    1. check cluster size\n",
    "#       https://console.anyscale.com/o/anyscale-internal/clusters\n",
    "#    2. verify size of head node\n",
    "#       click link next to 'Cluster compute config'\n",
    "#    3. look up how many cpu in this size instance in your cloud (e.g. aws)\n",
    "#       https://aws.amazon.com/ec2/instance-types/\n",
    "#    4. set num_workers=N, where N > num cpu on that instance type\n",
    "#       RayPlugin(num_workers=10, ...)\n",
    "NUM_TRAINING_WORKERS = 8\n",
    "USE_GPU = True\n",
    "\n",
    "if not REGULAR_PYTHON:\n",
    "\n",
    "    if RAY_SMOKE_TEST:\n",
    "        # Run Ray locally on your laptop\n",
    "        print(\"You are running Ray Local\")\n",
    "\n",
    "        # RayPlugin(fixed num training workers)\n",
    "        NUM_TRAINING_WORKERS = AVAILABLE_LOCAL_CPU\n",
    "        if AVAILABLE_GPU == 0:\n",
    "            USE_GPU = False\n",
    "            \n",
    "        plugin = RayPlugin(\n",
    "            num_workers=AVAILABLE_LOCAL_CPU,\n",
    "            num_cpus_per_worker=1,\n",
    "            use_gpu=USE_GPU,\n",
    "            # add this to skip warnings\n",
    "            # https://github.com/PyTorchLightning/pytorch-lightning/discussions/6761\n",
    "            find_unused_parameters=False,\n",
    "        )\n",
    "\n",
    "    else:\n",
    "        # Run Ray on any cloud using Anyscale\n",
    "        print(\"You are running Ray on a Cloud with Anyscale\")\n",
    "\n",
    "        # To enable Ray Train plugin to use Anyscale autoscaling, \n",
    "        # set num_cpu greater than #cpu on head node of cluster.\n",
    "        # Example for AWS m5.2xlarge: RayPlugin(num_workers = 10)\n",
    "        plugin = RayPlugin(\n",
    "            num_workers=NUM_TRAINING_WORKERS,\n",
    "            num_cpus_per_worker=1,\n",
    "            use_gpu=USE_GPU,\n",
    "            # skip warnings, https://github.com/PyTorchLightning/pytorch-lightning/discussions/6761\n",
    "            find_unused_parameters=False,\n",
    "        )\n",
    "\n",
    "else:\n",
    "    # Not running Ray\n",
    "    plugin = None\n",
    "    print(\"You are running regular Python, not running Ray.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e279f51b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learning_rate = 0.05\n",
      "hidden_size = 20\n",
      "attention_head_size = 8\n",
      "hidden_continuous_size = 4\n",
      "limit_train_batches = 0.25\n",
      "checkpoints location: lightning_logs/default/version_16\n",
      "Number of parameters in network: 33.2k\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(RayExecutor pid=2930)\u001b[0m Global seed set to 415\n",
      "\u001b[2m\u001b[36m(RayExecutor pid=2930)\u001b[0m initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/8\n",
      "\u001b[2m\u001b[36m(RayExecutor pid=2931)\u001b[0m Global seed set to 415\n",
      "\u001b[2m\u001b[36m(RayExecutor pid=2931)\u001b[0m initializing ddp: GLOBAL_RANK: 6, MEMBER: 7/8\n",
      "\u001b[2m\u001b[36m(RayExecutor pid=2936)\u001b[0m Global seed set to 415\n",
      "\u001b[2m\u001b[36m(RayExecutor pid=2936)\u001b[0m initializing ddp: GLOBAL_RANK: 3, MEMBER: 4/8\n",
      "\u001b[2m\u001b[36m(RayExecutor pid=2937)\u001b[0m Global seed set to 415\n",
      "\u001b[2m\u001b[36m(RayExecutor pid=2937)\u001b[0m initializing ddp: GLOBAL_RANK: 1, MEMBER: 2/8\n",
      "\u001b[2m\u001b[36m(RayExecutor pid=2932)\u001b[0m Global seed set to 415\n",
      "\u001b[2m\u001b[36m(RayExecutor pid=2932)\u001b[0m initializing ddp: GLOBAL_RANK: 4, MEMBER: 5/8\n",
      "\u001b[2m\u001b[36m(RayExecutor pid=2935)\u001b[0m Global seed set to 415\n",
      "\u001b[2m\u001b[36m(RayExecutor pid=2935)\u001b[0m initializing ddp: GLOBAL_RANK: 5, MEMBER: 6/8\n",
      "\u001b[2m\u001b[36m(RayExecutor pid=2933)\u001b[0m Global seed set to 415\n",
      "\u001b[2m\u001b[36m(RayExecutor pid=2933)\u001b[0m initializing ddp: GLOBAL_RANK: 7, MEMBER: 8/8\n",
      "\u001b[2m\u001b[36m(RayExecutor pid=2934)\u001b[0m Global seed set to 415\n",
      "\u001b[2m\u001b[36m(RayExecutor pid=2934)\u001b[0m initializing ddp: GLOBAL_RANK: 2, MEMBER: 3/8\n",
      "\u001b[2m\u001b[36m(RayExecutor pid=2930)\u001b[0m \n",
      "\u001b[2m\u001b[36m(RayExecutor pid=2930)\u001b[0m    | Name                               | Type                            | Params\n",
      "\u001b[2m\u001b[36m(RayExecutor pid=2930)\u001b[0m ----------------------------------------------------------------------------------------\n",
      "\u001b[2m\u001b[36m(RayExecutor pid=2930)\u001b[0m 0  | loss                               | QuantileLoss                    | 0     \n",
      "\u001b[2m\u001b[36m(RayExecutor pid=2930)\u001b[0m 1  | logging_metrics                    | ModuleList                      | 0     \n",
      "\u001b[2m\u001b[36m(RayExecutor pid=2930)\u001b[0m 2  | input_embeddings                   | MultiEmbedding                  | 8.6 K \n",
      "\u001b[2m\u001b[36m(RayExecutor pid=2930)\u001b[0m 3  | prescalers                         | ModuleDict                      | 48    \n",
      "\u001b[2m\u001b[36m(RayExecutor pid=2930)\u001b[0m 4  | static_variable_selection          | VariableSelectionNetwork        | 1.3 K \n",
      "\u001b[2m\u001b[36m(RayExecutor pid=2930)\u001b[0m 5  | encoder_variable_selection         | VariableSelectionNetwork        | 1.4 K \n",
      "\u001b[2m\u001b[36m(RayExecutor pid=2930)\u001b[0m 6  | decoder_variable_selection         | VariableSelectionNetwork        | 954   \n",
      "\u001b[2m\u001b[36m(RayExecutor pid=2930)\u001b[0m 7  | static_context_variable_selection  | GatedResidualNetwork            | 1.7 K \n",
      "\u001b[2m\u001b[36m(RayExecutor pid=2930)\u001b[0m 8  | static_context_initial_hidden_lstm | GatedResidualNetwork            | 1.7 K \n",
      "\u001b[2m\u001b[36m(RayExecutor pid=2930)\u001b[0m 9  | static_context_initial_cell_lstm   | GatedResidualNetwork            | 1.7 K \n",
      "\u001b[2m\u001b[36m(RayExecutor pid=2930)\u001b[0m 10 | static_context_enrichment          | GatedResidualNetwork            | 1.7 K \n",
      "\u001b[2m\u001b[36m(RayExecutor pid=2930)\u001b[0m 11 | lstm_encoder                       | LSTM                            | 3.4 K \n",
      "\u001b[2m\u001b[36m(RayExecutor pid=2930)\u001b[0m 12 | lstm_decoder                       | LSTM                            | 3.4 K \n",
      "\u001b[2m\u001b[36m(RayExecutor pid=2930)\u001b[0m 13 | post_lstm_gate_encoder             | GatedLinearUnit                 | 840   \n",
      "\u001b[2m\u001b[36m(RayExecutor pid=2930)\u001b[0m 14 | post_lstm_add_norm_encoder         | AddNorm                         | 40    \n",
      "\u001b[2m\u001b[36m(RayExecutor pid=2930)\u001b[0m 15 | static_enrichment                  | GatedResidualNetwork            | 2.1 K \n",
      "\u001b[2m\u001b[36m(RayExecutor pid=2930)\u001b[0m 16 | multihead_attn                     | InterpretableMultiHeadAttention | 754   \n",
      "\u001b[2m\u001b[36m(RayExecutor pid=2930)\u001b[0m 17 | post_attn_gate_norm                | GateAddNorm                     | 880   \n",
      "\u001b[2m\u001b[36m(RayExecutor pid=2930)\u001b[0m 18 | pos_wise_ff                        | GatedResidualNetwork            | 1.7 K \n",
      "\u001b[2m\u001b[36m(RayExecutor pid=2930)\u001b[0m 19 | pre_output_gate_norm               | GateAddNorm                     | 880   \n",
      "\u001b[2m\u001b[36m(RayExecutor pid=2930)\u001b[0m 20 | output_layer                       | Linear                          | 147   \n",
      "\u001b[2m\u001b[36m(RayExecutor pid=2930)\u001b[0m ----------------------------------------------------------------------------------------\n",
      "\u001b[2m\u001b[36m(RayExecutor pid=2930)\u001b[0m 33.2 K    Trainable params\n",
      "\u001b[2m\u001b[36m(RayExecutor pid=2930)\u001b[0m 0         Non-trainable params\n",
      "\u001b[2m\u001b[36m(RayExecutor pid=2930)\u001b[0m 33.2 K    Total params\n",
      "\u001b[2m\u001b[36m(RayExecutor pid=2930)\u001b[0m 0.133     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation sanity check:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "Validation sanity check: 100%|██████████| 1/1 [01:10<00:00, 70.73s/it]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(RayExecutor pid=2931)\u001b[0m Global seed set to 415\n",
      "\u001b[2m\u001b[36m(RayExecutor pid=2933)\u001b[0m Global seed set to 415\n",
      "\u001b[2m\u001b[36m(RayExecutor pid=2932)\u001b[0m Global seed set to 415\n",
      "\u001b[2m\u001b[36m(RayExecutor pid=2935)\u001b[0m Global seed set to 415\n",
      "\u001b[2m\u001b[36m(RayExecutor pid=2936)\u001b[0m Global seed set to 415\n",
      "\u001b[2m\u001b[36m(RayExecutor pid=2930)\u001b[0m Global seed set to 415\n",
      "\u001b[2m\u001b[36m(RayExecutor pid=2937)\u001b[0m Global seed set to 415\n",
      "\u001b[2m\u001b[36m(RayExecutor pid=2934)\u001b[0m Global seed set to 415\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                      \n",
      "Epoch 0:   0%|          | 0/373 [00:00<00:02, 152.37it/s]   \n",
      "Epoch 0:   0%|          | 1/373 [00:35<1:50:49, 17.88s/it, loss=11.8, v_num=16, train_loss_step=11.80]\n",
      "Epoch 0:   1%|          | 2/373 [00:40<1:23:18, 13.47s/it, loss=12.3, v_num=16, train_loss_step=12.90]\n",
      "Epoch 0:   1%|          | 3/373 [00:45<1:09:32, 11.28s/it, loss=10.7, v_num=16, train_loss_step=7.360]\n",
      "Epoch 0:   1%|          | 4/373 [00:49<1:01:04,  9.93s/it, loss=10.7, v_num=16, train_loss_step=7.360]\n",
      "Epoch 0:   1%|          | 4/373 [00:49<1:01:04,  9.93s/it, loss=10.3, v_num=16, train_loss_step=9.230]\n",
      "Epoch 0:   1%|▏         | 5/373 [00:54<55:12,  9.00s/it, loss=9.52, v_num=16, train_loss_step=6.340]  \n",
      "Epoch 0:   2%|▏         | 6/373 [00:58<50:52,  8.32s/it, loss=10.4, v_num=16, train_loss_step=14.80]\n",
      "Epoch 0:   2%|▏         | 7/373 [01:03<48:10,  7.90s/it, loss=9.93, v_num=16, train_loss_step=7.100]\n",
      "Epoch 0:   2%|▏         | 8/373 [01:08<45:58,  7.56s/it, loss=10.1, v_num=16, train_loss_step=11.10]\n",
      "Epoch 0:   2%|▏         | 9/373 [01:12<44:00,  7.25s/it, loss=9.87, v_num=16, train_loss_step=8.170]\n",
      "Epoch 0:   3%|▎         | 10/373 [01:17<42:22,  7.00s/it, loss=9.74, v_num=16, train_loss_step=8.590]\n",
      "Epoch 0:   3%|▎         | 11/373 [01:21<41:02,  6.80s/it, loss=9.72, v_num=16, train_loss_step=9.540]\n",
      "Epoch 0:   3%|▎         | 12/373 [01:26<39:57,  6.64s/it, loss=9.53, v_num=16, train_loss_step=7.480]\n",
      "Epoch 0:   3%|▎         | 13/373 [01:30<38:56,  6.49s/it, loss=9.49, v_num=16, train_loss_step=9.020]\n",
      "Epoch 0:   4%|▍         | 14/373 [01:35<38:07,  6.37s/it, loss=9.41, v_num=16, train_loss_step=8.370]\n",
      "Epoch 0:   4%|▍         | 15/373 [01:39<37:15,  6.24s/it, loss=9.24, v_num=16, train_loss_step=6.740]\n",
      "Epoch 0:   4%|▍         | 16/373 [01:44<36:26,  6.12s/it, loss=9, v_num=16, train_loss_step=5.390]   \n",
      "Epoch 0:   5%|▍         | 17/373 [01:48<35:44,  6.02s/it, loss=8.67, v_num=16, train_loss_step=3.520]\n",
      "Epoch 0:   5%|▍         | 18/373 [01:52<34:59,  5.91s/it, loss=8.5, v_num=16, train_loss_step=5.540] \n",
      "Epoch 0:   5%|▌         | 19/373 [01:56<34:21,  5.82s/it, loss=8.35, v_num=16, train_loss_step=5.610]\n",
      "Epoch 0:   5%|▌         | 20/373 [02:00<33:49,  5.75s/it, loss=8.24, v_num=16, train_loss_step=6.160]\n",
      "Epoch 0:   6%|▌         | 21/373 [02:04<33:19,  5.68s/it, loss=7.94, v_num=16, train_loss_step=5.840]\n",
      "Epoch 0:   6%|▌         | 22/373 [02:09<32:50,  5.61s/it, loss=7.55, v_num=16, train_loss_step=5.020]\n",
      "Epoch 0:   6%|▌         | 23/373 [02:13<32:22,  5.55s/it, loss=7.48, v_num=16, train_loss_step=6.110]\n",
      "Epoch 0:   6%|▋         | 24/373 [02:17<31:59,  5.50s/it, loss=7.29, v_num=16, train_loss_step=5.430]\n",
      "Epoch 0:   7%|▋         | 25/373 [02:22<31:42,  5.47s/it, loss=7.25, v_num=16, train_loss_step=5.450]\n",
      "Epoch 0:   7%|▋         | 26/373 [02:26<31:19,  5.42s/it, loss=6.79, v_num=16, train_loss_step=5.530]\n",
      "Epoch 0:   7%|▋         | 27/373 [02:30<30:58,  5.37s/it, loss=6.66, v_num=16, train_loss_step=4.600]\n",
      "Epoch 0:   8%|▊         | 28/373 [02:34<30:35,  5.32s/it, loss=6.34, v_num=16, train_loss_step=4.690]\n",
      "Epoch 0:   8%|▊         | 29/373 [02:38<30:15,  5.28s/it, loss=6.29, v_num=16, train_loss_step=7.220]\n",
      "Epoch 0:   8%|▊         | 30/373 [02:42<29:54,  5.23s/it, loss=6.17, v_num=16, train_loss_step=6.090]\n",
      "Epoch 0:   8%|▊         | 31/373 [02:46<29:34,  5.19s/it, loss=5.97, v_num=16, train_loss_step=5.530]\n",
      "Epoch 0:   9%|▊         | 32/373 [02:49<29:13,  5.14s/it, loss=5.92, v_num=16, train_loss_step=6.610]\n",
      "Epoch 0:   9%|▉         | 33/373 [02:53<28:54,  5.10s/it, loss=5.73, v_num=16, train_loss_step=5.140]\n",
      "Epoch 0:   9%|▉         | 34/373 [02:57<28:38,  5.07s/it, loss=5.58, v_num=16, train_loss_step=5.340]\n",
      "Epoch 0:   9%|▉         | 35/373 [03:01<28:22,  5.04s/it, loss=5.45, v_num=16, train_loss_step=4.200]\n",
      "Epoch 0:  10%|▉         | 36/373 [03:05<28:05,  5.00s/it, loss=5.4, v_num=16, train_loss_step=4.360] \n",
      "Epoch 0:  10%|▉         | 37/373 [03:08<27:50,  4.97s/it, loss=5.46, v_num=16, train_loss_step=4.840]\n",
      "Epoch 0:  10%|█         | 38/373 [03:12<27:34,  4.94s/it, loss=5.48, v_num=16, train_loss_step=5.780]\n",
      "Epoch 0:  10%|█         | 39/373 [03:16<27:22,  4.92s/it, loss=5.46, v_num=16, train_loss_step=5.300]\n",
      "Epoch 0:  11%|█         | 40/373 [03:20<27:09,  4.89s/it, loss=5.32, v_num=16, train_loss_step=3.400]\n",
      "Epoch 0:  11%|█         | 41/373 [03:24<26:54,  4.86s/it, loss=5.34, v_num=16, train_loss_step=6.280]\n",
      "Epoch 0:  11%|█▏        | 42/373 [03:28<26:41,  4.84s/it, loss=5.31, v_num=16, train_loss_step=4.290]\n",
      "Epoch 0:  12%|█▏        | 43/373 [03:32<26:30,  4.82s/it, loss=5.25, v_num=16, train_loss_step=4.950]\n",
      "Epoch 0:  12%|█▏        | 44/373 [03:35<26:18,  4.80s/it, loss=5.3, v_num=16, train_loss_step=6.510] \n",
      "Epoch 0:  12%|█▏        | 45/373 [03:39<26:06,  4.78s/it, loss=5.36, v_num=16, train_loss_step=6.620]\n",
      "Epoch 0:  12%|█▏        | 46/373 [03:43<25:55,  4.76s/it, loss=5.42, v_num=16, train_loss_step=6.700]\n",
      "Epoch 0:  13%|█▎        | 47/373 [03:47<25:45,  4.74s/it, loss=5.42, v_num=16, train_loss_step=4.560]\n",
      "Epoch 0:  13%|█▎        | 48/373 [03:51<25:34,  4.72s/it, loss=5.53, v_num=16, train_loss_step=6.960]\n",
      "Epoch 0:  13%|█▎        | 49/373 [03:55<25:24,  4.70s/it, loss=5.46, v_num=16, train_loss_step=5.850]\n",
      "Epoch 0:  13%|█▎        | 50/373 [03:59<25:14,  4.69s/it, loss=5.38, v_num=16, train_loss_step=4.470]\n",
      "Epoch 0:  14%|█▎        | 51/373 [04:03<25:06,  4.68s/it, loss=5.34, v_num=16, train_loss_step=4.710]\n",
      "Epoch 0:  14%|█▍        | 52/373 [04:07<24:58,  4.67s/it, loss=5.21, v_num=16, train_loss_step=3.950]\n",
      "Epoch 0:  14%|█▍        | 53/373 [04:11<24:50,  4.66s/it, loss=5.21, v_num=16, train_loss_step=5.140]\n",
      "Epoch 0:  14%|█▍        | 54/373 [04:15<24:43,  4.65s/it, loss=5.23, v_num=16, train_loss_step=5.670]\n",
      "Epoch 0:  15%|█▍        | 55/373 [04:19<24:35,  4.64s/it, loss=5.25, v_num=16, train_loss_step=4.700]\n",
      "Epoch 0:  15%|█▌        | 56/373 [04:23<24:27,  4.63s/it, loss=5.19, v_num=16, train_loss_step=3.180]\n",
      "Epoch 0:  15%|█▌        | 57/373 [04:27<24:19,  4.62s/it, loss=5.3, v_num=16, train_loss_step=6.950] \n",
      "Epoch 0:  16%|█▌        | 58/373 [04:31<24:11,  4.61s/it, loss=5.23, v_num=16, train_loss_step=4.440]\n",
      "Epoch 0:  16%|█▌        | 59/373 [04:35<24:02,  4.59s/it, loss=5.24, v_num=16, train_loss_step=5.430]\n",
      "Epoch 0:  16%|█▌        | 60/373 [04:39<23:55,  4.59s/it, loss=5.35, v_num=16, train_loss_step=5.710]\n",
      "Epoch 0:  16%|█▋        | 61/373 [04:43<23:48,  4.58s/it, loss=5.32, v_num=16, train_loss_step=5.670]\n",
      "Epoch 0:  17%|█▋        | 62/373 [04:47<23:41,  4.57s/it, loss=5.33, v_num=16, train_loss_step=4.490]\n",
      "Epoch 0:  17%|█▋        | 63/373 [04:51<23:33,  4.56s/it, loss=5.33, v_num=16, train_loss_step=4.810]\n",
      "Epoch 0:  17%|█▋        | 64/373 [04:55<23:26,  4.55s/it, loss=5.23, v_num=16, train_loss_step=4.670]\n",
      "Epoch 0:  17%|█▋        | 65/373 [04:59<23:18,  4.54s/it, loss=5.13, v_num=16, train_loss_step=4.560]\n",
      "Epoch 0:  18%|█▊        | 66/373 [05:03<23:10,  4.53s/it, loss=4.99, v_num=16, train_loss_step=3.850]\n",
      "Epoch 0:  18%|█▊        | 67/373 [05:07<23:04,  4.52s/it, loss=5.01, v_num=16, train_loss_step=5.000]\n",
      "Epoch 0:  18%|█▊        | 68/373 [05:11<22:57,  4.52s/it, loss=4.9, v_num=16, train_loss_step=4.770] \n",
      "Epoch 0:  18%|█▊        | 69/373 [05:15<22:50,  4.51s/it, loss=4.9, v_num=16, train_loss_step=5.850]\n",
      "Epoch 0:  19%|█▉        | 70/373 [05:19<22:45,  4.51s/it, loss=4.86, v_num=16, train_loss_step=3.710]\n",
      "Epoch 0:  19%|█▉        | 71/373 [05:24<22:40,  4.50s/it, loss=4.93, v_num=16, train_loss_step=6.060]\n",
      "Epoch 0:  19%|█▉        | 72/373 [05:28<22:34,  4.50s/it, loss=4.9, v_num=16, train_loss_step=3.370] \n",
      "Epoch 0:  20%|█▉        | 73/373 [05:32<22:29,  4.50s/it, loss=4.91, v_num=16, train_loss_step=5.360]\n",
      "Epoch 0:  20%|█▉        | 74/373 [05:36<22:23,  4.49s/it, loss=4.95, v_num=16, train_loss_step=6.330]\n",
      "Epoch 0:  20%|██        | 75/373 [05:41<22:17,  4.49s/it, loss=5, v_num=16, train_loss_step=5.740]   \n",
      "Epoch 0:  20%|██        | 76/373 [05:45<22:10,  4.48s/it, loss=5.08, v_num=16, train_loss_step=4.810]\n",
      "Epoch 0:  21%|██        | 77/373 [05:49<22:04,  4.47s/it, loss=4.9, v_num=16, train_loss_step=3.400] \n",
      "Epoch 0:  21%|██        | 78/373 [05:52<21:57,  4.47s/it, loss=4.93, v_num=16, train_loss_step=4.940]\n",
      "Epoch 0:  21%|██        | 79/373 [05:56<21:51,  4.46s/it, loss=4.83, v_num=16, train_loss_step=3.460]\n",
      "Epoch 0:  21%|██▏       | 80/373 [06:00<21:44,  4.45s/it, loss=4.73, v_num=16, train_loss_step=3.760]\n",
      "Epoch 0:  22%|██▏       | 81/373 [06:04<21:38,  4.45s/it, loss=4.74, v_num=16, train_loss_step=5.950]\n",
      "Epoch 0:  22%|██▏       | 82/373 [06:08<21:32,  4.44s/it, loss=4.72, v_num=16, train_loss_step=4.030]\n",
      "Epoch 0:  22%|██▏       | 83/373 [06:12<21:26,  4.44s/it, loss=4.73, v_num=16, train_loss_step=5.070]\n",
      "Epoch 0:  23%|██▎       | 84/373 [06:16<21:21,  4.43s/it, loss=4.72, v_num=16, train_loss_step=4.340]\n",
      "Epoch 0:  23%|██▎       | 85/373 [06:20<21:15,  4.43s/it, loss=4.75, v_num=16, train_loss_step=5.180]\n",
      "Epoch 0:  23%|██▎       | 86/373 [06:24<21:09,  4.42s/it, loss=4.84, v_num=16, train_loss_step=5.660]\n",
      "Epoch 0:  23%|██▎       | 87/373 [06:28<21:03,  4.42s/it, loss=4.79, v_num=16, train_loss_step=4.110]\n",
      "Epoch 0:  24%|██▎       | 88/373 [06:32<20:57,  4.41s/it, loss=4.77, v_num=16, train_loss_step=4.370]\n",
      "Epoch 0:  24%|██▍       | 89/373 [06:36<20:51,  4.41s/it, loss=4.63, v_num=16, train_loss_step=2.980]\n",
      "Epoch 0:  24%|██▍       | 90/373 [06:40<20:46,  4.40s/it, loss=4.64, v_num=16, train_loss_step=3.980]\n",
      "Epoch 0:  24%|██▍       | 91/373 [06:44<20:40,  4.40s/it, loss=4.54, v_num=16, train_loss_step=3.930]\n",
      "Epoch 0:  25%|██▍       | 92/373 [06:48<20:35,  4.40s/it, loss=4.53, v_num=16, train_loss_step=3.230]\n",
      "Epoch 0:  25%|██▍       | 93/373 [06:52<20:29,  4.39s/it, loss=4.45, v_num=16, train_loss_step=3.710]\n",
      "Epoch 0:  25%|██▌       | 94/373 [06:56<20:24,  4.39s/it, loss=4.31, v_num=16, train_loss_step=3.470]\n",
      "Epoch 0:  25%|██▌       | 95/373 [07:00<20:19,  4.39s/it, loss=4.22, v_num=16, train_loss_step=4.010]\n",
      "Epoch 0:  26%|██▌       | 96/373 [07:04<20:13,  4.38s/it, loss=4.15, v_num=16, train_loss_step=3.440]\n",
      "Epoch 0:  26%|██▌       | 97/373 [07:08<20:07,  4.38s/it, loss=4.18, v_num=16, train_loss_step=3.940]\n",
      "Epoch 0:  26%|██▋       | 98/373 [07:12<20:02,  4.37s/it, loss=4.11, v_num=16, train_loss_step=3.540]\n",
      "Epoch 0:  27%|██▋       | 99/373 [07:16<19:56,  4.37s/it, loss=4.13, v_num=16, train_loss_step=3.840]\n",
      "Epoch 0:  27%|██▋       | 100/373 [07:20<19:51,  4.36s/it, loss=4.19, v_num=16, train_loss_step=4.920]\n",
      "Epoch 0:  27%|██▋       | 101/373 [07:25<19:46,  4.36s/it, loss=4.11, v_num=16, train_loss_step=4.370]\n",
      "Epoch 0:  27%|██▋       | 102/373 [07:29<19:41,  4.36s/it, loss=4.04, v_num=16, train_loss_step=2.700]\n",
      "Epoch 0:  28%|██▊       | 103/373 [07:33<19:36,  4.36s/it, loss=3.93, v_num=16, train_loss_step=2.820]\n",
      "Epoch 0:  28%|██▊       | 104/373 [07:36<19:30,  4.35s/it, loss=3.85, v_num=16, train_loss_step=2.730]\n",
      "Epoch 0:  28%|██▊       | 105/373 [07:40<19:25,  4.35s/it, loss=3.85, v_num=16, train_loss_step=5.140]\n",
      "Epoch 0:  28%|██▊       | 106/373 [07:44<19:19,  4.34s/it, loss=3.74, v_num=16, train_loss_step=3.560]\n",
      "Epoch 0:  29%|██▊       | 107/373 [07:48<19:14,  4.34s/it, loss=3.73, v_num=16, train_loss_step=3.830]\n",
      "Epoch 0:  29%|██▉       | 108/373 [07:52<19:09,  4.34s/it, loss=3.73, v_num=16, train_loss_step=4.400]\n",
      "Epoch 0:  29%|██▉       | 109/373 [07:57<19:05,  4.34s/it, loss=3.76, v_num=16, train_loss_step=3.710]\n",
      "Epoch 0:  29%|██▉       | 110/373 [08:01<19:00,  4.34s/it, loss=3.72, v_num=16, train_loss_step=3.070]\n",
      "Epoch 0:  30%|██▉       | 111/373 [08:05<18:56,  4.34s/it, loss=3.7, v_num=16, train_loss_step=3.630] \n",
      "Epoch 0:  30%|███       | 112/373 [08:10<18:52,  4.34s/it, loss=3.69, v_num=16, train_loss_step=2.940]\n",
      "Epoch 0:  30%|███       | 113/373 [08:14<18:47,  4.34s/it, loss=3.7, v_num=16, train_loss_step=4.010] \n",
      "Epoch 0:  31%|███       | 114/373 [08:18<18:42,  4.33s/it, loss=3.71, v_num=16, train_loss_step=3.680]\n",
      "Epoch 0:  31%|███       | 115/373 [08:22<18:37,  4.33s/it, loss=3.65, v_num=16, train_loss_step=2.690]\n",
      "Epoch 0:  31%|███       | 116/373 [08:27<18:34,  4.34s/it, loss=3.63, v_num=16, train_loss_step=2.990]\n",
      "Epoch 0:  31%|███▏      | 117/373 [08:32<18:31,  4.34s/it, loss=3.56, v_num=16, train_loss_step=2.630]\n",
      "Epoch 0:  32%|███▏      | 118/373 [08:36<18:26,  4.34s/it, loss=3.55, v_num=16, train_loss_step=3.350]\n",
      "Epoch 0:  32%|███▏      | 119/373 [08:40<18:22,  4.34s/it, loss=3.52, v_num=16, train_loss_step=3.250]\n",
      "Epoch 0:  32%|███▏      | 120/373 [08:44<18:17,  4.34s/it, loss=3.49, v_num=16, train_loss_step=4.370]\n",
      "Epoch 0:  32%|███▏      | 121/373 [08:48<18:12,  4.33s/it, loss=3.4, v_num=16, train_loss_step=2.560] \n",
      "Epoch 0:  33%|███▎      | 122/373 [08:52<18:07,  4.33s/it, loss=3.45, v_num=16, train_loss_step=3.640]\n",
      "Epoch 0:  33%|███▎      | 123/373 [08:57<18:03,  4.33s/it, loss=3.44, v_num=16, train_loss_step=2.540]\n",
      "Epoch 0:  33%|███▎      | 124/373 [09:02<17:59,  4.34s/it, loss=3.45, v_num=16, train_loss_step=3.040]\n",
      "Epoch 0:  34%|███▎      | 125/373 [09:06<17:55,  4.34s/it, loss=3.34, v_num=16, train_loss_step=2.880]\n",
      "Epoch 0:  34%|███▍      | 126/373 [09:10<17:50,  4.34s/it, loss=3.35, v_num=16, train_loss_step=3.880]\n",
      "Epoch 0:  34%|███▍      | 127/373 [09:14<17:46,  4.34s/it, loss=3.37, v_num=16, train_loss_step=4.130]\n",
      "Epoch 0:  34%|███▍      | 128/373 [09:19<17:41,  4.33s/it, loss=3.31, v_num=16, train_loss_step=3.120]\n",
      "Epoch 0:  35%|███▍      | 129/373 [09:23<17:36,  4.33s/it, loss=3.32, v_num=16, train_loss_step=4.010]\n",
      "Epoch 0:  35%|███▍      | 130/373 [09:27<17:31,  4.33s/it, loss=3.3, v_num=16, train_loss_step=2.700] \n",
      "Epoch 0:  35%|███▌      | 131/373 [09:31<17:26,  4.33s/it, loss=3.32, v_num=16, train_loss_step=3.900]\n",
      "Epoch 0:  35%|███▌      | 132/373 [09:35<17:22,  4.33s/it, loss=3.36, v_num=16, train_loss_step=3.920]\n",
      "Epoch 0:  36%|███▌      | 133/373 [09:39<17:17,  4.32s/it, loss=3.3, v_num=16, train_loss_step=2.680] \n",
      "Epoch 0:  36%|███▌      | 134/373 [09:42<17:11,  4.32s/it, loss=3.23, v_num=16, train_loss_step=2.340]\n",
      "Epoch 0:  36%|███▌      | 135/373 [09:46<17:06,  4.31s/it, loss=3.26, v_num=16, train_loss_step=3.300]\n",
      "Epoch 0:  36%|███▋      | 136/373 [09:50<17:01,  4.31s/it, loss=3.26, v_num=16, train_loss_step=2.960]\n",
      "Epoch 0:  37%|███▋      | 137/373 [09:54<16:56,  4.31s/it, loss=3.32, v_num=16, train_loss_step=3.800]\n",
      "Epoch 0:  37%|███▋      | 138/373 [09:58<16:51,  4.30s/it, loss=3.34, v_num=16, train_loss_step=3.760]\n",
      "Epoch 0:  37%|███▋      | 139/373 [10:02<16:46,  4.30s/it, loss=3.34, v_num=16, train_loss_step=3.300]\n",
      "Epoch 0:  38%|███▊      | 140/373 [10:06<16:41,  4.30s/it, loss=3.29, v_num=16, train_loss_step=3.340]\n",
      "Epoch 0:  38%|███▊      | 141/373 [10:10<16:37,  4.30s/it, loss=3.39, v_num=16, train_loss_step=4.620]\n",
      "Epoch 0:  38%|███▊      | 142/373 [10:14<16:33,  4.30s/it, loss=3.37, v_num=16, train_loss_step=3.260]\n",
      "Epoch 0:  38%|███▊      | 143/373 [10:18<16:28,  4.30s/it, loss=3.44, v_num=16, train_loss_step=3.840]\n",
      "Epoch 0:  39%|███▊      | 144/373 [10:22<16:23,  4.29s/it, loss=3.46, v_num=16, train_loss_step=3.390]\n",
      "Epoch 0:  39%|███▉      | 145/373 [10:26<16:19,  4.29s/it, loss=3.44, v_num=16, train_loss_step=2.610]\n",
      "Epoch 0:  39%|███▉      | 146/373 [10:30<16:14,  4.29s/it, loss=3.39, v_num=16, train_loss_step=2.800]\n",
      "Epoch 0:  39%|███▉      | 147/373 [10:34<16:09,  4.29s/it, loss=3.39, v_num=16, train_loss_step=4.200]\n",
      "Epoch 0:  40%|███▉      | 148/373 [10:38<16:04,  4.29s/it, loss=3.39, v_num=16, train_loss_step=3.090]\n",
      "Epoch 0:  40%|███▉      | 149/373 [10:42<15:59,  4.29s/it, loss=3.38, v_num=16, train_loss_step=3.890]\n",
      "Epoch 0:  40%|████      | 150/373 [10:46<15:55,  4.28s/it, loss=3.4, v_num=16, train_loss_step=2.940] \n",
      "Epoch 0:  40%|████      | 151/373 [10:51<15:51,  4.28s/it, loss=3.31, v_num=16, train_loss_step=2.100]\n",
      "Epoch 0:  41%|████      | 152/373 [10:55<15:46,  4.28s/it, loss=3.24, v_num=16, train_loss_step=2.660]\n",
      "Epoch 0:  41%|████      | 153/373 [10:58<15:41,  4.28s/it, loss=3.26, v_num=16, train_loss_step=3.110]\n",
      "Epoch 0:  41%|████▏     | 154/373 [11:02<15:36,  4.28s/it, loss=3.32, v_num=16, train_loss_step=3.380]\n",
      "Epoch 0:  42%|████▏     | 155/373 [11:06<15:31,  4.27s/it, loss=3.27, v_num=16, train_loss_step=2.460]\n",
      "Epoch 0:  42%|████▏     | 156/373 [11:10<15:27,  4.27s/it, loss=3.28, v_num=16, train_loss_step=3.020]\n",
      "Epoch 0:  42%|████▏     | 157/373 [11:14<15:22,  4.27s/it, loss=3.27, v_num=16, train_loss_step=3.590]\n",
      "Epoch 0:  42%|████▏     | 158/373 [11:18<15:17,  4.27s/it, loss=3.28, v_num=16, train_loss_step=4.070]\n",
      "Epoch 0:  43%|████▎     | 159/373 [11:22<15:13,  4.27s/it, loss=3.3, v_num=16, train_loss_step=3.740] \n",
      "Epoch 0:  43%|████▎     | 160/373 [11:26<15:08,  4.27s/it, loss=3.25, v_num=16, train_loss_step=2.230]\n",
      "Epoch 0:  43%|████▎     | 161/373 [11:30<15:04,  4.27s/it, loss=3.14, v_num=16, train_loss_step=2.360]\n",
      "Epoch 0:  43%|████▎     | 162/373 [11:34<14:59,  4.26s/it, loss=3.11, v_num=16, train_loss_step=2.750]\n",
      "Epoch 0:  44%|████▎     | 163/373 [11:38<14:54,  4.26s/it, loss=3.07, v_num=16, train_loss_step=3.000]\n",
      "Epoch 0:  44%|████▍     | 164/373 [11:43<14:50,  4.26s/it, loss=3.05, v_num=16, train_loss_step=2.970]\n",
      "Epoch 0:  44%|████▍     | 165/373 [11:47<14:46,  4.26s/it, loss=3.06, v_num=16, train_loss_step=2.900]\n",
      "Epoch 0:  45%|████▍     | 166/373 [11:51<14:41,  4.26s/it, loss=3.08, v_num=16, train_loss_step=3.210]\n",
      "Epoch 0:  45%|████▍     | 167/373 [11:55<14:37,  4.26s/it, loss=3.01, v_num=16, train_loss_step=2.830]\n",
      "Epoch 0:  45%|████▌     | 168/373 [11:59<14:33,  4.26s/it, loss=3, v_num=16, train_loss_step=2.780]   \n",
      "Epoch 0:  45%|████▌     | 169/373 [12:04<14:28,  4.26s/it, loss=2.92, v_num=16, train_loss_step=2.310]\n",
      "Epoch 0:  46%|████▌     | 170/373 [12:08<14:24,  4.26s/it, loss=2.86, v_num=16, train_loss_step=1.640]\n",
      "Epoch 0:  46%|████▌     | 171/373 [12:11<14:19,  4.26s/it, loss=2.92, v_num=16, train_loss_step=3.440]\n",
      "Epoch 0:  46%|████▌     | 172/373 [12:15<14:15,  4.25s/it, loss=2.94, v_num=16, train_loss_step=3.090]\n",
      "Epoch 0:  46%|████▋     | 173/373 [12:19<14:10,  4.25s/it, loss=3.01, v_num=16, train_loss_step=4.400]\n",
      "Epoch 0:  47%|████▋     | 174/373 [12:23<14:05,  4.25s/it, loss=2.96, v_num=16, train_loss_step=2.460]\n",
      "Epoch 0:  47%|████▋     | 175/373 [12:27<14:01,  4.25s/it, loss=2.95, v_num=16, train_loss_step=2.310]\n",
      "Epoch 0:  47%|████▋     | 176/373 [12:31<13:56,  4.24s/it, loss=2.93, v_num=16, train_loss_step=2.620]\n",
      "Epoch 0:  47%|████▋     | 177/373 [12:35<13:51,  4.24s/it, loss=2.87, v_num=16, train_loss_step=2.250]\n",
      "Epoch 0:  48%|████▊     | 178/373 [12:39<13:47,  4.24s/it, loss=2.84, v_num=16, train_loss_step=3.600]\n",
      "Epoch 0:  48%|████▊     | 179/373 [12:43<13:42,  4.24s/it, loss=2.81, v_num=16, train_loss_step=3.090]\n",
      "Epoch 0:  48%|████▊     | 180/373 [12:47<13:38,  4.24s/it, loss=2.87, v_num=16, train_loss_step=3.350]\n",
      "Epoch 0:  49%|████▊     | 181/373 [12:52<13:34,  4.24s/it, loss=2.95, v_num=16, train_loss_step=4.100]\n",
      "Epoch 0:  49%|████▉     | 182/373 [12:56<13:30,  4.24s/it, loss=2.94, v_num=16, train_loss_step=2.500]\n",
      "Epoch 0:  49%|████▉     | 183/373 [13:00<13:26,  4.24s/it, loss=2.96, v_num=16, train_loss_step=3.370]\n",
      "Epoch 0:  49%|████▉     | 184/373 [13:04<13:21,  4.24s/it, loss=2.94, v_num=16, train_loss_step=2.490]\n",
      "Epoch 0:  50%|████▉     | 185/373 [13:08<13:17,  4.24s/it, loss=2.94, v_num=16, train_loss_step=2.920]\n",
      "Epoch 0:  50%|████▉     | 186/373 [13:12<13:12,  4.24s/it, loss=2.93, v_num=16, train_loss_step=3.080]\n",
      "Epoch 0:  50%|█████     | 187/373 [13:16<13:08,  4.24s/it, loss=2.95, v_num=16, train_loss_step=3.310]\n",
      "Epoch 0:  50%|█████     | 188/373 [13:21<13:04,  4.24s/it, loss=2.97, v_num=16, train_loss_step=3.140]\n",
      "Epoch 0:  51%|█████     | 189/373 [13:25<12:59,  4.24s/it, loss=3, v_num=16, train_loss_step=2.860]   \n",
      "Epoch 0:  51%|█████     | 190/373 [13:29<12:55,  4.24s/it, loss=3.05, v_num=16, train_loss_step=2.610]\n",
      "Epoch 0:  51%|█████     | 191/373 [13:33<12:50,  4.23s/it, loss=3, v_num=16, train_loss_step=2.520]   \n",
      "Epoch 0:  51%|█████▏    | 192/373 [13:37<12:46,  4.23s/it, loss=2.99, v_num=16, train_loss_step=2.830]\n",
      "Epoch 0:  52%|█████▏    | 193/373 [13:41<12:42,  4.24s/it, loss=2.9, v_num=16, train_loss_step=2.600] \n",
      "Epoch 0:  52%|█████▏    | 194/373 [13:45<12:38,  4.24s/it, loss=2.87, v_num=16, train_loss_step=1.920]\n",
      "Epoch 0:  52%|█████▏    | 195/373 [13:50<12:33,  4.24s/it, loss=2.9, v_num=16, train_loss_step=2.790] \n",
      "Epoch 0:  53%|█████▎    | 196/373 [13:54<12:29,  4.24s/it, loss=2.88, v_num=16, train_loss_step=2.240]\n",
      "Epoch 0:  53%|█████▎    | 197/373 [13:58<12:25,  4.24s/it, loss=2.93, v_num=16, train_loss_step=3.220]\n",
      "Epoch 0:  53%|█████▎    | 198/373 [14:02<12:21,  4.24s/it, loss=2.87, v_num=16, train_loss_step=2.490]\n",
      "Epoch 0:  53%|█████▎    | 199/373 [14:07<12:17,  4.24s/it, loss=2.87, v_num=16, train_loss_step=3.100]\n",
      "Epoch 0:  54%|█████▎    | 200/373 [14:11<12:13,  4.24s/it, loss=2.87, v_num=16, train_loss_step=3.320]\n",
      "Epoch 0:  54%|█████▍    | 201/373 [14:16<12:09,  4.24s/it, loss=2.84, v_num=16, train_loss_step=3.560]\n",
      "Epoch 0:  54%|█████▍    | 202/373 [14:21<12:05,  4.24s/it, loss=2.85, v_num=16, train_loss_step=2.640]\n",
      "Epoch 0:  54%|█████▍    | 203/373 [14:25<12:01,  4.24s/it, loss=2.8, v_num=16, train_loss_step=2.340] \n",
      "Epoch 0:  55%|█████▍    | 204/373 [14:29<11:57,  4.24s/it, loss=2.83, v_num=16, train_loss_step=3.120]\n",
      "Epoch 0:  55%|█████▍    | 205/373 [14:34<11:52,  4.24s/it, loss=2.84, v_num=16, train_loss_step=3.130]\n",
      "Epoch 0:  55%|█████▌    | 206/373 [14:38<11:48,  4.24s/it, loss=2.81, v_num=16, train_loss_step=2.390]\n",
      "Epoch 0:  55%|█████▌    | 207/373 [14:42<11:44,  4.24s/it, loss=2.76, v_num=16, train_loss_step=2.410]\n",
      "Epoch 0:  56%|█████▌    | 208/373 [14:46<11:40,  4.24s/it, loss=2.73, v_num=16, train_loss_step=2.570]\n",
      "Epoch 0:  56%|█████▌    | 209/373 [14:50<11:35,  4.24s/it, loss=2.72, v_num=16, train_loss_step=2.540]\n",
      "Epoch 0:  56%|█████▋    | 210/373 [14:55<11:31,  4.24s/it, loss=2.71, v_num=16, train_loss_step=2.400]\n",
      "Epoch 0:  57%|█████▋    | 211/373 [14:59<11:27,  4.24s/it, loss=2.71, v_num=16, train_loss_step=2.540]\n",
      "Epoch 0:  57%|█████▋    | 212/373 [15:03<11:23,  4.24s/it, loss=2.7, v_num=16, train_loss_step=2.590] \n",
      "Epoch 0:  57%|█████▋    | 213/373 [15:07<11:18,  4.24s/it, loss=2.7, v_num=16, train_loss_step=2.770]\n",
      "Epoch 0:  57%|█████▋    | 214/373 [15:11<11:14,  4.24s/it, loss=2.75, v_num=16, train_loss_step=2.790]\n",
      "Epoch 0:  58%|█████▊    | 215/373 [15:15<11:09,  4.24s/it, loss=2.74, v_num=16, train_loss_step=2.660]\n",
      "Epoch 0:  58%|█████▊    | 216/373 [15:19<11:05,  4.24s/it, loss=2.79, v_num=16, train_loss_step=3.230]\n",
      "Epoch 0:  58%|█████▊    | 217/373 [15:23<11:01,  4.24s/it, loss=2.79, v_num=16, train_loss_step=3.220]\n",
      "Epoch 0:  58%|█████▊    | 218/373 [15:27<10:56,  4.24s/it, loss=2.83, v_num=16, train_loss_step=3.330]\n",
      "Epoch 0:  59%|█████▊    | 219/373 [15:31<10:52,  4.24s/it, loss=2.81, v_num=16, train_loss_step=2.670]\n",
      "Epoch 0:  59%|█████▉    | 220/373 [15:36<10:48,  4.24s/it, loss=2.77, v_num=16, train_loss_step=2.520]\n",
      "Epoch 0:  59%|█████▉    | 221/373 [15:40<10:43,  4.23s/it, loss=2.74, v_num=16, train_loss_step=2.980]\n",
      "Epoch 0:  60%|█████▉    | 222/373 [15:44<10:39,  4.23s/it, loss=2.75, v_num=16, train_loss_step=2.750]\n",
      "Epoch 0:  60%|█████▉    | 223/373 [15:48<10:34,  4.23s/it, loss=2.8, v_num=16, train_loss_step=3.350] \n",
      "Epoch 0:  60%|██████    | 224/373 [15:51<10:30,  4.23s/it, loss=2.79, v_num=16, train_loss_step=2.930]\n",
      "Epoch 0:  60%|██████    | 225/373 [15:55<10:25,  4.23s/it, loss=2.77, v_num=16, train_loss_step=2.830]\n",
      "Epoch 0:  61%|██████    | 226/373 [16:00<10:21,  4.23s/it, loss=2.8, v_num=16, train_loss_step=2.920] \n",
      "Epoch 0:  61%|██████    | 227/373 [16:03<10:17,  4.23s/it, loss=2.79, v_num=16, train_loss_step=2.180]\n",
      "Epoch 0:  61%|██████    | 228/373 [16:07<10:12,  4.23s/it, loss=2.76, v_num=16, train_loss_step=1.920]\n",
      "Epoch 0:  61%|██████▏   | 229/373 [16:11<10:08,  4.23s/it, loss=2.73, v_num=16, train_loss_step=2.010]\n",
      "Epoch 0:  62%|██████▏   | 230/373 [16:15<10:04,  4.22s/it, loss=2.74, v_num=16, train_loss_step=2.530]\n",
      "Epoch 0:  62%|██████▏   | 231/373 [16:19<09:59,  4.22s/it, loss=2.78, v_num=16, train_loss_step=3.500]\n",
      "Epoch 0:  62%|██████▏   | 232/373 [16:23<09:55,  4.22s/it, loss=2.79, v_num=16, train_loss_step=2.710]\n",
      "Epoch 0:  62%|██████▏   | 233/373 [16:27<09:51,  4.22s/it, loss=2.79, v_num=16, train_loss_step=2.750]\n",
      "Epoch 0:  63%|██████▎   | 234/373 [16:31<09:46,  4.22s/it, loss=2.77, v_num=16, train_loss_step=2.330]\n",
      "Epoch 0:  63%|██████▎   | 235/373 [16:36<09:42,  4.22s/it, loss=2.77, v_num=16, train_loss_step=2.740]\n",
      "Epoch 0:  63%|██████▎   | 236/373 [16:40<09:38,  4.22s/it, loss=2.72, v_num=16, train_loss_step=2.160]\n",
      "Epoch 0:  64%|██████▎   | 237/373 [16:44<09:33,  4.22s/it, loss=2.67, v_num=16, train_loss_step=2.280]\n",
      "Epoch 0:  64%|██████▍   | 238/373 [16:47<09:29,  4.22s/it, loss=2.63, v_num=16, train_loss_step=2.500]\n",
      "Epoch 0:  64%|██████▍   | 239/373 [16:51<09:24,  4.22s/it, loss=2.6, v_num=16, train_loss_step=2.060] \n",
      "Epoch 0:  64%|██████▍   | 240/373 [16:55<09:20,  4.22s/it, loss=2.64, v_num=16, train_loss_step=3.450]\n",
      "Epoch 0:  65%|██████▍   | 241/373 [16:59<09:16,  4.21s/it, loss=2.61, v_num=16, train_loss_step=2.370]\n",
      "Epoch 0:  65%|██████▍   | 242/373 [17:03<09:11,  4.21s/it, loss=2.62, v_num=16, train_loss_step=2.800]\n",
      "Epoch 0:  65%|██████▌   | 243/373 [17:07<09:07,  4.21s/it, loss=2.54, v_num=16, train_loss_step=1.850]\n",
      "Epoch 0:  65%|██████▌   | 244/373 [17:11<09:03,  4.21s/it, loss=2.53, v_num=16, train_loss_step=2.640]\n",
      "Epoch 0:  66%|██████▌   | 245/373 [17:16<08:59,  4.21s/it, loss=2.51, v_num=16, train_loss_step=2.420]\n",
      "Epoch 0:  66%|██████▌   | 246/373 [17:20<08:54,  4.21s/it, loss=2.49, v_num=16, train_loss_step=2.680]\n",
      "Epoch 0:  66%|██████▌   | 247/373 [17:24<08:50,  4.21s/it, loss=2.48, v_num=16, train_loss_step=1.820]\n",
      "Epoch 0:  66%|██████▋   | 248/373 [17:28<08:46,  4.21s/it, loss=2.54, v_num=16, train_loss_step=3.130]\n",
      "Epoch 0:  67%|██████▋   | 249/373 [17:32<08:42,  4.21s/it, loss=2.55, v_num=16, train_loss_step=2.190]\n",
      "Epoch 0:  67%|██████▋   | 250/373 [17:36<08:37,  4.21s/it, loss=2.58, v_num=16, train_loss_step=3.280]\n",
      "Epoch 0:  67%|██████▋   | 251/373 [17:41<08:33,  4.21s/it, loss=2.55, v_num=16, train_loss_step=2.780]\n",
      "Epoch 0:  68%|██████▊   | 252/373 [17:45<08:29,  4.21s/it, loss=2.54, v_num=16, train_loss_step=2.540]\n",
      "Epoch 0:  68%|██████▊   | 253/373 [17:49<08:25,  4.21s/it, loss=2.52, v_num=16, train_loss_step=2.470]\n",
      "Epoch 0:  68%|██████▊   | 254/373 [17:53<08:21,  4.21s/it, loss=2.53, v_num=16, train_loss_step=2.400]\n",
      "Epoch 0:  68%|██████▊   | 255/373 [17:58<08:16,  4.21s/it, loss=2.52, v_num=16, train_loss_step=2.650]\n",
      "Epoch 0:  69%|██████▊   | 256/373 [18:02<08:12,  4.21s/it, loss=2.53, v_num=16, train_loss_step=2.320]\n",
      "Epoch 0:  69%|██████▉   | 257/373 [18:06<08:08,  4.21s/it, loss=2.51, v_num=16, train_loss_step=1.860]\n",
      "Epoch 0:  69%|██████▉   | 258/373 [18:10<08:04,  4.21s/it, loss=2.5, v_num=16, train_loss_step=2.220] \n",
      "Epoch 0:  69%|██████▉   | 259/373 [18:14<07:59,  4.21s/it, loss=2.56, v_num=16, train_loss_step=3.430]\n",
      "Epoch 0:  70%|██████▉   | 260/373 [18:18<07:55,  4.21s/it, loss=2.54, v_num=16, train_loss_step=2.950]\n",
      "Epoch 0:  70%|██████▉   | 261/373 [18:24<07:52,  4.22s/it, loss=2.52, v_num=16, train_loss_step=2.080]\n",
      "Epoch 0:  70%|███████   | 262/373 [18:29<07:48,  4.22s/it, loss=2.53, v_num=16, train_loss_step=2.850]\n",
      "Epoch 0:  71%|███████   | 263/373 [18:34<07:44,  4.22s/it, loss=2.58, v_num=16, train_loss_step=2.830]\n",
      "Epoch 0:  71%|███████   | 264/373 [18:39<07:40,  4.23s/it, loss=2.56, v_num=16, train_loss_step=2.280]\n",
      "Epoch 0:  71%|███████   | 265/373 [18:44<07:36,  4.23s/it, loss=2.56, v_num=16, train_loss_step=2.490]\n",
      "Epoch 0:  71%|███████▏  | 266/373 [18:48<07:32,  4.23s/it, loss=2.54, v_num=16, train_loss_step=2.200]\n",
      "Epoch 0:  72%|███████▏  | 267/373 [18:53<07:28,  4.23s/it, loss=2.55, v_num=16, train_loss_step=2.080]\n",
      "Epoch 0:  72%|███████▏  | 268/373 [18:58<07:24,  4.23s/it, loss=2.52, v_num=16, train_loss_step=2.460]\n",
      "Epoch 0:  72%|███████▏  | 269/373 [19:02<07:20,  4.23s/it, loss=2.54, v_num=16, train_loss_step=2.580]\n",
      "Epoch 0:  72%|███████▏  | 270/373 [19:07<07:15,  4.23s/it, loss=2.49, v_num=16, train_loss_step=2.430]\n",
      "Epoch 0:  73%|███████▎  | 271/373 [19:11<07:11,  4.23s/it, loss=2.51, v_num=16, train_loss_step=3.000]\n",
      "Epoch 0:  73%|███████▎  | 272/373 [19:15<07:07,  4.23s/it, loss=2.51, v_num=16, train_loss_step=2.540]\n",
      "Epoch 0:  73%|███████▎  | 273/373 [19:19<07:03,  4.23s/it, loss=2.53, v_num=16, train_loss_step=2.950]\n",
      "Epoch 0:  73%|███████▎  | 274/373 [19:24<06:59,  4.23s/it, loss=2.53, v_num=16, train_loss_step=2.410]\n",
      "Epoch 0:  74%|███████▎  | 275/373 [19:28<06:55,  4.24s/it, loss=2.5, v_num=16, train_loss_step=2.100] \n",
      "Epoch 0:  74%|███████▍  | 276/373 [19:34<06:51,  4.24s/it, loss=2.54, v_num=16, train_loss_step=2.990]\n",
      "Epoch 0:  74%|███████▍  | 277/373 [19:38<06:47,  4.24s/it, loss=2.56, v_num=16, train_loss_step=2.310]\n",
      "Epoch 0:  75%|███████▍  | 278/373 [19:43<06:42,  4.24s/it, loss=2.58, v_num=16, train_loss_step=2.720]\n",
      "Epoch 0:  75%|███████▍  | 279/373 [19:47<06:38,  4.24s/it, loss=2.52, v_num=16, train_loss_step=2.130]\n",
      "Epoch 0:  75%|███████▌  | 280/373 [19:52<06:34,  4.24s/it, loss=2.5, v_num=16, train_loss_step=2.670] \n",
      "Epoch 0:  75%|███████▌  | 281/373 [19:56<06:30,  4.24s/it, loss=2.5, v_num=16, train_loss_step=1.930]\n",
      "Epoch 0:  76%|███████▌  | 282/373 [20:01<06:26,  4.25s/it, loss=2.46, v_num=16, train_loss_step=2.100]\n",
      "Epoch 0:  76%|███████▌  | 283/373 [20:05<06:22,  4.25s/it, loss=2.46, v_num=16, train_loss_step=2.850]\n",
      "Epoch 0:  76%|███████▌  | 284/373 [20:10<06:17,  4.25s/it, loss=2.46, v_num=16, train_loss_step=2.280]\n",
      "Epoch 0:  76%|███████▋  | 285/373 [20:14<06:13,  4.25s/it, loss=2.42, v_num=16, train_loss_step=1.610]\n",
      "Epoch 0:  77%|███████▋  | 286/373 [20:19<06:09,  4.25s/it, loss=2.43, v_num=16, train_loss_step=2.520]\n",
      "Epoch 0:  77%|███████▋  | 287/373 [20:23<06:05,  4.25s/it, loss=2.46, v_num=16, train_loss_step=2.730]\n",
      "Epoch 0:  77%|███████▋  | 288/373 [20:27<06:01,  4.25s/it, loss=2.47, v_num=16, train_loss_step=2.530]\n",
      "Epoch 0:  77%|███████▋  | 289/373 [20:32<05:56,  4.25s/it, loss=2.45, v_num=16, train_loss_step=2.280]\n",
      "Epoch 0:  78%|███████▊  | 290/373 [20:36<05:52,  4.25s/it, loss=2.4, v_num=16, train_loss_step=1.460] \n",
      "Epoch 0:  78%|███████▊  | 291/373 [20:40<05:48,  4.25s/it, loss=2.38, v_num=16, train_loss_step=2.500]\n",
      "Epoch 0:  78%|███████▊  | 292/373 [20:44<05:44,  4.25s/it, loss=2.38, v_num=16, train_loss_step=2.540]\n",
      "Epoch 0:  79%|███████▊  | 293/373 [20:49<05:39,  4.25s/it, loss=2.37, v_num=16, train_loss_step=2.750]\n",
      "Epoch 0:  79%|███████▉  | 294/373 [20:53<05:35,  4.25s/it, loss=2.35, v_num=16, train_loss_step=2.100]\n",
      "Epoch 0:  79%|███████▉  | 295/373 [20:57<05:31,  4.25s/it, loss=2.36, v_num=16, train_loss_step=2.130]\n",
      "Epoch 0:  79%|███████▉  | 296/373 [21:02<05:27,  4.25s/it, loss=2.34, v_num=16, train_loss_step=2.590]\n",
      "Epoch 0:  80%|███████▉  | 297/373 [21:07<05:23,  4.25s/it, loss=2.32, v_num=16, train_loss_step=1.970]\n",
      "Epoch 0:  80%|███████▉  | 298/373 [21:12<05:19,  4.25s/it, loss=2.28, v_num=16, train_loss_step=2.020]\n",
      "Epoch 0:  80%|████████  | 299/373 [21:16<05:14,  4.26s/it, loss=2.25, v_num=16, train_loss_step=1.530]\n",
      "Epoch 0:  80%|████████  | 300/373 [21:21<05:10,  4.26s/it, loss=2.24, v_num=16, train_loss_step=2.450]\n",
      "Epoch 0:  81%|████████  | 301/373 [21:27<05:06,  4.26s/it, loss=2.29, v_num=16, train_loss_step=2.820]\n",
      "Epoch 0:  81%|████████  | 302/373 [21:31<05:02,  4.26s/it, loss=2.29, v_num=16, train_loss_step=2.070]\n",
      "Epoch 0:  81%|████████  | 303/373 [21:36<04:58,  4.26s/it, loss=2.25, v_num=16, train_loss_step=2.090]\n",
      "Epoch 0:  82%|████████▏ | 304/373 [21:40<04:54,  4.27s/it, loss=2.24, v_num=16, train_loss_step=2.000]\n",
      "Epoch 0:  82%|████████▏ | 305/373 [21:45<04:50,  4.27s/it, loss=2.26, v_num=16, train_loss_step=2.160]\n",
      "Epoch 0:  82%|████████▏ | 306/373 [21:49<04:45,  4.27s/it, loss=2.29, v_num=16, train_loss_step=3.070]\n",
      "Epoch 0:  82%|████████▏ | 307/373 [21:54<04:41,  4.27s/it, loss=2.3, v_num=16, train_loss_step=2.870] \n",
      "Epoch 0:  83%|████████▎ | 308/373 [21:59<04:37,  4.27s/it, loss=2.28, v_num=16, train_loss_step=2.260]\n",
      "Epoch 0:  83%|████████▎ | 309/373 [22:03<04:33,  4.27s/it, loss=2.27, v_num=16, train_loss_step=1.990]\n",
      "Epoch 0:  83%|████████▎ | 310/373 [22:07<04:29,  4.27s/it, loss=2.32, v_num=16, train_loss_step=2.420]\n",
      "Epoch 0:  83%|████████▎ | 311/373 [22:12<04:24,  4.27s/it, loss=2.31, v_num=16, train_loss_step=2.290]\n",
      "Epoch 0:  84%|████████▎ | 312/373 [22:17<04:20,  4.27s/it, loss=2.29, v_num=16, train_loss_step=2.190]\n",
      "Epoch 0:  84%|████████▍ | 313/373 [22:21<04:16,  4.27s/it, loss=2.28, v_num=16, train_loss_step=2.560]\n",
      "Epoch 0:  84%|████████▍ | 314/373 [22:26<04:12,  4.28s/it, loss=2.27, v_num=16, train_loss_step=1.870]\n",
      "Epoch 0:  84%|████████▍ | 315/373 [22:31<04:08,  4.28s/it, loss=2.28, v_num=16, train_loss_step=2.400]\n",
      "Epoch 0:  85%|████████▍ | 316/373 [22:36<04:03,  4.28s/it, loss=2.26, v_num=16, train_loss_step=2.050]\n",
      "Epoch 0:  85%|████████▍ | 317/373 [22:41<03:59,  4.28s/it, loss=2.27, v_num=16, train_loss_step=2.220]\n",
      "Epoch 0:  85%|████████▌ | 318/373 [22:46<03:55,  4.28s/it, loss=2.34, v_num=16, train_loss_step=3.570]\n",
      "Epoch 0:  86%|████████▌ | 319/373 [22:50<03:51,  4.28s/it, loss=2.36, v_num=16, train_loss_step=1.820]\n",
      "Epoch 0:  86%|████████▌ | 320/373 [22:56<03:47,  4.29s/it, loss=2.34, v_num=16, train_loss_step=2.130]\n",
      "Epoch 0:  86%|████████▌ | 321/373 [23:01<03:43,  4.29s/it, loss=2.3, v_num=16, train_loss_step=2.040] \n",
      "Epoch 0:  86%|████████▋ | 322/373 [23:06<03:38,  4.29s/it, loss=2.32, v_num=16, train_loss_step=2.480]\n",
      "Epoch 0:  87%|████████▋ | 323/373 [23:11<03:34,  4.29s/it, loss=2.33, v_num=16, train_loss_step=2.220]\n",
      "Epoch 0:  87%|████████▋ | 324/373 [23:16<03:30,  4.30s/it, loss=2.37, v_num=16, train_loss_step=2.850]\n",
      "Epoch 0:  87%|████████▋ | 325/373 [23:21<03:26,  4.30s/it, loss=2.42, v_num=16, train_loss_step=3.110]\n",
      "Epoch 0:  87%|████████▋ | 326/373 [23:26<03:22,  4.30s/it, loss=2.35, v_num=16, train_loss_step=1.690]\n",
      "Epoch 0:  88%|████████▊ | 327/373 [23:31<03:17,  4.30s/it, loss=2.32, v_num=16, train_loss_step=2.150]\n",
      "Epoch 0:  88%|████████▊ | 328/373 [23:36<03:13,  4.30s/it, loss=2.33, v_num=16, train_loss_step=2.450]\n",
      "Epoch 0:  88%|████████▊ | 329/373 [23:41<03:09,  4.31s/it, loss=2.31, v_num=16, train_loss_step=1.700]\n",
      "Epoch 0:  88%|████████▊ | 330/373 [23:46<03:05,  4.31s/it, loss=2.27, v_num=16, train_loss_step=1.660]\n",
      "Epoch 0:  89%|████████▊ | 331/373 [23:51<03:01,  4.31s/it, loss=2.25, v_num=16, train_loss_step=1.860]\n",
      "Epoch 0:  89%|████████▉ | 332/373 [23:56<02:56,  4.31s/it, loss=2.22, v_num=16, train_loss_step=1.590]\n",
      "Epoch 0:  89%|████████▉ | 333/373 [24:01<02:52,  4.32s/it, loss=2.18, v_num=16, train_loss_step=1.730]\n",
      "Epoch 0:  90%|████████▉ | 334/373 [24:06<02:48,  4.32s/it, loss=2.17, v_num=16, train_loss_step=1.770]\n",
      "Epoch 0:  90%|████████▉ | 335/373 [24:11<02:44,  4.32s/it, loss=2.17, v_num=16, train_loss_step=2.210]\n",
      "Epoch 0:  90%|█████████ | 336/373 [24:15<02:39,  4.32s/it, loss=2.19, v_num=16, train_loss_step=2.530]\n",
      "Epoch 0:  90%|█████████ | 337/373 [24:20<02:35,  4.32s/it, loss=2.2, v_num=16, train_loss_step=2.500] \n",
      "Epoch 0:  91%|█████████ | 338/373 [24:24<02:31,  4.32s/it, loss=2.12, v_num=16, train_loss_step=1.910]\n",
      "Epoch 0:  91%|█████████ | 339/373 [24:29<02:26,  4.32s/it, loss=2.14, v_num=16, train_loss_step=2.270]\n",
      "Epoch 0:  91%|█████████ | 340/373 [24:33<02:22,  4.32s/it, loss=2.17, v_num=16, train_loss_step=2.730]\n",
      "Epoch 0:  91%|█████████▏| 341/373 [24:38<02:18,  4.32s/it, loss=2.17, v_num=16, train_loss_step=1.910]\n",
      "Epoch 0:  92%|█████████▏| 342/373 [24:42<02:14,  4.32s/it, loss=2.16, v_num=16, train_loss_step=2.290]\n",
      "Epoch 0:  92%|█████████▏| 343/373 [24:47<02:09,  4.32s/it, loss=2.17, v_num=16, train_loss_step=2.520]\n",
      "Epoch 0:  92%|█████████▏| 344/373 [24:51<02:05,  4.32s/it, loss=2.14, v_num=16, train_loss_step=2.250]\n",
      "Epoch 0:  92%|█████████▏| 345/373 [24:56<02:01,  4.32s/it, loss=2.07, v_num=16, train_loss_step=1.650]\n",
      "Epoch 0:  93%|█████████▎| 346/373 [25:01<01:56,  4.33s/it, loss=2.12, v_num=16, train_loss_step=2.790]\n",
      "Epoch 0:  93%|█████████▎| 347/373 [25:05<01:52,  4.33s/it, loss=2.12, v_num=16, train_loss_step=1.990]\n",
      "Epoch 0:  93%|█████████▎| 348/373 [25:09<01:48,  4.33s/it, loss=2.1, v_num=16, train_loss_step=2.230] \n",
      "Epoch 0:  94%|█████████▎| 349/373 [25:13<01:43,  4.33s/it, loss=2.14, v_num=16, train_loss_step=2.330]\n",
      "Epoch 0:  94%|█████████▍| 350/373 [25:17<01:39,  4.32s/it, loss=2.18, v_num=16, train_loss_step=2.520]\n",
      "Epoch 0:  94%|█████████▍| 351/373 [25:23<01:35,  4.33s/it, loss=2.23, v_num=16, train_loss_step=2.860]\n",
      "Epoch 0:  94%|█████████▍| 352/373 [25:27<01:30,  4.33s/it, loss=2.28, v_num=16, train_loss_step=2.630]\n",
      "Epoch 0:  95%|█████████▍| 353/373 [25:32<01:26,  4.33s/it, loss=2.29, v_num=16, train_loss_step=1.970]\n",
      "Epoch 0:  95%|█████████▍| 354/373 [25:36<01:22,  4.33s/it, loss=2.38, v_num=16, train_loss_step=3.490]\n",
      "Epoch 0:  95%|█████████▌| 355/373 [25:40<01:17,  4.33s/it, loss=2.38, v_num=16, train_loss_step=2.160]\n",
      "Epoch 0:  95%|█████████▌| 356/373 [25:45<01:13,  4.33s/it, loss=2.37, v_num=16, train_loss_step=2.330]\n",
      "Epoch 0:  96%|█████████▌| 357/373 [25:49<01:09,  4.33s/it, loss=2.41, v_num=16, train_loss_step=3.360]\n",
      "Epoch 0:  96%|█████████▌| 358/373 [25:54<01:04,  4.33s/it, loss=2.42, v_num=16, train_loss_step=2.160]\n",
      "Epoch 0:  96%|█████████▌| 359/373 [25:58<01:00,  4.33s/it, loss=2.39, v_num=16, train_loss_step=1.700]\n",
      "Epoch 0:  97%|█████████▋| 360/373 [26:02<00:56,  4.33s/it, loss=2.37, v_num=16, train_loss_step=2.260]\n",
      "Epoch 0:  97%|█████████▋| 361/373 [26:07<00:51,  4.33s/it, loss=2.39, v_num=16, train_loss_step=2.240]\n",
      "Epoch 0:  97%|█████████▋| 362/373 [26:12<00:47,  4.33s/it, loss=2.39, v_num=16, train_loss_step=2.240]\n",
      "Epoch 0:  97%|█████████▋| 362/373 [26:12<00:47,  4.33s/it, loss=2.39, v_num=16, train_loss_step=2.300]\n",
      "Epoch 0:  97%|█████████▋| 363/373 [26:17<00:43,  4.33s/it, loss=2.38, v_num=16, train_loss_step=2.320]\n",
      "Epoch 0:  98%|█████████▊| 364/373 [26:21<00:39,  4.33s/it, loss=2.39, v_num=16, train_loss_step=2.490]\n",
      "Epoch 0:  98%|█████████▊| 365/373 [26:26<00:34,  4.33s/it, loss=2.44, v_num=16, train_loss_step=2.660]\n",
      "Epoch 0:  98%|█████████▊| 366/373 [26:30<00:30,  4.33s/it, loss=2.45, v_num=16, train_loss_step=2.940]\n",
      "Epoch 0:  98%|█████████▊| 367/373 [26:35<00:26,  4.33s/it, loss=2.43, v_num=16, train_loss_step=1.620]\n",
      "Epoch 0:  99%|█████████▊| 368/373 [26:39<00:21,  4.33s/it, loss=2.44, v_num=16, train_loss_step=2.490]\n",
      "Epoch 0:  99%|█████████▉| 369/373 [26:43<00:17,  4.33s/it, loss=2.43, v_num=16, train_loss_step=2.080]\n",
      "Epoch 0:  99%|█████████▉| 370/373 [26:47<00:13,  4.33s/it, loss=2.43, v_num=16, train_loss_step=2.590]\n",
      "Epoch 0:  99%|█████████▉| 371/373 [26:52<00:08,  4.33s/it, loss=2.42, v_num=16, train_loss_step=2.600]\n",
      "Epoch 0: 100%|█████████▉| 372/373 [26:56<00:04,  4.34s/it, loss=2.41, v_num=16, train_loss_step=2.350]\n",
      "\u001b[2m\u001b[36m(RayExecutor pid=2930)\u001b[0m \n",
      "Validating: 0it [00:00, ?it/s]\u001b[A0m \n",
      "Validating:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(RayExecutor pid=2931)\u001b[0m /Users/christy/mambaforge/envs/ray/lib/python3.8/site-packages/pytorch_forecasting/models/temporal_fusion_transformer/__init__.py:781: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "\u001b[2m\u001b[36m(RayExecutor pid=2931)\u001b[0m   fig, ax = plt.subplots()\n",
      "\u001b[2m\u001b[36m(RayExecutor pid=2933)\u001b[0m /Users/christy/mambaforge/envs/ray/lib/python3.8/site-packages/pytorch_forecasting/models/temporal_fusion_transformer/__init__.py:781: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "\u001b[2m\u001b[36m(RayExecutor pid=2933)\u001b[0m   fig, ax = plt.subplots()\n",
      "\u001b[2m\u001b[36m(RayExecutor pid=2932)\u001b[0m /Users/christy/mambaforge/envs/ray/lib/python3.8/site-packages/pytorch_forecasting/models/temporal_fusion_transformer/__init__.py:781: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "\u001b[2m\u001b[36m(RayExecutor pid=2932)\u001b[0m   fig, ax = plt.subplots()\n",
      "\u001b[2m\u001b[36m(RayExecutor pid=2935)\u001b[0m /Users/christy/mambaforge/envs/ray/lib/python3.8/site-packages/pytorch_forecasting/models/temporal_fusion_transformer/__init__.py:781: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "\u001b[2m\u001b[36m(RayExecutor pid=2935)\u001b[0m   fig, ax = plt.subplots()\n",
      "\u001b[2m\u001b[36m(RayExecutor pid=2936)\u001b[0m /Users/christy/mambaforge/envs/ray/lib/python3.8/site-packages/pytorch_forecasting/models/temporal_fusion_transformer/__init__.py:781: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "\u001b[2m\u001b[36m(RayExecutor pid=2936)\u001b[0m   fig, ax = plt.subplots()\n",
      "\u001b[2m\u001b[36m(RayExecutor pid=2937)\u001b[0m /Users/christy/mambaforge/envs/ray/lib/python3.8/site-packages/pytorch_forecasting/models/temporal_fusion_transformer/__init__.py:781: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "\u001b[2m\u001b[36m(RayExecutor pid=2937)\u001b[0m   fig, ax = plt.subplots()\n",
      "\u001b[2m\u001b[36m(RayExecutor pid=2934)\u001b[0m /Users/christy/mambaforge/envs/ray/lib/python3.8/site-packages/pytorch_forecasting/models/temporal_fusion_transformer/__init__.py:781: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "\u001b[2m\u001b[36m(RayExecutor pid=2934)\u001b[0m   fig, ax = plt.subplots()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(RayExecutor pid=2930)\u001b[0m \n",
      "Validating: 100%|██████████| 1/1 [03:16<00:00, 196.91s/it]\u001b[A\n",
      "Epoch 0: 100%|██████████| 373/373 [30:15<00:00,  4.86s/it, loss=2.41, v_num=16, train_loss_step=2.350, val_loss=1.990]\n",
      "                                                          \u001b[A\n",
      "Epoch 1:   0%|          | 0/373 [00:00<00:00, 4505.16it/s, loss=2.41, v_num=16, train_loss_step=2.350, val_loss=1.990] \n",
      "Epoch 1:   0%|          | 1/373 [01:14<3:51:03, 37.27s/it, loss=2.45, v_num=16, train_loss_step=2.810, val_loss=1.990, train_loss_epoch=3.400]\n",
      "Epoch 1:   1%|          | 2/373 [01:18<2:41:39, 26.14s/it, loss=2.45, v_num=16, train_loss_step=2.810, val_loss=1.990, train_loss_epoch=3.400]\n",
      "Epoch 1:   1%|          | 2/373 [01:18<2:41:39, 26.14s/it, loss=2.39, v_num=16, train_loss_step=2.370, val_loss=1.990, train_loss_epoch=3.400]\n",
      "Epoch 1:   1%|          | 3/373 [01:22<2:06:30, 20.52s/it, loss=2.36, v_num=16, train_loss_step=1.500, val_loss=1.990, train_loss_epoch=3.400]\n",
      "Epoch 1:   1%|          | 4/373 [01:25<1:45:06, 17.09s/it, loss=2.31, v_num=16, train_loss_step=1.380, val_loss=1.990, train_loss_epoch=3.400]\n",
      "Epoch 1:   1%|▏         | 5/373 [01:28<1:30:44, 14.79s/it, loss=2.23, v_num=16, train_loss_step=1.830, val_loss=1.990, train_loss_epoch=3.400]\n",
      "Epoch 1:   2%|▏         | 6/373 [01:32<1:20:35, 13.18s/it, loss=2.26, v_num=16, train_loss_step=2.720, val_loss=1.990, train_loss_epoch=3.400]\n",
      "Epoch 1:   2%|▏         | 7/373 [01:35<1:12:54, 11.95s/it, loss=2.27, v_num=16, train_loss_step=1.940, val_loss=1.990, train_loss_epoch=3.400]\n",
      "Epoch 1:   2%|▏         | 8/373 [01:39<1:06:58, 11.01s/it, loss=2.28, v_num=16, train_loss_step=2.440, val_loss=1.990, train_loss_epoch=3.400]\n",
      "Epoch 1:   2%|▏         | 9/373 [01:42<1:02:19, 10.27s/it, loss=2.27, v_num=16, train_loss_step=1.980, val_loss=1.990, train_loss_epoch=3.400]\n",
      "Epoch 1:   3%|▎         | 10/373 [01:46<58:27,  9.66s/it, loss=2.29, v_num=16, train_loss_step=2.700, val_loss=1.990, train_loss_epoch=3.400] \n",
      "Epoch 1:   3%|▎         | 11/373 [01:49<55:12,  9.15s/it, loss=2.31, v_num=16, train_loss_step=2.720, val_loss=1.990, train_loss_epoch=3.400]\n",
      "Epoch 1:   3%|▎         | 12/373 [01:53<52:28,  8.72s/it, loss=2.29, v_num=16, train_loss_step=2.150, val_loss=1.990, train_loss_epoch=3.400]\n",
      "Epoch 1:   3%|▎         | 13/373 [01:56<50:05,  8.35s/it, loss=2.32, v_num=16, train_loss_step=3.220, val_loss=1.990, train_loss_epoch=3.400]\n",
      "Epoch 1:   4%|▍         | 14/373 [02:00<47:59,  8.02s/it, loss=2.27, v_num=16, train_loss_step=1.900, val_loss=1.990, train_loss_epoch=3.400]\n",
      "Epoch 1:   4%|▍         | 15/373 [02:03<46:09,  7.73s/it, loss=2.31, v_num=16, train_loss_step=2.420, val_loss=1.990, train_loss_epoch=3.400]\n",
      "Epoch 1:   4%|▍         | 16/373 [02:07<44:29,  7.48s/it, loss=2.31, v_num=16, train_loss_step=2.450, val_loss=1.990, train_loss_epoch=3.400]\n",
      "Epoch 1:   5%|▍         | 17/373 [02:10<42:58,  7.24s/it, loss=2.29, v_num=16, train_loss_step=1.720, val_loss=1.990, train_loss_epoch=3.400]\n",
      "Epoch 1:   5%|▍         | 18/373 [02:13<41:39,  7.04s/it, loss=2.26, v_num=16, train_loss_step=1.980, val_loss=1.990, train_loss_epoch=3.400]\n",
      "Epoch 1:   5%|▌         | 19/373 [02:17<40:26,  6.86s/it, loss=2.24, v_num=16, train_loss_step=2.240, val_loss=1.990, train_loss_epoch=3.400]\n",
      "Epoch 1:   5%|▌         | 20/373 [02:20<39:19,  6.68s/it, loss=2.2, v_num=16, train_loss_step=1.600, val_loss=1.990, train_loss_epoch=3.400] \n",
      "Epoch 1:   6%|▌         | 21/373 [02:23<38:18,  6.53s/it, loss=2.17, v_num=16, train_loss_step=2.170, val_loss=1.990, train_loss_epoch=3.400]\n",
      "Epoch 1:   6%|▌         | 22/373 [02:27<37:24,  6.39s/it, loss=2.16, v_num=16, train_loss_step=2.070, val_loss=1.990, train_loss_epoch=3.400]\n",
      "Epoch 1:   6%|▌         | 23/373 [02:30<36:33,  6.27s/it, loss=2.19, v_num=16, train_loss_step=2.220, val_loss=1.990, train_loss_epoch=3.400]\n",
      "Epoch 1:   6%|▋         | 24/373 [02:33<35:45,  6.15s/it, loss=2.23, v_num=16, train_loss_step=2.100, val_loss=1.990, train_loss_epoch=3.400]\n",
      "Epoch 1:   7%|▋         | 25/373 [02:36<35:00,  6.04s/it, loss=2.27, v_num=16, train_loss_step=2.580, val_loss=1.990, train_loss_epoch=3.400]\n",
      "Epoch 1:   7%|▋         | 26/373 [02:40<34:19,  5.93s/it, loss=2.24, v_num=16, train_loss_step=2.160, val_loss=1.990, train_loss_epoch=3.400]\n",
      "Epoch 1:   7%|▋         | 27/373 [02:43<33:41,  5.84s/it, loss=2.22, v_num=16, train_loss_step=1.600, val_loss=1.990, train_loss_epoch=3.400]\n",
      "Epoch 1:   8%|▊         | 28/373 [02:46<33:05,  5.76s/it, loss=2.18, v_num=16, train_loss_step=1.710, val_loss=1.990, train_loss_epoch=3.400]\n",
      "Epoch 1:   8%|▊         | 29/373 [02:50<32:31,  5.67s/it, loss=2.2, v_num=16, train_loss_step=2.290, val_loss=1.990, train_loss_epoch=3.400] \n",
      "Epoch 1:   8%|▊         | 30/373 [02:53<31:59,  5.60s/it, loss=2.19, v_num=16, train_loss_step=2.430, val_loss=1.990, train_loss_epoch=3.400]\n",
      "Epoch 1:   8%|▊         | 31/373 [02:56<31:29,  5.52s/it, loss=2.15, v_num=16, train_loss_step=2.030, val_loss=1.990, train_loss_epoch=3.400]\n",
      "Epoch 1:   9%|▊         | 32/373 [03:00<31:01,  5.46s/it, loss=2.14, v_num=16, train_loss_step=1.960, val_loss=1.990, train_loss_epoch=3.400]\n",
      "Epoch 1:   9%|▉         | 33/373 [03:03<30:35,  5.40s/it, loss=2.08, v_num=16, train_loss_step=1.970, val_loss=1.990, train_loss_epoch=3.400]\n",
      "Epoch 1:   9%|▉         | 34/373 [03:06<30:10,  5.34s/it, loss=2.07, v_num=16, train_loss_step=1.640, val_loss=1.990, train_loss_epoch=3.400]\n",
      "Epoch 1:   9%|▉         | 35/373 [03:10<29:45,  5.28s/it, loss=2.02, v_num=16, train_loss_step=1.520, val_loss=1.990, train_loss_epoch=3.400]\n",
      "Epoch 1:  10%|▉         | 36/373 [03:13<29:22,  5.23s/it, loss=2.05, v_num=16, train_loss_step=3.020, val_loss=1.990, train_loss_epoch=3.400]\n",
      "Epoch 1:  10%|▉         | 37/373 [03:16<29:00,  5.18s/it, loss=2.08, v_num=16, train_loss_step=2.240, val_loss=1.990, train_loss_epoch=3.400]\n",
      "Epoch 1:  10%|█         | 38/373 [03:20<28:39,  5.13s/it, loss=2.1, v_num=16, train_loss_step=2.450, val_loss=1.990, train_loss_epoch=3.400] \n",
      "Epoch 1:  10%|█         | 39/373 [03:23<28:19,  5.09s/it, loss=2.11, v_num=16, train_loss_step=2.460, val_loss=1.990, train_loss_epoch=3.400]\n",
      "Epoch 1:  11%|█         | 40/373 [03:27<28:02,  5.05s/it, loss=2.15, v_num=16, train_loss_step=2.310, val_loss=1.990, train_loss_epoch=3.400]\n",
      "Epoch 1:  11%|█         | 41/373 [03:30<27:44,  5.01s/it, loss=2.11, v_num=16, train_loss_step=1.440, val_loss=1.990, train_loss_epoch=3.400]\n",
      "Epoch 1:  11%|█▏        | 42/373 [03:33<27:26,  4.97s/it, loss=2.12, v_num=16, train_loss_step=2.180, val_loss=1.990, train_loss_epoch=3.400]\n",
      "Epoch 1:  12%|█▏        | 43/373 [03:37<27:08,  4.94s/it, loss=2.1, v_num=16, train_loss_step=1.810, val_loss=1.990, train_loss_epoch=3.400] \n",
      "Epoch 1:  12%|█▏        | 44/373 [03:40<26:52,  4.90s/it, loss=2.1, v_num=16, train_loss_step=2.150, val_loss=1.990, train_loss_epoch=3.400]\n",
      "Epoch 1:  12%|█▏        | 45/373 [03:43<26:36,  4.87s/it, loss=2.08, v_num=16, train_loss_step=2.300, val_loss=1.990, train_loss_epoch=3.400]\n",
      "Epoch 1:  12%|█▏        | 46/373 [03:47<26:21,  4.84s/it, loss=2.08, v_num=16, train_loss_step=2.030, val_loss=1.990, train_loss_epoch=3.400]\n",
      "Epoch 1:  13%|█▎        | 47/373 [03:50<26:07,  4.81s/it, loss=2.11, v_num=16, train_loss_step=2.280, val_loss=1.990, train_loss_epoch=3.400]\n",
      "Epoch 1:  13%|█▎        | 48/373 [03:54<25:52,  4.78s/it, loss=2.1, v_num=16, train_loss_step=1.540, val_loss=1.990, train_loss_epoch=3.400] \n",
      "Epoch 1:  13%|█▎        | 49/373 [03:57<25:38,  4.75s/it, loss=2.1, v_num=16, train_loss_step=2.260, val_loss=1.990, train_loss_epoch=3.400]\n",
      "Epoch 1:  13%|█▎        | 50/373 [04:00<25:23,  4.72s/it, loss=2.08, v_num=16, train_loss_step=2.010, val_loss=1.990, train_loss_epoch=3.400]\n",
      "Epoch 1:  14%|█▎        | 51/373 [04:04<25:12,  4.70s/it, loss=2.1, v_num=16, train_loss_step=2.390, val_loss=1.990, train_loss_epoch=3.400] \n",
      "Epoch 1:  14%|█▍        | 52/373 [04:07<25:00,  4.68s/it, loss=2.11, v_num=16, train_loss_step=2.230, val_loss=1.990, train_loss_epoch=3.400]\n",
      "Epoch 1:  14%|█▍        | 53/373 [04:11<24:48,  4.65s/it, loss=2.13, v_num=16, train_loss_step=2.360, val_loss=1.990, train_loss_epoch=3.400]\n",
      "Epoch 1:  14%|█▍        | 54/373 [04:14<24:36,  4.63s/it, loss=2.16, v_num=16, train_loss_step=2.110, val_loss=1.990, train_loss_epoch=3.400]\n",
      "Epoch 1:  15%|█▍        | 55/373 [04:18<24:25,  4.61s/it, loss=2.16, v_num=16, train_loss_step=2.110, val_loss=1.990, train_loss_epoch=3.400]\n",
      "Epoch 1:  15%|█▍        | 55/373 [04:18<24:25,  4.61s/it, loss=2.15, v_num=16, train_loss_step=1.500, val_loss=1.990, train_loss_epoch=3.400]\n",
      "Epoch 1:  15%|█▌        | 56/373 [04:21<24:13,  4.58s/it, loss=2.09, v_num=16, train_loss_step=1.740, val_loss=1.990, train_loss_epoch=3.400]\n",
      "Epoch 1:  15%|█▌        | 57/373 [04:24<24:02,  4.56s/it, loss=2.08, v_num=16, train_loss_step=1.970, val_loss=1.990, train_loss_epoch=3.400]\n",
      "Epoch 1:  16%|█▌        | 58/373 [04:28<23:51,  4.54s/it, loss=2.06, v_num=16, train_loss_step=2.100, val_loss=1.990, train_loss_epoch=3.400]\n",
      "Epoch 1:  16%|█▌        | 59/373 [04:31<23:41,  4.53s/it, loss=2.07, v_num=16, train_loss_step=2.670, val_loss=1.990, train_loss_epoch=3.400]\n",
      "Epoch 1:  16%|█▌        | 60/373 [04:35<23:32,  4.51s/it, loss=2.07, v_num=16, train_loss_step=2.340, val_loss=1.990, train_loss_epoch=3.400]\n",
      "Epoch 1:  16%|█▋        | 61/373 [04:38<23:23,  4.50s/it, loss=2.11, v_num=16, train_loss_step=2.270, val_loss=1.990, train_loss_epoch=3.400]\n",
      "Epoch 1:  17%|█▋        | 62/373 [04:42<23:15,  4.49s/it, loss=2.11, v_num=16, train_loss_step=2.160, val_loss=1.990, train_loss_epoch=3.400]\n",
      "Epoch 1:  17%|█▋        | 63/373 [04:46<23:06,  4.47s/it, loss=2.13, v_num=16, train_loss_step=2.180, val_loss=1.990, train_loss_epoch=3.400]\n",
      "Epoch 1:  17%|█▋        | 64/373 [04:49<22:57,  4.46s/it, loss=2.11, v_num=16, train_loss_step=1.740, val_loss=1.990, train_loss_epoch=3.400]\n",
      "Epoch 1:  17%|█▋        | 65/373 [04:53<22:48,  4.44s/it, loss=2.13, v_num=16, train_loss_step=2.750, val_loss=1.990, train_loss_epoch=3.400]\n",
      "Epoch 1:  18%|█▊        | 66/373 [04:56<22:40,  4.43s/it, loss=2.15, v_num=16, train_loss_step=2.480, val_loss=1.990, train_loss_epoch=3.400]\n",
      "Epoch 1:  18%|█▊        | 67/373 [05:00<22:31,  4.42s/it, loss=2.14, v_num=16, train_loss_step=1.990, val_loss=1.990, train_loss_epoch=3.400]\n",
      "Epoch 1:  18%|█▊        | 68/373 [05:03<22:22,  4.40s/it, loss=2.19, v_num=16, train_loss_step=2.520, val_loss=1.990, train_loss_epoch=3.400]\n",
      "Epoch 1:  18%|█▊        | 69/373 [05:07<22:14,  4.39s/it, loss=2.18, v_num=16, train_loss_step=2.120, val_loss=1.990, train_loss_epoch=3.400]\n",
      "Epoch 1:  19%|█▉        | 70/373 [05:11<22:07,  4.38s/it, loss=2.17, v_num=16, train_loss_step=1.830, val_loss=1.990, train_loss_epoch=3.400]\n",
      "Epoch 1:  19%|█▉        | 71/373 [05:14<22:00,  4.37s/it, loss=2.13, v_num=16, train_loss_step=1.570, val_loss=1.990, train_loss_epoch=3.400]\n",
      "Epoch 1:  19%|█▉        | 72/373 [05:18<21:52,  4.36s/it, loss=2.14, v_num=16, train_loss_step=2.450, val_loss=1.990, train_loss_epoch=3.400]\n",
      "Epoch 1:  20%|█▉        | 73/373 [05:21<21:44,  4.35s/it, loss=2.13, v_num=16, train_loss_step=2.130, val_loss=1.990, train_loss_epoch=3.400]\n",
      "Epoch 1:  20%|█▉        | 74/373 [05:25<21:37,  4.34s/it, loss=2.11, v_num=16, train_loss_step=1.730, val_loss=1.990, train_loss_epoch=3.400]\n",
      "Epoch 1:  20%|██        | 75/373 [05:28<21:29,  4.33s/it, loss=2.18, v_num=16, train_loss_step=2.790, val_loss=1.990, train_loss_epoch=3.400]\n",
      "Epoch 1:  20%|██        | 76/373 [05:32<21:21,  4.32s/it, loss=2.21, v_num=16, train_loss_step=2.410, val_loss=1.990, train_loss_epoch=3.400]\n",
      "Epoch 1:  21%|██        | 77/373 [05:35<21:14,  4.30s/it, loss=2.22, v_num=16, train_loss_step=2.120, val_loss=1.990, train_loss_epoch=3.400]\n",
      "Epoch 1:  21%|██        | 78/373 [05:39<21:07,  4.30s/it, loss=2.25, v_num=16, train_loss_step=2.690, val_loss=1.990, train_loss_epoch=3.400]\n",
      "Epoch 1:  21%|██        | 79/373 [05:43<21:00,  4.29s/it, loss=2.18, v_num=16, train_loss_step=1.370, val_loss=1.990, train_loss_epoch=3.400]\n",
      "Epoch 1:  21%|██▏       | 80/373 [05:46<20:53,  4.28s/it, loss=2.18, v_num=16, train_loss_step=2.300, val_loss=1.990, train_loss_epoch=3.400]\n",
      "Epoch 1:  22%|██▏       | 81/373 [05:50<20:46,  4.27s/it, loss=2.18, v_num=16, train_loss_step=2.240, val_loss=1.990, train_loss_epoch=3.400]\n",
      "Epoch 1:  22%|██▏       | 82/373 [05:53<20:39,  4.26s/it, loss=2.18, v_num=16, train_loss_step=2.130, val_loss=1.990, train_loss_epoch=3.400]\n",
      "Epoch 1:  22%|██▏       | 83/373 [05:56<20:32,  4.25s/it, loss=2.14, v_num=16, train_loss_step=1.470, val_loss=1.990, train_loss_epoch=3.400]\n",
      "Epoch 1:  23%|██▎       | 84/373 [06:00<20:25,  4.24s/it, loss=2.14, v_num=16, train_loss_step=1.720, val_loss=1.990, train_loss_epoch=3.400]\n",
      "Epoch 1:  23%|██▎       | 85/373 [06:04<20:19,  4.24s/it, loss=2.12, v_num=16, train_loss_step=2.260, val_loss=1.990, train_loss_epoch=3.400]\n",
      "Epoch 1:  23%|██▎       | 86/373 [06:07<20:12,  4.23s/it, loss=2.09, v_num=16, train_loss_step=1.990, val_loss=1.990, train_loss_epoch=3.400]\n",
      "Epoch 1:  23%|██▎       | 87/373 [06:11<20:06,  4.22s/it, loss=2.1, v_num=16, train_loss_step=2.250, val_loss=1.990, train_loss_epoch=3.400] \n",
      "Epoch 1:  24%|██▎       | 88/373 [06:14<20:00,  4.21s/it, loss=2.09, v_num=16, train_loss_step=2.260, val_loss=1.990, train_loss_epoch=3.400]\n",
      "Epoch 1:  24%|██▍       | 89/373 [06:18<19:53,  4.20s/it, loss=2.09, v_num=16, train_loss_step=2.120, val_loss=1.990, train_loss_epoch=3.400]\n",
      "Epoch 1:  24%|██▍       | 90/373 [06:21<19:47,  4.20s/it, loss=2.1, v_num=16, train_loss_step=1.930, val_loss=1.990, train_loss_epoch=3.400] \n",
      "Epoch 1:  24%|██▍       | 91/373 [06:25<19:41,  4.19s/it, loss=2.11, v_num=16, train_loss_step=1.940, val_loss=1.990, train_loss_epoch=3.400]\n",
      "Epoch 1:  25%|██▍       | 92/373 [06:28<19:35,  4.18s/it, loss=2.11, v_num=16, train_loss_step=2.270, val_loss=1.990, train_loss_epoch=3.400]\n",
      "Epoch 1:  25%|██▍       | 93/373 [06:32<19:28,  4.17s/it, loss=2.14, v_num=16, train_loss_step=2.910, val_loss=1.990, train_loss_epoch=3.400]\n",
      "Epoch 1:  25%|██▌       | 94/373 [06:35<19:22,  4.17s/it, loss=2.17, v_num=16, train_loss_step=2.170, val_loss=1.990, train_loss_epoch=3.400]\n",
      "Epoch 1:  25%|██▌       | 95/373 [06:39<19:16,  4.16s/it, loss=2.13, v_num=16, train_loss_step=2.040, val_loss=1.990, train_loss_epoch=3.400]\n",
      "Epoch 1:  26%|██▌       | 96/373 [06:42<19:10,  4.15s/it, loss=2.13, v_num=16, train_loss_step=2.330, val_loss=1.990, train_loss_epoch=3.400]\n",
      "Epoch 1:  26%|██▌       | 97/373 [06:46<19:04,  4.15s/it, loss=2.13, v_num=16, train_loss_step=2.120, val_loss=1.990, train_loss_epoch=3.400]\n",
      "Epoch 1:  26%|██▋       | 98/373 [06:50<18:59,  4.14s/it, loss=2.11, v_num=16, train_loss_step=2.400, val_loss=1.990, train_loss_epoch=3.400]\n",
      "Epoch 1:  27%|██▋       | 99/373 [06:53<18:53,  4.14s/it, loss=2.12, v_num=16, train_loss_step=1.650, val_loss=1.990, train_loss_epoch=3.400]\n",
      "Epoch 1:  27%|██▋       | 100/373 [06:57<18:48,  4.13s/it, loss=2.14, v_num=16, train_loss_step=2.650, val_loss=1.990, train_loss_epoch=3.400]\n",
      "Epoch 1:  27%|██▋       | 101/373 [07:01<18:43,  4.13s/it, loss=2.13, v_num=16, train_loss_step=2.030, val_loss=1.990, train_loss_epoch=3.400]\n",
      "Epoch 1:  27%|██▋       | 102/373 [07:04<18:37,  4.12s/it, loss=2.13, v_num=16, train_loss_step=2.030, val_loss=1.990, train_loss_epoch=3.400]\n",
      "Epoch 1:  27%|██▋       | 102/373 [07:04<18:37,  4.12s/it, loss=2.11, v_num=16, train_loss_step=1.630, val_loss=1.990, train_loss_epoch=3.400]\n",
      "Epoch 1:  28%|██▊       | 103/373 [07:08<18:32,  4.12s/it, loss=2.15, v_num=16, train_loss_step=2.380, val_loss=1.990, train_loss_epoch=3.400]\n",
      "Epoch 1:  28%|██▊       | 104/373 [07:12<18:26,  4.11s/it, loss=2.23, v_num=16, train_loss_step=3.350, val_loss=1.990, train_loss_epoch=3.400]\n",
      "Epoch 1:  28%|██▊       | 105/373 [07:15<18:21,  4.11s/it, loss=2.22, v_num=16, train_loss_step=1.990, val_loss=1.990, train_loss_epoch=3.400]\n",
      "Epoch 1:  28%|██▊       | 106/373 [07:19<18:16,  4.11s/it, loss=2.2, v_num=16, train_loss_step=1.630, val_loss=1.990, train_loss_epoch=3.400] \n",
      "Epoch 1:  29%|██▊       | 107/373 [07:22<18:09,  4.10s/it, loss=2.24, v_num=16, train_loss_step=3.050, val_loss=1.990, train_loss_epoch=3.400]\n",
      "Epoch 1:  29%|██▉       | 108/373 [07:25<18:04,  4.09s/it, loss=2.21, v_num=16, train_loss_step=1.540, val_loss=1.990, train_loss_epoch=3.400]\n",
      "Epoch 1:  29%|██▉       | 109/373 [07:29<17:58,  4.09s/it, loss=2.17, v_num=16, train_loss_step=1.320, val_loss=1.990, train_loss_epoch=3.400]\n",
      "Epoch 1:  29%|██▉       | 110/373 [07:33<17:53,  4.08s/it, loss=2.18, v_num=16, train_loss_step=2.250, val_loss=1.990, train_loss_epoch=3.400]\n",
      "Epoch 1:  30%|██▉       | 111/373 [07:36<17:48,  4.08s/it, loss=2.21, v_num=16, train_loss_step=2.450, val_loss=1.990, train_loss_epoch=3.400]\n",
      "Epoch 1:  30%|███       | 112/373 [07:40<17:42,  4.07s/it, loss=2.18, v_num=16, train_loss_step=1.690, val_loss=1.990, train_loss_epoch=3.400]\n",
      "Epoch 1:  30%|███       | 113/373 [07:43<17:37,  4.07s/it, loss=2.14, v_num=16, train_loss_step=2.040, val_loss=1.990, train_loss_epoch=3.400]\n",
      "Epoch 1:  31%|███       | 114/373 [07:46<17:31,  4.06s/it, loss=2.1, v_num=16, train_loss_step=1.500, val_loss=1.990, train_loss_epoch=3.400] \n",
      "Epoch 1:  31%|███       | 115/373 [07:50<17:26,  4.05s/it, loss=2.07, v_num=16, train_loss_step=1.430, val_loss=1.990, train_loss_epoch=3.400]\n",
      "Epoch 1:  31%|███       | 116/373 [07:53<17:20,  4.05s/it, loss=2.07, v_num=16, train_loss_step=2.210, val_loss=1.990, train_loss_epoch=3.400]\n",
      "Epoch 1:  31%|███▏      | 117/373 [07:57<17:15,  4.05s/it, loss=2.05, v_num=16, train_loss_step=1.840, val_loss=1.990, train_loss_epoch=3.400]\n",
      "Epoch 1:  32%|███▏      | 118/373 [08:00<17:10,  4.04s/it, loss=2.05, v_num=16, train_loss_step=2.340, val_loss=1.990, train_loss_epoch=3.400]\n",
      "Epoch 1:  32%|███▏      | 119/373 [08:04<17:05,  4.04s/it, loss=2.05, v_num=16, train_loss_step=1.720, val_loss=1.990, train_loss_epoch=3.400]\n",
      "Epoch 1:  32%|███▏      | 120/373 [08:08<17:02,  4.04s/it, loss=2.05, v_num=16, train_loss_step=2.690, val_loss=1.990, train_loss_epoch=3.400]\n",
      "Epoch 1:  32%|███▏      | 121/373 [08:12<16:57,  4.04s/it, loss=2.04, v_num=16, train_loss_step=1.810, val_loss=1.990, train_loss_epoch=3.400]\n",
      "Epoch 1:  33%|███▎      | 122/373 [08:16<16:53,  4.04s/it, loss=2.07, v_num=16, train_loss_step=2.200, val_loss=1.990, train_loss_epoch=3.400]\n",
      "Epoch 1:  33%|███▎      | 123/373 [08:20<16:48,  4.03s/it, loss=2.05, v_num=16, train_loss_step=1.990, val_loss=1.990, train_loss_epoch=3.400]\n",
      "Epoch 1:  33%|███▎      | 124/373 [08:23<16:43,  4.03s/it, loss=2, v_num=16, train_loss_step=2.240, val_loss=1.990, train_loss_epoch=3.400]   \n",
      "Epoch 1:  34%|███▎      | 125/373 [08:27<16:38,  4.02s/it, loss=2.03, v_num=16, train_loss_step=2.560, val_loss=1.990, train_loss_epoch=3.400]\n",
      "Epoch 1:  34%|███▍      | 126/373 [08:30<16:32,  4.02s/it, loss=2.03, v_num=16, train_loss_step=2.560, val_loss=1.990, train_loss_epoch=3.400]\n",
      "Epoch 1:  34%|███▍      | 126/373 [08:30<16:32,  4.02s/it, loss=2.03, v_num=16, train_loss_step=1.720, val_loss=1.990, train_loss_epoch=3.400]\n",
      "Epoch 1:  34%|███▍      | 127/373 [08:33<16:27,  4.01s/it, loss=1.96, v_num=16, train_loss_step=1.690, val_loss=1.990, train_loss_epoch=3.400]\n",
      "Epoch 1:  34%|███▍      | 128/373 [08:37<16:21,  4.01s/it, loss=1.96, v_num=16, train_loss_step=1.600, val_loss=1.990, train_loss_epoch=3.400]\n",
      "Epoch 1:  35%|███▍      | 129/373 [08:40<16:16,  4.00s/it, loss=2, v_num=16, train_loss_step=2.090, val_loss=1.990, train_loss_epoch=3.400]   \n",
      "Epoch 1:  35%|███▍      | 130/373 [08:43<16:11,  4.00s/it, loss=1.96, v_num=16, train_loss_step=1.400, val_loss=1.990, train_loss_epoch=3.400]\n",
      "Epoch 1:  35%|███▌      | 131/373 [08:47<16:06,  3.99s/it, loss=1.9, v_num=16, train_loss_step=1.310, val_loss=1.990, train_loss_epoch=3.400] \n",
      "Epoch 1:  35%|███▌      | 132/373 [08:50<16:01,  3.99s/it, loss=1.91, v_num=16, train_loss_step=1.870, val_loss=1.990, train_loss_epoch=3.400]\n",
      "Epoch 1:  36%|███▌      | 133/373 [08:53<15:55,  3.98s/it, loss=1.92, v_num=16, train_loss_step=2.100, val_loss=1.990, train_loss_epoch=3.400]\n",
      "Epoch 1:  36%|███▌      | 134/373 [08:57<15:50,  3.98s/it, loss=1.95, v_num=16, train_loss_step=2.090, val_loss=1.990, train_loss_epoch=3.400]\n",
      "Epoch 1:  36%|███▌      | 135/373 [09:00<15:45,  3.97s/it, loss=1.99, v_num=16, train_loss_step=2.420, val_loss=1.990, train_loss_epoch=3.400]\n",
      "Epoch 1:  36%|███▋      | 136/373 [09:03<15:40,  3.97s/it, loss=2.01, v_num=16, train_loss_step=2.460, val_loss=1.990, train_loss_epoch=3.400]\n",
      "Epoch 1:  37%|███▋      | 137/373 [09:07<15:35,  3.96s/it, loss=2.01, v_num=16, train_loss_step=1.820, val_loss=1.990, train_loss_epoch=3.400]\n",
      "Epoch 1:  37%|███▋      | 138/373 [09:10<15:30,  3.96s/it, loss=2.01, v_num=16, train_loss_step=2.320, val_loss=1.990, train_loss_epoch=3.400]\n",
      "Epoch 1:  37%|███▋      | 139/373 [09:13<15:25,  3.96s/it, loss=2, v_num=16, train_loss_step=1.600, val_loss=1.990, train_loss_epoch=3.400]   \n",
      "Epoch 1:  38%|███▊      | 140/373 [09:17<15:20,  3.95s/it, loss=1.96, v_num=16, train_loss_step=1.990, val_loss=1.990, train_loss_epoch=3.400]\n",
      "Epoch 1:  38%|███▊      | 141/373 [09:20<15:15,  3.95s/it, loss=1.93, v_num=16, train_loss_step=1.130, val_loss=1.990, train_loss_epoch=3.400]\n",
      "Epoch 1:  38%|███▊      | 142/373 [09:23<15:10,  3.94s/it, loss=1.94, v_num=16, train_loss_step=2.320, val_loss=1.990, train_loss_epoch=3.400]\n",
      "Epoch 1:  38%|███▊      | 143/373 [09:28<15:07,  3.94s/it, loss=1.94, v_num=16, train_loss_step=2.070, val_loss=1.990, train_loss_epoch=3.400]\n",
      "Epoch 1:  39%|███▊      | 144/373 [09:32<15:03,  3.95s/it, loss=1.94, v_num=16, train_loss_step=2.140, val_loss=1.990, train_loss_epoch=3.400]\n",
      "Epoch 1:  39%|███▉      | 145/373 [09:36<14:59,  3.95s/it, loss=1.88, v_num=16, train_loss_step=1.550, val_loss=1.990, train_loss_epoch=3.400]\n",
      "Epoch 1:  39%|███▉      | 146/373 [09:40<14:56,  3.95s/it, loss=1.89, v_num=16, train_loss_step=1.770, val_loss=1.990, train_loss_epoch=3.400]\n",
      "Epoch 1:  39%|███▉      | 147/373 [09:48<14:58,  3.97s/it, loss=1.91, v_num=16, train_loss_step=2.140, val_loss=1.990, train_loss_epoch=3.400]\n",
      "Epoch 1:  40%|███▉      | 148/373 [09:52<14:54,  3.97s/it, loss=1.96, v_num=16, train_loss_step=2.590, val_loss=1.990, train_loss_epoch=3.400]\n",
      "Epoch 1:  40%|███▉      | 149/373 [09:57<14:52,  3.99s/it, loss=1.96, v_num=16, train_loss_step=2.150, val_loss=1.990, train_loss_epoch=3.400]\n",
      "Epoch 1:  40%|████      | 150/373 [10:59<16:14,  4.37s/it, loss=2, v_num=16, train_loss_step=2.180, val_loss=1.990, train_loss_epoch=3.400]   \n",
      "Epoch 1:  40%|████      | 151/373 [11:04<16:10,  4.37s/it, loss=2.02, v_num=16, train_loss_step=1.770, val_loss=1.990, train_loss_epoch=3.400]\n",
      "Epoch 1:  41%|████      | 152/373 [11:09<16:06,  4.37s/it, loss=2.04, v_num=16, train_loss_step=2.120, val_loss=1.990, train_loss_epoch=3.400]\n",
      "Epoch 1:  41%|████      | 153/373 [11:13<16:02,  4.37s/it, loss=2.04, v_num=16, train_loss_step=2.130, val_loss=1.990, train_loss_epoch=3.400]\n",
      "Epoch 1:  41%|████▏     | 154/373 [11:17<15:57,  4.37s/it, loss=2.02, v_num=16, train_loss_step=1.660, val_loss=1.990, train_loss_epoch=3.400]\n",
      "Epoch 1:  42%|████▏     | 155/373 [11:22<15:53,  4.37s/it, loss=2, v_num=16, train_loss_step=2.000, val_loss=1.990, train_loss_epoch=3.400]   \n",
      "Epoch 1:  42%|████▏     | 156/373 [11:27<15:49,  4.38s/it, loss=1.96, v_num=16, train_loss_step=1.840, val_loss=1.990, train_loss_epoch=3.400]\n",
      "Epoch 1:  42%|████▏     | 157/373 [11:31<15:45,  4.38s/it, loss=1.98, v_num=16, train_loss_step=2.170, val_loss=1.990, train_loss_epoch=3.400]\n",
      "Epoch 1:  42%|████▏     | 158/373 [11:35<15:41,  4.38s/it, loss=1.97, v_num=16, train_loss_step=2.140, val_loss=1.990, train_loss_epoch=3.400]\n",
      "Epoch 1:  43%|████▎     | 159/373 [11:41<15:37,  4.38s/it, loss=1.95, v_num=16, train_loss_step=1.220, val_loss=1.990, train_loss_epoch=3.400]\n",
      "Epoch 1:  43%|████▎     | 160/373 [11:45<15:33,  4.38s/it, loss=1.96, v_num=16, train_loss_step=2.020, val_loss=1.990, train_loss_epoch=3.400]\n",
      "Epoch 1:  43%|████▎     | 161/373 [11:49<15:28,  4.38s/it, loss=2.02, v_num=16, train_loss_step=2.380, val_loss=1.990, train_loss_epoch=3.400]\n",
      "Epoch 1:  43%|████▎     | 162/373 [11:54<15:24,  4.38s/it, loss=2.01, v_num=16, train_loss_step=2.190, val_loss=1.990, train_loss_epoch=3.400]\n",
      "Epoch 1:  44%|████▎     | 163/373 [11:58<15:20,  4.38s/it, loss=1.98, v_num=16, train_loss_step=1.490, val_loss=1.990, train_loss_epoch=3.400]\n",
      "Epoch 1:  44%|████▍     | 164/373 [12:03<15:16,  4.39s/it, loss=1.98, v_num=16, train_loss_step=2.160, val_loss=1.990, train_loss_epoch=3.400]\n",
      "Epoch 1:  44%|████▍     | 165/373 [12:07<15:12,  4.39s/it, loss=2.03, v_num=16, train_loss_step=2.530, val_loss=1.990, train_loss_epoch=3.400]\n",
      "Epoch 1:  45%|████▍     | 166/373 [12:12<15:08,  4.39s/it, loss=2.03, v_num=16, train_loss_step=1.700, val_loss=1.990, train_loss_epoch=3.400]\n",
      "Epoch 1:  45%|████▍     | 167/373 [12:17<15:04,  4.39s/it, loss=2.06, v_num=16, train_loss_step=2.810, val_loss=1.990, train_loss_epoch=3.400]\n",
      "Epoch 1:  45%|████▌     | 168/373 [12:21<14:59,  4.39s/it, loss=2.04, v_num=16, train_loss_step=2.090, val_loss=1.990, train_loss_epoch=3.400]\n",
      "Epoch 1:  45%|████▌     | 169/373 [12:25<14:55,  4.39s/it, loss=2.07, v_num=16, train_loss_step=2.720, val_loss=1.990, train_loss_epoch=3.400]\n",
      "Epoch 1:  46%|████▌     | 170/373 [12:29<14:50,  4.39s/it, loss=2.08, v_num=16, train_loss_step=2.390, val_loss=1.990, train_loss_epoch=3.400]\n",
      "Epoch 1:  46%|████▌     | 171/373 [12:34<14:45,  4.39s/it, loss=2.07, v_num=16, train_loss_step=1.670, val_loss=1.990, train_loss_epoch=3.400]\n",
      "Epoch 1:  46%|████▌     | 172/373 [12:39<14:41,  4.39s/it, loss=2.04, v_num=16, train_loss_step=1.570, val_loss=1.990, train_loss_epoch=3.400]\n",
      "Epoch 1:  46%|████▋     | 173/373 [12:43<14:37,  4.39s/it, loss=2.04, v_num=16, train_loss_step=1.940, val_loss=1.990, train_loss_epoch=3.400]\n",
      "Epoch 1:  47%|████▋     | 174/373 [12:48<14:33,  4.39s/it, loss=2.04, v_num=16, train_loss_step=1.850, val_loss=1.990, train_loss_epoch=3.400]\n",
      "Epoch 1:  47%|████▋     | 175/373 [12:52<14:29,  4.39s/it, loss=2.04, v_num=16, train_loss_step=1.850, val_loss=1.990, train_loss_epoch=3.400]\n",
      "Epoch 1:  47%|████▋     | 175/373 [12:52<14:29,  4.39s/it, loss=2.04, v_num=16, train_loss_step=1.890, val_loss=1.990, train_loss_epoch=3.400]\n",
      "Epoch 1:  47%|████▋     | 176/373 [12:57<14:25,  4.39s/it, loss=2.04, v_num=16, train_loss_step=1.920, val_loss=1.990, train_loss_epoch=3.400]\n",
      "Epoch 1:  47%|████▋     | 177/373 [13:02<14:21,  4.39s/it, loss=2.01, v_num=16, train_loss_step=1.440, val_loss=1.990, train_loss_epoch=3.400]\n",
      "Epoch 1:  48%|████▊     | 178/373 [13:07<14:17,  4.40s/it, loss=2.01, v_num=16, train_loss_step=1.440, val_loss=1.990, train_loss_epoch=3.400]\n",
      "Epoch 1:  48%|████▊     | 178/373 [13:07<14:17,  4.40s/it, loss=1.98, v_num=16, train_loss_step=1.620, val_loss=1.990, train_loss_epoch=3.400]\n",
      "Epoch 1:  48%|████▊     | 179/373 [13:11<14:13,  4.40s/it, loss=1.98, v_num=16, train_loss_step=1.620, val_loss=1.990, train_loss_epoch=3.400]\n",
      "Epoch 1:  48%|████▊     | 179/373 [13:11<14:13,  4.40s/it, loss=2.02, v_num=16, train_loss_step=2.070, val_loss=1.990, train_loss_epoch=3.400]\n",
      "Epoch 1:  48%|████▊     | 180/373 [13:16<14:09,  4.40s/it, loss=2.01, v_num=16, train_loss_step=1.740, val_loss=1.990, train_loss_epoch=3.400]\n",
      "Epoch 1:  49%|████▊     | 181/373 [13:20<14:04,  4.40s/it, loss=2, v_num=16, train_loss_step=2.240, val_loss=1.990, train_loss_epoch=3.400]   \n",
      "Epoch 1:  49%|████▉     | 182/373 [13:25<14:00,  4.40s/it, loss=1.99, v_num=16, train_loss_step=1.890, val_loss=1.990, train_loss_epoch=3.400]\n",
      "Epoch 1:  49%|████▉     | 183/373 [13:29<13:56,  4.40s/it, loss=1.98, v_num=16, train_loss_step=1.370, val_loss=1.990, train_loss_epoch=3.400]\n",
      "Epoch 1:  49%|████▉     | 184/373 [13:34<13:52,  4.40s/it, loss=1.97, v_num=16, train_loss_step=2.010, val_loss=1.990, train_loss_epoch=3.400]\n",
      "Epoch 1:  50%|████▉     | 185/373 [13:39<13:48,  4.40s/it, loss=1.91, v_num=16, train_loss_step=1.190, val_loss=1.990, train_loss_epoch=3.400]\n",
      "Epoch 1:  50%|████▉     | 186/373 [13:44<13:44,  4.41s/it, loss=1.95, v_num=16, train_loss_step=2.530, val_loss=1.990, train_loss_epoch=3.400]\n",
      "Epoch 1:  50%|█████     | 187/373 [13:49<13:40,  4.41s/it, loss=1.89, v_num=16, train_loss_step=1.630, val_loss=1.990, train_loss_epoch=3.400]\n",
      "Epoch 1:  50%|█████     | 188/373 [13:54<13:37,  4.42s/it, loss=1.86, v_num=16, train_loss_step=1.550, val_loss=1.990, train_loss_epoch=3.400]\n",
      "Epoch 1:  51%|█████     | 189/373 [13:59<13:33,  4.42s/it, loss=1.86, v_num=16, train_loss_step=2.620, val_loss=1.990, train_loss_epoch=3.400]\n",
      "Epoch 1:  51%|█████     | 190/373 [14:04<13:28,  4.42s/it, loss=1.86, v_num=16, train_loss_step=2.380, val_loss=1.990, train_loss_epoch=3.400]\n",
      "Epoch 1:  51%|█████     | 191/373 [14:09<13:24,  4.42s/it, loss=1.85, v_num=16, train_loss_step=1.530, val_loss=1.990, train_loss_epoch=3.400]\n",
      "Epoch 1:  51%|█████▏    | 192/373 [14:13<13:20,  4.42s/it, loss=1.85, v_num=16, train_loss_step=1.530, val_loss=1.990, train_loss_epoch=3.400]\n",
      "Epoch 1:  51%|█████▏    | 192/373 [14:14<13:20,  4.43s/it, loss=1.89, v_num=16, train_loss_step=2.330, val_loss=1.990, train_loss_epoch=3.400]\n",
      "Epoch 1:  52%|█████▏    | 193/373 [14:19<13:17,  4.43s/it, loss=1.87, v_num=16, train_loss_step=1.560, val_loss=1.990, train_loss_epoch=3.400]\n",
      "Epoch 1:  52%|█████▏    | 194/373 [14:23<13:12,  4.43s/it, loss=1.88, v_num=16, train_loss_step=2.040, val_loss=1.990, train_loss_epoch=3.400]\n",
      "Epoch 1:  52%|█████▏    | 195/373 [14:28<13:08,  4.43s/it, loss=1.86, v_num=16, train_loss_step=1.590, val_loss=1.990, train_loss_epoch=3.400]\n",
      "Epoch 1:  53%|█████▎    | 196/373 [14:33<13:04,  4.43s/it, loss=1.9, v_num=16, train_loss_step=2.700, val_loss=1.990, train_loss_epoch=3.400] \n",
      "Epoch 1:  53%|█████▎    | 197/373 [14:37<13:00,  4.43s/it, loss=1.93, v_num=16, train_loss_step=2.050, val_loss=1.990, train_loss_epoch=3.400]\n",
      "Epoch 1:  53%|█████▎    | 198/373 [14:42<12:56,  4.44s/it, loss=1.91, v_num=16, train_loss_step=1.170, val_loss=1.990, train_loss_epoch=3.400]\n",
      "Epoch 1:  53%|█████▎    | 199/373 [14:47<12:52,  4.44s/it, loss=1.91, v_num=16, train_loss_step=2.100, val_loss=1.990, train_loss_epoch=3.400]\n",
      "Epoch 1:  54%|█████▎    | 200/373 [14:53<12:48,  4.45s/it, loss=1.91, v_num=16, train_loss_step=1.730, val_loss=1.990, train_loss_epoch=3.400]\n",
      "Epoch 1:  54%|█████▍    | 201/373 [14:58<12:45,  4.45s/it, loss=1.9, v_num=16, train_loss_step=2.080, val_loss=1.990, train_loss_epoch=3.400] \n",
      "Epoch 1:  54%|█████▍    | 202/373 [15:03<12:41,  4.45s/it, loss=1.92, v_num=16, train_loss_step=2.260, val_loss=1.990, train_loss_epoch=3.400]\n",
      "Epoch 1:  54%|█████▍    | 203/373 [15:08<12:37,  4.45s/it, loss=1.95, v_num=16, train_loss_step=1.930, val_loss=1.990, train_loss_epoch=3.400]\n",
      "Epoch 1:  55%|█████▍    | 204/373 [15:13<12:33,  4.46s/it, loss=1.91, v_num=16, train_loss_step=1.270, val_loss=1.990, train_loss_epoch=3.400]\n",
      "Epoch 1:  55%|█████▍    | 205/373 [15:18<12:29,  4.46s/it, loss=1.95, v_num=16, train_loss_step=2.010, val_loss=1.990, train_loss_epoch=3.400]\n",
      "Epoch 1:  55%|█████▌    | 206/373 [15:24<12:25,  4.46s/it, loss=1.91, v_num=16, train_loss_step=1.670, val_loss=1.990, train_loss_epoch=3.400]\n",
      "Epoch 1:  55%|█████▌    | 207/373 [15:29<12:21,  4.47s/it, loss=1.91, v_num=16, train_loss_step=1.720, val_loss=1.990, train_loss_epoch=3.400]\n",
      "Epoch 1:  56%|█████▌    | 208/373 [15:34<12:17,  4.47s/it, loss=1.94, v_num=16, train_loss_step=1.980, val_loss=1.990, train_loss_epoch=3.400]\n",
      "Epoch 1:  56%|█████▌    | 209/373 [15:39<12:13,  4.47s/it, loss=1.88, v_num=16, train_loss_step=1.600, val_loss=1.990, train_loss_epoch=3.400]\n",
      "Epoch 1:  56%|█████▋    | 210/373 [15:44<12:09,  4.47s/it, loss=1.86, v_num=16, train_loss_step=1.940, val_loss=1.990, train_loss_epoch=3.400]\n",
      "Epoch 1:  57%|█████▋    | 211/373 [15:48<12:04,  4.47s/it, loss=1.87, v_num=16, train_loss_step=1.770, val_loss=1.990, train_loss_epoch=3.400]\n",
      "Epoch 1:  57%|█████▋    | 212/373 [15:53<12:00,  4.48s/it, loss=1.87, v_num=16, train_loss_step=2.280, val_loss=1.990, train_loss_epoch=3.400]\n",
      "Epoch 1:  57%|█████▋    | 213/373 [15:57<11:56,  4.48s/it, loss=1.9, v_num=16, train_loss_step=2.120, val_loss=1.990, train_loss_epoch=3.400] \n",
      "Epoch 1:  57%|█████▋    | 214/373 [16:02<11:52,  4.48s/it, loss=1.91, v_num=16, train_loss_step=2.320, val_loss=1.990, train_loss_epoch=3.400]\n",
      "Epoch 1:  58%|█████▊    | 215/373 [16:07<11:47,  4.48s/it, loss=1.93, v_num=16, train_loss_step=1.900, val_loss=1.990, train_loss_epoch=3.400]\n",
      "Epoch 1:  58%|█████▊    | 216/373 [16:12<11:43,  4.48s/it, loss=1.89, v_num=16, train_loss_step=1.940, val_loss=1.990, train_loss_epoch=3.400]\n",
      "Epoch 1:  58%|█████▊    | 217/373 [16:17<11:39,  4.48s/it, loss=1.85, v_num=16, train_loss_step=1.270, val_loss=1.990, train_loss_epoch=3.400]\n",
      "Epoch 1:  58%|█████▊    | 218/373 [16:22<11:35,  4.49s/it, loss=1.88, v_num=16, train_loss_step=1.770, val_loss=1.990, train_loss_epoch=3.400]\n",
      "Epoch 1:  59%|█████▊    | 219/373 [16:27<11:31,  4.49s/it, loss=1.88, v_num=16, train_loss_step=2.030, val_loss=1.990, train_loss_epoch=3.400]\n",
      "Epoch 1:  59%|█████▉    | 220/373 [16:32<11:26,  4.49s/it, loss=1.89, v_num=16, train_loss_step=1.960, val_loss=1.990, train_loss_epoch=3.400]\n",
      "Epoch 1:  59%|█████▉    | 221/373 [16:37<11:22,  4.49s/it, loss=1.89, v_num=16, train_loss_step=2.010, val_loss=1.990, train_loss_epoch=3.400]\n",
      "Epoch 1:  60%|█████▉    | 222/373 [16:42<11:18,  4.50s/it, loss=1.83, v_num=16, train_loss_step=1.210, val_loss=1.990, train_loss_epoch=3.400]\n",
      "Epoch 1:  60%|█████▉    | 223/373 [16:47<11:14,  4.50s/it, loss=1.84, v_num=16, train_loss_step=2.090, val_loss=1.990, train_loss_epoch=3.400]\n",
      "Epoch 1:  60%|██████    | 224/373 [16:52<11:10,  4.50s/it, loss=1.93, v_num=16, train_loss_step=3.100, val_loss=1.990, train_loss_epoch=3.400]\n",
      "Epoch 1:  60%|██████    | 225/373 [16:56<11:05,  4.50s/it, loss=1.91, v_num=16, train_loss_step=1.600, val_loss=1.990, train_loss_epoch=3.400]\n",
      "Epoch 1:  61%|██████    | 226/373 [17:01<11:01,  4.50s/it, loss=1.91, v_num=16, train_loss_step=1.590, val_loss=1.990, train_loss_epoch=3.400]\n",
      "Epoch 1:  61%|██████    | 227/373 [17:06<10:57,  4.50s/it, loss=1.92, v_num=16, train_loss_step=1.950, val_loss=1.990, train_loss_epoch=3.400]\n",
      "Epoch 1:  61%|██████    | 228/373 [17:11<10:52,  4.50s/it, loss=1.94, v_num=16, train_loss_step=2.440, val_loss=1.990, train_loss_epoch=3.400]\n",
      "Epoch 1:  61%|██████▏   | 229/373 [17:16<10:48,  4.50s/it, loss=1.94, v_num=16, train_loss_step=1.480, val_loss=1.990, train_loss_epoch=3.400]\n",
      "Epoch 1:  62%|██████▏   | 230/373 [17:20<10:44,  4.51s/it, loss=1.91, v_num=16, train_loss_step=1.400, val_loss=1.990, train_loss_epoch=3.400]\n",
      "Epoch 1:  62%|██████▏   | 231/373 [17:26<10:40,  4.51s/it, loss=1.91, v_num=16, train_loss_step=1.680, val_loss=1.990, train_loss_epoch=3.400]\n",
      "Epoch 1:  62%|██████▏   | 232/373 [17:30<10:35,  4.51s/it, loss=1.96, v_num=16, train_loss_step=3.250, val_loss=1.990, train_loss_epoch=3.400]\n",
      "Epoch 1:  62%|██████▏   | 233/373 [17:35<10:31,  4.51s/it, loss=1.91, v_num=16, train_loss_step=1.260, val_loss=1.990, train_loss_epoch=3.400]\n",
      "Epoch 1:  63%|██████▎   | 234/373 [17:39<10:26,  4.51s/it, loss=1.9, v_num=16, train_loss_step=2.050, val_loss=1.990, train_loss_epoch=3.400] \n",
      "Epoch 1:  63%|██████▎   | 235/373 [17:44<10:22,  4.51s/it, loss=1.89, v_num=16, train_loss_step=1.670, val_loss=1.990, train_loss_epoch=3.400]\n",
      "Epoch 1:  63%|██████▎   | 236/373 [17:48<10:17,  4.51s/it, loss=1.89, v_num=16, train_loss_step=2.060, val_loss=1.990, train_loss_epoch=3.400]\n",
      "Epoch 1:  64%|██████▎   | 237/373 [17:53<10:13,  4.51s/it, loss=1.94, v_num=16, train_loss_step=2.240, val_loss=1.990, train_loss_epoch=3.400]\n",
      "Epoch 1:  64%|██████▍   | 238/373 [17:57<10:08,  4.51s/it, loss=1.94, v_num=16, train_loss_step=1.670, val_loss=1.990, train_loss_epoch=3.400]\n",
      "Epoch 1:  64%|██████▍   | 239/373 [18:02<10:04,  4.51s/it, loss=1.95, v_num=16, train_loss_step=2.330, val_loss=1.990, train_loss_epoch=3.400]\n",
      "Epoch 1:  64%|██████▍   | 240/373 [18:08<10:00,  4.51s/it, loss=1.96, v_num=16, train_loss_step=2.040, val_loss=1.990, train_loss_epoch=3.400]\n",
      "Epoch 1:  65%|██████▍   | 241/373 [18:13<09:56,  4.52s/it, loss=1.96, v_num=16, train_loss_step=2.040, val_loss=1.990, train_loss_epoch=3.400]\n",
      "Epoch 1:  65%|██████▍   | 241/373 [18:13<09:56,  4.52s/it, loss=1.94, v_num=16, train_loss_step=1.650, val_loss=1.990, train_loss_epoch=3.400]\n",
      "Epoch 1:  65%|██████▍   | 242/373 [18:18<09:52,  4.52s/it, loss=1.96, v_num=16, train_loss_step=1.590, val_loss=1.990, train_loss_epoch=3.400]\n",
      "Epoch 1:  65%|██████▌   | 243/373 [18:22<09:47,  4.52s/it, loss=1.94, v_num=16, train_loss_step=1.670, val_loss=1.990, train_loss_epoch=3.400]\n",
      "Epoch 1:  65%|██████▌   | 244/373 [18:27<09:43,  4.52s/it, loss=1.89, v_num=16, train_loss_step=2.160, val_loss=1.990, train_loss_epoch=3.400]\n",
      "Epoch 1:  66%|██████▌   | 245/373 [18:31<09:38,  4.52s/it, loss=1.91, v_num=16, train_loss_step=1.960, val_loss=1.990, train_loss_epoch=3.400]\n",
      "Epoch 1:  66%|██████▌   | 246/373 [18:36<09:33,  4.52s/it, loss=1.94, v_num=16, train_loss_step=2.250, val_loss=1.990, train_loss_epoch=3.400]\n",
      "Epoch 1:  66%|██████▌   | 247/373 [18:41<09:29,  4.52s/it, loss=1.95, v_num=16, train_loss_step=2.080, val_loss=1.990, train_loss_epoch=3.400]\n",
      "Epoch 1:  66%|██████▋   | 248/373 [18:46<09:25,  4.52s/it, loss=1.91, v_num=16, train_loss_step=1.700, val_loss=1.990, train_loss_epoch=3.400]\n",
      "Epoch 1:  67%|██████▋   | 249/373 [18:51<09:21,  4.52s/it, loss=1.93, v_num=16, train_loss_step=1.820, val_loss=1.990, train_loss_epoch=3.400]\n",
      "Epoch 1:  67%|██████▋   | 250/373 [18:55<09:16,  4.53s/it, loss=1.96, v_num=16, train_loss_step=2.010, val_loss=1.990, train_loss_epoch=3.400]\n",
      "Epoch 1:  67%|██████▋   | 251/373 [19:01<09:12,  4.53s/it, loss=1.99, v_num=16, train_loss_step=2.300, val_loss=1.990, train_loss_epoch=3.400]\n",
      "Epoch 1:  68%|██████▊   | 252/373 [19:06<09:08,  4.53s/it, loss=1.93, v_num=16, train_loss_step=2.030, val_loss=1.990, train_loss_epoch=3.400]\n",
      "Epoch 1:  68%|██████▊   | 253/373 [19:10<09:03,  4.53s/it, loss=1.97, v_num=16, train_loss_step=2.160, val_loss=1.990, train_loss_epoch=3.400]\n",
      "Epoch 1:  68%|██████▊   | 254/373 [19:14<08:58,  4.53s/it, loss=1.95, v_num=16, train_loss_step=1.550, val_loss=1.990, train_loss_epoch=3.400]\n",
      "Epoch 1:  68%|██████▊   | 255/373 [19:19<08:54,  4.53s/it, loss=1.96, v_num=16, train_loss_step=1.990, val_loss=1.990, train_loss_epoch=3.400]\n",
      "Epoch 1:  69%|██████▊   | 256/373 [19:24<08:50,  4.53s/it, loss=1.92, v_num=16, train_loss_step=1.290, val_loss=1.990, train_loss_epoch=3.400]\n",
      "Epoch 1:  69%|██████▉   | 257/373 [19:29<08:45,  4.53s/it, loss=1.92, v_num=16, train_loss_step=2.060, val_loss=1.990, train_loss_epoch=3.400]\n",
      "Epoch 1:  69%|██████▉   | 258/373 [19:33<08:41,  4.53s/it, loss=1.93, v_num=16, train_loss_step=2.050, val_loss=1.990, train_loss_epoch=3.400]\n",
      "Epoch 1:  69%|██████▉   | 259/373 [19:38<08:36,  4.53s/it, loss=1.9, v_num=16, train_loss_step=1.680, val_loss=1.990, train_loss_epoch=3.400] \n",
      "Epoch 1:  70%|██████▉   | 260/373 [19:42<08:32,  4.53s/it, loss=1.89, v_num=16, train_loss_step=1.860, val_loss=1.990, train_loss_epoch=3.400]\n",
      "Epoch 1:  70%|██████▉   | 261/373 [19:47<08:27,  4.53s/it, loss=1.95, v_num=16, train_loss_step=2.800, val_loss=1.990, train_loss_epoch=3.400]\n",
      "Epoch 1:  70%|███████   | 262/373 [19:52<08:23,  4.53s/it, loss=1.97, v_num=16, train_loss_step=1.880, val_loss=1.990, train_loss_epoch=3.400]\n",
      "Epoch 1:  71%|███████   | 263/373 [19:57<08:18,  4.54s/it, loss=1.97, v_num=16, train_loss_step=1.850, val_loss=1.990, train_loss_epoch=3.400]\n",
      "Epoch 1:  71%|███████   | 264/373 [20:02<08:14,  4.54s/it, loss=1.95, v_num=16, train_loss_step=1.700, val_loss=1.990, train_loss_epoch=3.400]\n",
      "Epoch 1:  71%|███████   | 265/373 [20:07<08:10,  4.54s/it, loss=1.93, v_num=16, train_loss_step=1.560, val_loss=1.990, train_loss_epoch=3.400]\n",
      "Epoch 1:  71%|███████▏  | 266/373 [20:11<08:05,  4.54s/it, loss=1.92, v_num=16, train_loss_step=1.930, val_loss=1.990, train_loss_epoch=3.400]\n",
      "Epoch 1:  72%|███████▏  | 267/373 [20:16<08:01,  4.54s/it, loss=1.91, v_num=16, train_loss_step=1.960, val_loss=1.990, train_loss_epoch=3.400]\n",
      "Epoch 1:  72%|███████▏  | 268/373 [20:21<07:56,  4.54s/it, loss=1.93, v_num=16, train_loss_step=2.050, val_loss=1.990, train_loss_epoch=3.400]\n",
      "Epoch 1:  72%|███████▏  | 269/373 [20:25<07:52,  4.54s/it, loss=1.91, v_num=16, train_loss_step=1.490, val_loss=1.990, train_loss_epoch=3.400]\n",
      "Epoch 1:  72%|███████▏  | 270/373 [20:30<07:47,  4.54s/it, loss=1.9, v_num=16, train_loss_step=1.820, val_loss=1.990, train_loss_epoch=3.400] \n",
      "Epoch 1:  73%|███████▎  | 271/373 [20:35<07:43,  4.54s/it, loss=1.89, v_num=16, train_loss_step=2.130, val_loss=1.990, train_loss_epoch=3.400]\n",
      "Epoch 1:  73%|███████▎  | 272/373 [20:39<07:38,  4.54s/it, loss=1.89, v_num=16, train_loss_step=2.070, val_loss=1.990, train_loss_epoch=3.400]\n",
      "Epoch 1:  73%|███████▎  | 273/373 [20:44<07:34,  4.54s/it, loss=1.86, v_num=16, train_loss_step=1.390, val_loss=1.990, train_loss_epoch=3.400]\n",
      "Epoch 1:  73%|███████▎  | 274/373 [20:49<07:29,  4.54s/it, loss=1.87, v_num=16, train_loss_step=1.850, val_loss=1.990, train_loss_epoch=3.400]\n",
      "Epoch 1:  74%|███████▎  | 275/373 [20:54<07:25,  4.54s/it, loss=1.84, v_num=16, train_loss_step=1.370, val_loss=1.990, train_loss_epoch=3.400]\n",
      "Epoch 1:  74%|███████▍  | 276/373 [20:59<07:20,  4.55s/it, loss=1.89, v_num=16, train_loss_step=2.230, val_loss=1.990, train_loss_epoch=3.400]\n",
      "Epoch 1:  74%|███████▍  | 277/373 [21:04<07:16,  4.55s/it, loss=1.89, v_num=16, train_loss_step=2.120, val_loss=1.990, train_loss_epoch=3.400]\n",
      "Epoch 1:  75%|███████▍  | 278/373 [21:08<07:12,  4.55s/it, loss=1.87, v_num=16, train_loss_step=1.640, val_loss=1.990, train_loss_epoch=3.400]\n",
      "Epoch 1:  75%|███████▍  | 279/373 [21:13<07:07,  4.55s/it, loss=1.87, v_num=16, train_loss_step=1.730, val_loss=1.990, train_loss_epoch=3.400]\n",
      "Epoch 1:  75%|███████▌  | 280/373 [21:18<07:03,  4.55s/it, loss=1.84, v_num=16, train_loss_step=1.260, val_loss=1.990, train_loss_epoch=3.400]\n",
      "Epoch 1:  75%|███████▌  | 281/373 [21:23<06:58,  4.55s/it, loss=1.78, v_num=16, train_loss_step=1.580, val_loss=1.990, train_loss_epoch=3.400]\n",
      "Epoch 1:  76%|███████▌  | 282/373 [21:27<06:54,  4.55s/it, loss=1.78, v_num=16, train_loss_step=1.580, val_loss=1.990, train_loss_epoch=3.400]\n",
      "Epoch 1:  76%|███████▌  | 282/373 [21:28<06:54,  4.55s/it, loss=1.78, v_num=16, train_loss_step=1.810, val_loss=1.990, train_loss_epoch=3.400]\n",
      "Epoch 1:  76%|███████▌  | 283/373 [21:32<06:49,  4.55s/it, loss=1.78, v_num=16, train_loss_step=1.810, val_loss=1.990, train_loss_epoch=3.400]\n",
      "Epoch 1:  76%|███████▌  | 283/373 [21:33<06:49,  4.55s/it, loss=1.76, v_num=16, train_loss_step=1.530, val_loss=1.990, train_loss_epoch=3.400]\n",
      "Epoch 1:  76%|███████▌  | 284/373 [21:37<06:45,  4.55s/it, loss=1.74, v_num=16, train_loss_step=1.340, val_loss=1.990, train_loss_epoch=3.400]\n",
      "Epoch 1:  76%|███████▋  | 285/373 [21:42<06:40,  4.55s/it, loss=1.75, v_num=16, train_loss_step=1.670, val_loss=1.990, train_loss_epoch=3.400]\n",
      "Epoch 1:  77%|███████▋  | 286/373 [21:47<06:36,  4.56s/it, loss=1.74, v_num=16, train_loss_step=1.800, val_loss=1.990, train_loss_epoch=3.400]\n",
      "Epoch 1:  77%|███████▋  | 287/373 [21:53<06:32,  4.56s/it, loss=1.73, v_num=16, train_loss_step=1.750, val_loss=1.990, train_loss_epoch=3.400]\n",
      "Epoch 1:  77%|███████▋  | 288/373 [21:57<06:27,  4.56s/it, loss=1.69, v_num=16, train_loss_step=1.210, val_loss=1.990, train_loss_epoch=3.400]\n",
      "Epoch 1:  77%|███████▋  | 289/373 [22:02<06:23,  4.56s/it, loss=1.73, v_num=16, train_loss_step=2.240, val_loss=1.990, train_loss_epoch=3.400]\n",
      "Epoch 1:  78%|███████▊  | 290/373 [22:07<06:18,  4.56s/it, loss=1.7, v_num=16, train_loss_step=1.300, val_loss=1.990, train_loss_epoch=3.400] \n",
      "Epoch 1:  78%|███████▊  | 291/373 [22:11<06:14,  4.56s/it, loss=1.7, v_num=16, train_loss_step=2.030, val_loss=1.990, train_loss_epoch=3.400]\n",
      "Epoch 1:  78%|███████▊  | 292/373 [22:16<06:09,  4.56s/it, loss=1.69, v_num=16, train_loss_step=1.890, val_loss=1.990, train_loss_epoch=3.400]\n",
      "Epoch 1:  79%|███████▊  | 293/373 [22:21<06:05,  4.56s/it, loss=1.72, v_num=16, train_loss_step=2.050, val_loss=1.990, train_loss_epoch=3.400]\n",
      "Epoch 1:  79%|███████▉  | 294/373 [22:26<06:00,  4.56s/it, loss=1.68, v_num=16, train_loss_step=1.120, val_loss=1.990, train_loss_epoch=3.400]\n",
      "Epoch 1:  79%|███████▉  | 295/373 [22:31<05:56,  4.56s/it, loss=1.69, v_num=16, train_loss_step=1.570, val_loss=1.990, train_loss_epoch=3.400]\n",
      "Epoch 1:  79%|███████▉  | 296/373 [22:35<05:51,  4.57s/it, loss=1.67, v_num=16, train_loss_step=1.870, val_loss=1.990, train_loss_epoch=3.400]\n",
      "Epoch 1:  80%|███████▉  | 297/373 [22:40<05:46,  4.57s/it, loss=1.66, v_num=16, train_loss_step=1.820, val_loss=1.990, train_loss_epoch=3.400]\n",
      "Epoch 1:  80%|███████▉  | 298/373 [22:44<05:42,  4.57s/it, loss=1.66, v_num=16, train_loss_step=1.570, val_loss=1.990, train_loss_epoch=3.400]\n",
      "Epoch 1:  80%|████████  | 299/373 [22:49<05:37,  4.57s/it, loss=1.66, v_num=16, train_loss_step=1.770, val_loss=1.990, train_loss_epoch=3.400]\n",
      "Epoch 1:  80%|████████  | 300/373 [22:54<05:33,  4.57s/it, loss=1.68, v_num=16, train_loss_step=1.640, val_loss=1.990, train_loss_epoch=3.400]\n",
      "Epoch 1:  81%|████████  | 301/373 [22:59<05:28,  4.57s/it, loss=1.69, v_num=16, train_loss_step=1.880, val_loss=1.990, train_loss_epoch=3.400]\n",
      "Epoch 1:  81%|████████  | 302/373 [23:04<05:24,  4.57s/it, loss=1.69, v_num=16, train_loss_step=1.800, val_loss=1.990, train_loss_epoch=3.400]\n",
      "Epoch 1:  81%|████████  | 303/373 [23:08<05:19,  4.57s/it, loss=1.72, v_num=16, train_loss_step=2.080, val_loss=1.990, train_loss_epoch=3.400]\n",
      "Epoch 1:  82%|████████▏ | 304/373 [23:13<05:15,  4.57s/it, loss=1.73, v_num=16, train_loss_step=1.460, val_loss=1.990, train_loss_epoch=3.400]\n",
      "Epoch 1:  82%|████████▏ | 305/373 [23:17<05:10,  4.57s/it, loss=1.71, v_num=16, train_loss_step=1.290, val_loss=1.990, train_loss_epoch=3.400]\n",
      "Epoch 1:  82%|████████▏ | 306/373 [23:22<05:06,  4.57s/it, loss=1.72, v_num=16, train_loss_step=2.000, val_loss=1.990, train_loss_epoch=3.400]\n",
      "Epoch 1:  82%|████████▏ | 307/373 [23:27<05:01,  4.57s/it, loss=1.74, v_num=16, train_loss_step=2.250, val_loss=1.990, train_loss_epoch=3.400]\n",
      "Epoch 1:  83%|████████▎ | 308/373 [23:32<04:57,  4.57s/it, loss=1.8, v_num=16, train_loss_step=2.330, val_loss=1.990, train_loss_epoch=3.400] \n",
      "Epoch 1:  83%|████████▎ | 309/373 [23:36<04:52,  4.57s/it, loss=1.78, v_num=16, train_loss_step=1.830, val_loss=1.990, train_loss_epoch=3.400]\n",
      "Epoch 1:  83%|████████▎ | 310/373 [23:41<04:47,  4.57s/it, loss=1.8, v_num=16, train_loss_step=1.740, val_loss=1.990, train_loss_epoch=3.400] \n",
      "Epoch 1:  83%|████████▎ | 311/373 [23:46<04:43,  4.57s/it, loss=1.76, v_num=16, train_loss_step=1.280, val_loss=1.990, train_loss_epoch=3.400]\n",
      "Epoch 1:  84%|████████▎ | 312/373 [23:50<04:38,  4.57s/it, loss=1.78, v_num=16, train_loss_step=2.240, val_loss=1.990, train_loss_epoch=3.400]\n",
      "Epoch 1:  84%|████████▍ | 313/373 [23:55<04:34,  4.57s/it, loss=1.75, v_num=16, train_loss_step=1.560, val_loss=1.990, train_loss_epoch=3.400]\n",
      "Epoch 1:  84%|████████▍ | 314/373 [24:00<04:29,  4.57s/it, loss=1.79, v_num=16, train_loss_step=1.760, val_loss=1.990, train_loss_epoch=3.400]\n",
      "Epoch 1:  84%|████████▍ | 315/373 [24:05<04:25,  4.57s/it, loss=1.82, v_num=16, train_loss_step=2.250, val_loss=1.990, train_loss_epoch=3.400]\n",
      "Epoch 1:  85%|████████▍ | 316/373 [24:09<04:20,  4.57s/it, loss=1.78, v_num=16, train_loss_step=0.965, val_loss=1.990, train_loss_epoch=3.400]\n",
      "Epoch 1:  85%|████████▍ | 317/373 [24:14<04:16,  4.57s/it, loss=1.78, v_num=16, train_loss_step=1.850, val_loss=1.990, train_loss_epoch=3.400]\n",
      "Epoch 1:  85%|████████▌ | 318/373 [24:18<04:11,  4.57s/it, loss=1.81, v_num=16, train_loss_step=2.170, val_loss=1.990, train_loss_epoch=3.400]\n",
      "Epoch 1:  86%|████████▌ | 319/373 [24:23<04:06,  4.57s/it, loss=1.82, v_num=16, train_loss_step=2.030, val_loss=1.990, train_loss_epoch=3.400]\n",
      "Epoch 1:  86%|████████▌ | 320/373 [24:27<04:02,  4.57s/it, loss=1.84, v_num=16, train_loss_step=1.950, val_loss=1.990, train_loss_epoch=3.400]\n",
      "Epoch 1:  86%|████████▌ | 321/373 [24:32<03:57,  4.57s/it, loss=1.81, v_num=16, train_loss_step=1.460, val_loss=1.990, train_loss_epoch=3.400]\n",
      "Epoch 1:  86%|████████▋ | 322/373 [24:36<03:53,  4.57s/it, loss=1.81, v_num=16, train_loss_step=1.630, val_loss=1.990, train_loss_epoch=3.400]\n",
      "Epoch 1:  87%|████████▋ | 323/373 [24:40<03:48,  4.57s/it, loss=1.82, v_num=16, train_loss_step=2.330, val_loss=1.990, train_loss_epoch=3.400]\n",
      "Epoch 1:  87%|████████▋ | 324/373 [24:44<03:43,  4.57s/it, loss=1.84, v_num=16, train_loss_step=1.800, val_loss=1.990, train_loss_epoch=3.400]\n",
      "Epoch 1:  87%|████████▋ | 325/373 [24:49<03:39,  4.57s/it, loss=1.89, v_num=16, train_loss_step=2.300, val_loss=1.990, train_loss_epoch=3.400]\n",
      "Epoch 1:  87%|████████▋ | 326/373 [24:53<03:34,  4.57s/it, loss=1.88, v_num=16, train_loss_step=1.960, val_loss=1.990, train_loss_epoch=3.400]\n",
      "Epoch 1:  88%|████████▊ | 327/373 [24:58<03:30,  4.57s/it, loss=1.87, v_num=16, train_loss_step=1.990, val_loss=1.990, train_loss_epoch=3.400]\n",
      "Epoch 1:  88%|████████▊ | 328/373 [25:03<03:25,  4.57s/it, loss=1.85, v_num=16, train_loss_step=1.990, val_loss=1.990, train_loss_epoch=3.400]\n",
      "Epoch 1:  88%|████████▊ | 329/373 [25:07<03:21,  4.57s/it, loss=1.86, v_num=16, train_loss_step=2.010, val_loss=1.990, train_loss_epoch=3.400]\n",
      "Epoch 1:  88%|████████▊ | 330/373 [25:12<03:16,  4.57s/it, loss=1.86, v_num=16, train_loss_step=1.620, val_loss=1.990, train_loss_epoch=3.400]\n",
      "Epoch 1:  89%|████████▊ | 331/373 [25:16<03:11,  4.57s/it, loss=1.9, v_num=16, train_loss_step=2.210, val_loss=1.990, train_loss_epoch=3.400] \n",
      "Epoch 1:  89%|████████▉ | 332/373 [25:21<03:07,  4.57s/it, loss=1.89, v_num=16, train_loss_step=2.000, val_loss=1.990, train_loss_epoch=3.400]\n",
      "Epoch 1:  89%|████████▉ | 333/373 [25:25<03:02,  4.57s/it, loss=1.92, v_num=16, train_loss_step=2.200, val_loss=1.990, train_loss_epoch=3.400]\n",
      "Epoch 1:  90%|████████▉ | 334/373 [25:29<02:58,  4.57s/it, loss=1.94, v_num=16, train_loss_step=2.080, val_loss=1.990, train_loss_epoch=3.400]\n",
      "Epoch 1:  90%|████████▉ | 335/373 [25:34<02:53,  4.57s/it, loss=1.89, v_num=16, train_loss_step=1.340, val_loss=1.990, train_loss_epoch=3.400]\n",
      "Epoch 1:  90%|█████████ | 336/373 [25:38<02:48,  4.57s/it, loss=1.93, v_num=16, train_loss_step=1.610, val_loss=1.990, train_loss_epoch=3.400]\n",
      "Epoch 1:  90%|█████████ | 337/373 [25:42<02:44,  4.56s/it, loss=1.93, v_num=16, train_loss_step=1.820, val_loss=1.990, train_loss_epoch=3.400]\n",
      "Epoch 1:  91%|█████████ | 338/373 [25:47<02:39,  4.56s/it, loss=1.94, v_num=16, train_loss_step=2.540, val_loss=1.990, train_loss_epoch=3.400]\n",
      "Epoch 1:  91%|█████████ | 339/373 [25:51<02:35,  4.56s/it, loss=1.96, v_num=16, train_loss_step=2.340, val_loss=1.990, train_loss_epoch=3.400]\n",
      "Epoch 1:  91%|█████████ | 340/373 [25:56<02:30,  4.56s/it, loss=1.96, v_num=16, train_loss_step=2.040, val_loss=1.990, train_loss_epoch=3.400]\n",
      "Epoch 1:  91%|█████████▏| 341/373 [26:00<02:26,  4.56s/it, loss=2, v_num=16, train_loss_step=2.220, val_loss=1.990, train_loss_epoch=3.400]   \n",
      "Epoch 1:  92%|█████████▏| 342/373 [26:04<02:21,  4.56s/it, loss=2.05, v_num=16, train_loss_step=2.620, val_loss=1.990, train_loss_epoch=3.400]\n",
      "Epoch 1:  92%|█████████▏| 343/373 [26:09<02:16,  4.56s/it, loss=2.01, v_num=16, train_loss_step=1.510, val_loss=1.990, train_loss_epoch=3.400]\n",
      "Epoch 1:  92%|█████████▏| 344/373 [26:14<02:12,  4.56s/it, loss=2, v_num=16, train_loss_step=1.610, val_loss=1.990, train_loss_epoch=3.400]   \n",
      "Epoch 1:  92%|█████████▏| 345/373 [26:18<02:07,  4.56s/it, loss=2.01, v_num=16, train_loss_step=2.440, val_loss=1.990, train_loss_epoch=3.400]\n",
      "Epoch 1:  93%|█████████▎| 346/373 [26:23<02:03,  4.56s/it, loss=2.03, v_num=16, train_loss_step=2.460, val_loss=1.990, train_loss_epoch=3.400]\n",
      "Epoch 1:  93%|█████████▎| 347/373 [26:28<01:58,  4.56s/it, loss=2.03, v_num=16, train_loss_step=1.950, val_loss=1.990, train_loss_epoch=3.400]\n",
      "Epoch 1:  93%|█████████▎| 348/373 [26:32<01:54,  4.56s/it, loss=2.02, v_num=16, train_loss_step=1.870, val_loss=1.990, train_loss_epoch=3.400]\n",
      "Epoch 1:  94%|█████████▎| 349/373 [26:38<01:49,  4.57s/it, loss=1.99, v_num=16, train_loss_step=1.380, val_loss=1.990, train_loss_epoch=3.400]\n",
      "Epoch 1:  94%|█████████▍| 350/373 [26:43<01:45,  4.57s/it, loss=1.99, v_num=16, train_loss_step=1.650, val_loss=1.990, train_loss_epoch=3.400]\n",
      "Epoch 1:  94%|█████████▍| 351/373 [26:48<01:40,  4.57s/it, loss=2, v_num=16, train_loss_step=2.330, val_loss=1.990, train_loss_epoch=3.400]   \n",
      "Epoch 1:  94%|█████████▍| 352/373 [26:53<01:35,  4.57s/it, loss=2, v_num=16, train_loss_step=1.950, val_loss=1.990, train_loss_epoch=3.400]\n",
      "Epoch 1:  95%|█████████▍| 353/373 [26:58<01:31,  4.57s/it, loss=2.03, v_num=16, train_loss_step=2.790, val_loss=1.990, train_loss_epoch=3.400]\n",
      "Epoch 1:  95%|█████████▍| 354/373 [27:03<01:26,  4.57s/it, loss=2.03, v_num=16, train_loss_step=2.180, val_loss=1.990, train_loss_epoch=3.400]\n",
      "Epoch 1:  95%|█████████▌| 355/373 [27:08<01:22,  4.57s/it, loss=2.06, v_num=16, train_loss_step=1.840, val_loss=1.990, train_loss_epoch=3.400]\n",
      "Epoch 1:  95%|█████████▌| 356/373 [27:13<01:17,  4.58s/it, loss=2.05, v_num=16, train_loss_step=1.530, val_loss=1.990, train_loss_epoch=3.400]\n",
      "Epoch 1:  96%|█████████▌| 357/373 [27:18<01:13,  4.58s/it, loss=2.05, v_num=16, train_loss_step=1.810, val_loss=1.990, train_loss_epoch=3.400]\n",
      "Epoch 1:  96%|█████████▌| 358/373 [27:23<01:08,  4.58s/it, loss=2.04, v_num=16, train_loss_step=2.340, val_loss=1.990, train_loss_epoch=3.400]\n",
      "Epoch 1:  96%|█████████▌| 359/373 [27:27<01:04,  4.58s/it, loss=2.02, v_num=16, train_loss_step=1.880, val_loss=1.990, train_loss_epoch=3.400]\n",
      "Epoch 1:  97%|█████████▋| 360/373 [27:32<00:59,  4.58s/it, loss=2, v_num=16, train_loss_step=1.570, val_loss=1.990, train_loss_epoch=3.400]   \n",
      "Epoch 1:  97%|█████████▋| 361/373 [27:37<00:54,  4.58s/it, loss=1.97, v_num=16, train_loss_step=1.760, val_loss=1.990, train_loss_epoch=3.400]\n",
      "Epoch 1:  97%|█████████▋| 362/373 [27:42<00:50,  4.58s/it, loss=1.91, v_num=16, train_loss_step=1.310, val_loss=1.990, train_loss_epoch=3.400]\n",
      "Epoch 1:  97%|█████████▋| 363/373 [27:46<00:45,  4.58s/it, loss=1.92, v_num=16, train_loss_step=1.670, val_loss=1.990, train_loss_epoch=3.400]\n",
      "Epoch 1:  98%|█████████▊| 364/373 [27:51<00:41,  4.58s/it, loss=1.92, v_num=16, train_loss_step=1.620, val_loss=1.990, train_loss_epoch=3.400]\n",
      "Epoch 1:  98%|█████████▊| 365/373 [27:55<00:36,  4.58s/it, loss=1.88, v_num=16, train_loss_step=1.690, val_loss=1.990, train_loss_epoch=3.400]\n",
      "Epoch 1:  98%|█████████▊| 366/373 [28:00<00:32,  4.58s/it, loss=1.87, v_num=16, train_loss_step=2.310, val_loss=1.990, train_loss_epoch=3.400]\n",
      "Epoch 1:  98%|█████████▊| 367/373 [28:04<00:27,  4.58s/it, loss=1.85, v_num=16, train_loss_step=1.590, val_loss=1.990, train_loss_epoch=3.400]\n",
      "Epoch 1:  99%|█████████▊| 368/373 [28:12<00:22,  4.59s/it, loss=1.86, v_num=16, train_loss_step=2.080, val_loss=1.990, train_loss_epoch=3.400]\n",
      "Epoch 1:  99%|█████████▉| 369/373 [28:17<00:18,  4.59s/it, loss=1.91, v_num=16, train_loss_step=2.280, val_loss=1.990, train_loss_epoch=3.400]\n",
      "Epoch 1:  99%|█████████▉| 370/373 [28:22<00:13,  4.59s/it, loss=1.92, v_num=16, train_loss_step=1.930, val_loss=1.990, train_loss_epoch=3.400]\n",
      "Epoch 1:  99%|█████████▉| 371/373 [28:26<00:09,  4.59s/it, loss=1.89, v_num=16, train_loss_step=1.750, val_loss=1.990, train_loss_epoch=3.400]\n",
      "Epoch 1: 100%|█████████▉| 372/373 [28:31<00:04,  4.59s/it, loss=1.86, v_num=16, train_loss_step=1.330, val_loss=1.990, train_loss_epoch=3.400]\n",
      "\u001b[2m\u001b[36m(RayExecutor pid=2930)\u001b[0m \n",
      "Validating: 0it [00:00, ?it/s]\u001b[A0m \n",
      "Validating:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "\u001b[2m\u001b[36m(RayExecutor pid=2930)\u001b[0m \n",
      "Validating: 100%|██████████| 1/1 [02:06<00:00, 126.46s/it]\u001b[A\n",
      "Epoch 1: 100%|██████████| 373/373 [31:20<00:00,  5.03s/it, loss=1.86, v_num=16, train_loss_step=1.330, val_loss=1.510, train_loss_epoch=3.400]\n",
      "                                                          \u001b[A\n",
      "Epoch 1: 100%|██████████| 373/373 [31:21<00:00,  5.03s/it, loss=1.86, v_num=16, train_loss_step=1.330, val_loss=1.510, train_loss_epoch=3.400]\n",
      "lightning_logs/default/version_16/checkpoints/epoch=1-step=743.ckpt\n",
      "Finished in: 63.01327716906865 minutes\n",
      "val data loader: <class 'torch.utils.data.dataloader.DataLoader'>\n",
      "trainer: <class 'pytorch_lightning.trainer.trainer.Trainer'>\n",
      "plugin: <class 'ray_lightning.ray_ddp.RayPlugin'>\n",
      "best_model:<class 'pytorch_forecasting.models.temporal_fusion_transformer.TemporalFusionTransformer'>\n",
      "best_model_path:lightning_logs/default/version_16/checkpoints/epoch=1-step=743.ckpt\n",
      "CPU times: user 22min 17s, sys: 6min 50s, total: 29min 7s\n",
      "Wall time: 1h 3min\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# specify all the config parameters\n",
    "# smaller batch size on laptop gets rid of \"leaked semaphore objects\" warnings\n",
    "config = {\n",
    "    \"forecast_horizon\": 168,\n",
    "    \"context_length\": 63,\n",
    "    \"num_gpus\": AVAILABLE_GPU,\n",
    "    \"batch_size\": 128,  #64, \n",
    "    \"num_training_workers\": NUM_TRAINING_WORKERS,\n",
    "    \"epochs\": 2,\n",
    "    \"lr\": 0.05,\n",
    "    \"hidden_size\": 20,\n",
    "    \"dropout\": 0.1,\n",
    "    # \"dropout\": tune.choice([0, 0.05, 0.1]),\n",
    "    \"hidden_continuous_size\": 4,\n",
    "    \"attention_head_size\": NUM_TRAINING_WORKERS,\n",
    "    \"limit_train_batches\": 0.25,\n",
    "    \"fast_mode\": False,\n",
    "    \"tuning_run\": False,\n",
    "}\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "# Train the model, save the validation data loader and trainer objects\n",
    "validation_loader, trainer, best_model, best_model_path = \\\n",
    "    train_func(config, plugin)\n",
    "\n",
    "print(f\"Finished in: {(time.time()-start)/60} minutes\")\n",
    "print(f\"val data loader: {type(validation_loader)}\")\n",
    "print(f\"trainer: {type(trainer)}\")\n",
    "print(f\"plugin: {type(plugin)}\")\n",
    "print(f\"best_model:{type(best_model)}\")\n",
    "print(f\"best_model_path:{best_model_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ef9699c",
   "metadata": {},
   "source": [
    "# Evaluate Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ce034bd3-a0d7-410d-a169-bd051094bd69",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(RayExecutor pid=2935)\u001b[0m /Users/christy/mambaforge/envs/ray/lib/python3.8/multiprocessing/resource_tracker.py:216: UserWarning: resource_tracker: There appear to be 33 leaked semaphore objects to clean up at shutdown\n",
      "\u001b[2m\u001b[36m(RayExecutor pid=2935)\u001b[0m   warnings.warn('resource_tracker: There appear to be %d '\n",
      "\u001b[2m\u001b[36m(RayExecutor pid=2930)\u001b[0m /Users/christy/mambaforge/envs/ray/lib/python3.8/multiprocessing/resource_tracker.py:216: UserWarning: resource_tracker: There appear to be 65 leaked semaphore objects to clean up at shutdown\n",
      "\u001b[2m\u001b[36m(RayExecutor pid=2930)\u001b[0m   warnings.warn('resource_tracker: There appear to be %d '\n",
      "\u001b[2m\u001b[36m(RayExecutor pid=2931)\u001b[0m /Users/christy/mambaforge/envs/ray/lib/python3.8/multiprocessing/resource_tracker.py:216: UserWarning: resource_tracker: There appear to be 33 leaked semaphore objects to clean up at shutdown\n",
      "\u001b[2m\u001b[36m(RayExecutor pid=2931)\u001b[0m   warnings.warn('resource_tracker: There appear to be %d '\n",
      "\u001b[2m\u001b[36m(RayExecutor pid=2932)\u001b[0m /Users/christy/mambaforge/envs/ray/lib/python3.8/multiprocessing/resource_tracker.py:216: UserWarning: resource_tracker: There appear to be 33 leaked semaphore objects to clean up at shutdown\n",
      "\u001b[2m\u001b[36m(RayExecutor pid=2932)\u001b[0m   warnings.warn('resource_tracker: There appear to be %d '\n",
      "\u001b[2m\u001b[36m(RayExecutor pid=2936)\u001b[0m /Users/christy/mambaforge/envs/ray/lib/python3.8/multiprocessing/resource_tracker.py:216: UserWarning: resource_tracker: There appear to be 33 leaked semaphore objects to clean up at shutdown\n",
      "\u001b[2m\u001b[36m(RayExecutor pid=2936)\u001b[0m   warnings.warn('resource_tracker: There appear to be %d '\n",
      "\u001b[2m\u001b[36m(RayExecutor pid=2933)\u001b[0m /Users/christy/mambaforge/envs/ray/lib/python3.8/multiprocessing/resource_tracker.py:216: UserWarning: resource_tracker: There appear to be 33 leaked semaphore objects to clean up at shutdown\n",
      "\u001b[2m\u001b[36m(RayExecutor pid=2933)\u001b[0m   warnings.warn('resource_tracker: There appear to be %d '\n",
      "\u001b[2m\u001b[36m(RayExecutor pid=2937)\u001b[0m /Users/christy/mambaforge/envs/ray/lib/python3.8/multiprocessing/resource_tracker.py:216: UserWarning: resource_tracker: There appear to be 33 leaked semaphore objects to clean up at shutdown\n",
      "\u001b[2m\u001b[36m(RayExecutor pid=2937)\u001b[0m   warnings.warn('resource_tracker: There appear to be %d '\n",
      "\u001b[2m\u001b[36m(RayExecutor pid=2934)\u001b[0m /Users/christy/mambaforge/envs/ray/lib/python3.8/multiprocessing/resource_tracker.py:216: UserWarning: resource_tracker: There appear to be 33 leaked semaphore objects to clean up at shutdown\n",
      "\u001b[2m\u001b[36m(RayExecutor pid=2934)\u001b[0m   warnings.warn('resource_tracker: There appear to be %d '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE: 5.323119640350342\n",
      "RMSE: 17.71480369567871\n",
      "available quantiles: [0.02, 0.1, 0.25, 0.5, 0.75, 0.9, 0.98]\n",
      "Mean WQL over quantiles [0.25, 0.5, 0.75]: 0.2854246199131012\n"
     ]
    }
   ],
   "source": [
    "## EVALUATE THE DL MODEL\n",
    "\n",
    "# get actuals, predictions on validation data, as tensors\n",
    "actuals = torch.cat([y[0] for x, y in iter(validation_loader)])\n",
    "predictions = best_model.predict(validation_loader)\n",
    "\n",
    "# calculate MAE\n",
    "MAE = (actuals - predictions).abs().mean()\n",
    "print(f\"MAE: {MAE}\")\n",
    "\n",
    "# calculate RMSE\n",
    "criterion = torch.nn.MSELoss()\n",
    "RMSE = torch.sqrt(criterion(actuals, predictions))\n",
    "print(f\"RMSE: {RMSE}\")\n",
    "\n",
    "# calculate WQL\n",
    "print(f\"available quantiles: {best_model.loss.quantiles}\")\n",
    "# note: to get a single item quantile prediction:\n",
    "# example: quantile p50 for itemid=\"140\"\n",
    "# y_quantiles[1].detach().cpu()[43, : x[\"decoder_lengths\"][43]] \n",
    "# raw predictions are a dictionary from which quantiles can be extracted\n",
    "raw_predictions, x = best_model.predict(validation_loader, mode=\"raw\", return_x=True)\n",
    "desired_quantiles = [0.25, 0.5, 0.75]\n",
    "y_quantiles = best_model.to_quantiles(raw_predictions, desired_quantiles)\n",
    "WQL = calc_wql(actuals, y_quantiles, desired_quantiles)\n",
    "print(f\"Mean WQL over quantiles {desired_quantiles}: {WQL}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ce653c62-b262-489a-bfbc-6479a17d0dce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# best_model_path:lightning_logs/default/version_18/checkpoints/epoch=1-step=743.ckpt\n",
    "# CPU times: user 25min 38s, sys: 7min 32s, total: 33min 10s\n",
    "# Wall time: 1h 15min 59s\n",
    "# MAE: 5.265017032623291\n",
    "# RMSE: 17.64057159423828\n",
    "# available quantiles: [0.02, 0.1, 0.25, 0.5, 0.75, 0.9, 0.98]\n",
    "# Mean WQL over quantiles [0.25, 0.5, 0.75]: 0.28858140110969543"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2092fdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Visualize in tensorboard, hit ctrl-c when done\n",
    "# !tensorboard --logdir=lightning_logs --load_fast=false\n",
    "# Start tensorboard.\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir lightning_logs/\n",
    "\n",
    "# Doesn't seem to work when running on Cloud?\n",
    "# (base):~/christy-forecast-pytorch/lightning_tensorboard --logdir=lightning_logs --load_fast=falseoad_fast=false\n",
    "# 2021-12-16 13:21:52.644765: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
    "# 2021-12-16 13:21:52.644793: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
    "# 2021-12-16 13:21:52.644818: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (ses-gtzg9465hmpnyk4piy3b6nmx-head-node-type-8599797c78-4dm49): /proc/driver/nvidia/version does not exist\n",
    "# Serving TensorBoard on localhost; to expose to the network, use a proxy or pass --bind_all\n",
    "# TensorBoard 2.7.0 at http://localhost:6006/ (Press CTRL+C to quit)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec2fe873",
   "metadata": {},
   "source": [
    "# Plot actuals vs predictions\n",
    "\n",
    "Using built-in function <a href=\"https://pytorch-forecasting.readthedocs.io/en/latest/_modules/pytorch_forecasting/models/base_model.html#BaseModel.plot_prediction\">plot_prediction</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec0cba1e-a51f-44cf-8711-fc84404875aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a previous model\n",
    "# checkpoint_path = \"/users/christy/Documents/githubAnyscalePublic/AnyscaleDemos/forecasting_demos/lightning_logs/default/version_2/checkpoints/epoch=1-step=1489.ckpt\"\n",
    "# restart_model = ptf.models.TemporalFusionTransformer.load_from_checkpoint(checkpoint_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cb53971-9635-4f7c-92cd-9dfebe306faf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look up embedding index values of items you want to find\n",
    "# best_model.hparams\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bdfdd036-db5d-44ad-9c63-a0d10865b4e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "for idx in [141, 43, 144]:  # plot first 5 indexes (sort of a random sample)\n",
    "    best_model.plot_prediction(x, raw_predictions, idx=idx, add_loss_to_title=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7d7ead49-0097-45f5-8dfe-a968ffc0ed7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make plots square for copy/paste in blog, see options\n",
    "# https://pytorch-forecasting.readthedocs.io/en/latest/_modules/pytorch_forecasting/models/base_model.html#BaseModel.plot_prediction\n",
    "# grey lines denote the amount of attention the model pays to different points \n",
    "# in time when making the prediction.\n",
    "\n",
    "# note: grey lines in plots are model's attention\n",
    "# note: attention does not plot correctly with indexes, so create extra plot\n",
    "fig, axs = plt.subplots(4, 1, figsize=(15, 12), sharex=True, sharey=False)\n",
    "for i, idx in enumerate([141, 43, 144]): \n",
    "    best_model.plot_prediction(x, \n",
    "        raw_predictions, \n",
    "        idx=idx, \n",
    "        add_loss_to_title=True,\n",
    "        ax=axs[i+1]);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bf1da77-1e36-42b6-aade-a404bdcc1dee",
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx in range(5):  # plot first 5 indexes (sort of a random sample)\n",
    "    best_model.plot_prediction(x, raw_predictions, idx=idx, add_loss_to_title=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05297862-103d-49a6-a7ed-92414663b12b",
   "metadata": {},
   "source": [
    "# Shutdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9591a33f-7bc4-426a-842d-ebfb1636de61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# When finished, shut down ray\n",
    "\n",
    "if not REGULAR_PYTHON:\n",
    "    if ray.is_initialized():\n",
    "        ray.shutdown()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe38ac2e-1247-4444-8103-3b54460d5dd1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
